<!DOCTYPE html>
<html>
    <head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" >
    <title>
        
        Hidden Markov Model for Part of Speech Tagging · Shengbin&#39;s Studio
        
    </title>
    <link rel="icon" href= /assets/favicon2.ico>
    <!-- TODO: 在font-face加载完毕后改变字体  -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/webfont/1.6.28/webfontloader.js"></script>
    <!-- 提前加载place holder  -->
    <style type="text/css">
        @font-face {
            font-family: 'Oswald-Regular';
            src: url(/font/Oswald-Regular.ttf);
        }
    </style>
    <style type="text/css">
        .site-intro {
            position: relative;
            width: 100%;
            height: 50vh;
            overflow: hidden;
            box-shadow: -0.1rem 0 0.5rem 0 rgba(0, 0, 0, 0.8);
        }
        .site-intro-placeholder {
            position: absolute;
            z-index: -2;
            top: 0;
            left: 0px;
            width: calc(100% + 300px);
            height: 100%;
            background: repeating-linear-gradient(-45deg, #444 0, #444 80px, #333 80px, #333 160px);
            background-position: center center;
            transform: translate3d(-226px, 0, 0);
            animation: gradient-move 2.5s ease-out 0s 1;
        }
        @keyframes gradient-move {
        0% {
            transform: translate3d(-226px, 0, 0);
        }
        100% {
            transform: translate3d(0, 0, 0);
        }
    }
</style>
    <link rel="stylesheet" href = /css/style.css?v=20171218 />
    <script src="//cdn.staticfile.org/jquery/3.2.1/jquery.min.js" defer></script>
    
    <script src="/scripts/main.js" defer></script>
    <!-- 百度统计  -->
    
    <!-- 谷歌统计  -->
    
</head>
    
        <body class="post-body">
    
    
<header class="header">

    <div class="read-progress"></div>
    <div class="header-sidebar-menu">&#xe775;</div>
    <!-- post页的toggle banner  -->
    
    <div class="banner">
            <div class="blog-title">
                <a href="/" >Shengbin&#39;s Studio.</a>
            </div>
            <div class="post-title">
                <a href="#" class="post-name">Hidden Markov Model for Part of Speech Tagging</a>
            </div>
    </div>
    
    <a class="home-link" href=/>Shengbin's Studio.</a>
</header>
    <div class="wrapper">
        <div class="site-intro">
    
    <!-- 主页  -->
    
    
    <!-- 404页  -->
            
    <div class="site-intro-img" style="background-image: url(http://oumn0o088.bkt.clouddn.com/post-bg.jpg)"></div>
    <div class="site-intro-placeholder"></div>
    <div class="site-intro-meta">
        <!-- 标题  -->
        <h1 class="intro-title">
            <!-- 主页  -->
            
            Hidden Markov Model for Part of Speech Tagging
            <!-- 404 -->
            
        </h1>
        <!-- 副标题 -->
        <p class="intro-subtitle">
            <!-- 主页副标题  -->
            
            
            <!-- 404 -->
            
        </p>
        <!-- 文章页meta -->
        
            <!-- 文章页标签  -->
            
                <div class= post-intro-tags >
    
        <a class="post-tag" href="javascript:void(0);" data-href = machine learning>machine learning</a>
    
        <a class="post-tag" href="javascript:void(0);" data-href = natural language processing>natural language processing</a>
    
</div>
            
            <div class="post-intro-meta">
                <span class="post-intro-calander iconfont-archer">&#xe676;</span>
                <span class="post-intro-time">2017/09/23</span>
            </div>
        
    </div>
</div>
        <img class="loading" src="/assets/loading.svg" style="display: block; margin: 6rem auto 0 auto; width: 6rem; height: 6rem;" />
        <script>
            var browser = {
                    versions: function () {
                        var u = window.navigator.userAgent;
                        return {
                            userAgent: u,
                            trident: u.indexOf('Trident') > -1, //IE内核
                            presto: u.indexOf('Presto') > -1, //opera内核
                            webKit: u.indexOf('AppleWebKit') > -1, //苹果、谷歌内核
                            gecko: u.indexOf('Gecko') > -1 && u.indexOf('KHTML') == -1, //火狐内核
                            mobile: !!u.match(/AppleWebKit.*Mobile.*/), //是否为移动终端
                            ios: !!u.match(/\(i[^;]+;( U;)? CPU.+Mac OS X/), //ios终端
                            android: u.indexOf('Android') > -1 || u.indexOf('Linux') > -1, //android终端或者uc浏览器
                            iPhone: u.indexOf('iPhone') > -1 || u.indexOf('Mac') > -1, //是否为iPhone或者安卓QQ浏览器
                            iPad: u.indexOf('iPad') > -1, //是否为iPad
                            webApp: u.indexOf('Safari') == -1, //是否为web应用程序，没有头部与底部
                            weixin: u.indexOf('MicroMessenger') == -1, //是否为微信浏览器
                            uc: u.indexOf('UCBrowser') > -1 //是否为android下的UC浏览器
                        };
                    }()
                }

            function fontLoaded(){
                console.log('font loaded');
                if (document.getElementsByClassName('site-intro-meta')) {
                    document.getElementsByClassName('intro-title')[0].classList.add('intro-fade-in');
                    document.getElementsByClassName('intro-subtitle')[0].classList.add('intro-fade-in');
                    var postIntroTags = document.getElementsByClassName('post-intro-tags')[0],
                        postIntroMeat = document.getElementsByClassName('post-intro-meta')[0];
                        if (postIntroTags) {
                            postIntroTags.classList.add('post-fade-in');
                        }
                        if (postIntroMeat) {
                            postIntroMeat.classList.add('post-fade-in');
                        }
                    }
                }
                
            console.log("userAgent:" + browser.versions.userAgent);
            // UC不支持跨域，所以直接显示
            if (browser.versions.uc) {
                console.log("UCBrowser");
                fontLoaded();
            } else {
                WebFont.load({
                    custom: {
                        families: ['Oswald-Regular']
                    },
                    loading: function () {  //所有字体开始加载
                        // console.log('loading');
                    },
                    active: function () {  //所有字体已渲染
                        fontLoaded();
                    },
                    inactive: function () { //字体预加载失败，无效字体或浏览器不支持加载
                        console.log('inactive: timeout');
                        fontLoaded();
                    },
                    timeout: 7000 // Set the timeout to two seconds
                });
            }
        </script>
        <div class="container container-unloaded">
            <main class="main post-page">
    <article class="article-entry">
        <h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>In this post, we will train a Hidden Markov Model for part of speech tagging. The first 10K tagged sentence of ‘news’ in the brown corpus will be used to train the Hidden Markov Model. A sentence from ‘alice’ will be infered by this Hidden Markov Model. </p>
<h4 id="Extract-data"><a href="#Extract-data" class="headerlink" title="Extract data"></a>Extract data</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> nltk.tag.sequential <span class="keyword">import</span> UnigramTagger</span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> brown</span><br><span class="line"><span class="keyword">from</span> nltk.tokenize <span class="keyword">import</span> TreebankWordTokenizer</span><br><span class="line"></span><br><span class="line">alice = nltk.corpus.gutenberg.raw(<span class="string">'carroll-alice.txt'</span>)</span><br><span class="line">tagger = UnigramTagger(brown.tagged_sents(categories=<span class="string">'news'</span>)[:<span class="number">10000</span>])</span><br><span class="line">tokens = TreebankWordTokenizer().tokenize(alice[:<span class="number">1000</span>])</span><br><span class="line">tags = tagger.tag(tokens)</span><br><span class="line">sents = brown.tagged_sents(categories=<span class="string">'news'</span>)[:<span class="number">10000</span>]</span><br><span class="line">words = [w[<span class="number">0</span>] <span class="keyword">for</span> s <span class="keyword">in</span> sents <span class="keyword">for</span> w <span class="keyword">in</span> s]</span><br></pre></td></tr></table></figure>
<h5 id="Compress-the-tags"><a href="#Compress-the-tags" class="headerlink" title="Compress the tags"></a>Compress the tags</h5><p>The tags provided by brown corpus is to detail, it can be compressed in some extent.<br>Detelte the postfix of the generated tags, as followed. All the dashes and everything behind them are removed and all the asterisks are also removed, while the $ is kept as its original way.  </p>
<blockquote>
<p>NN-TL -&gt; NN<br>  BEZ<em> -&gt; BEZ<br>  FW-</em> -&gt; FW<br>  :-HL -&gt; :  </p>
</blockquote>
<p>After this process, the tags like “NN-TL” and “NN-JJ” become “NN” without its postfix and “JJ-TL” will become “JJ”.<br>Before compressing the tags, the total number of tags is 218 and after compressing,  the number of tags decrease to 88. Although it will have some effect on the precision of tags but it will improve the accurary of the output prediction.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">sents_temp = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(sents)):</span><br><span class="line">    sents_revised_list = []</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(len(sents[i])):</span><br><span class="line">        content = list(sents[i][j])</span><br><span class="line">        <span class="comment">#content[1]= re.sub("([\$]*[\-\+]+[\w\$\*]+)+$|\$+$|(?&lt;=\w)\*+$", "", content[1]) ## remove the $ version</span></span><br><span class="line">        content[<span class="number">1</span>]= re.sub(<span class="string">"([\-\+]+[\w\$\*]+)+$|(?&lt;=\w)\*+$"</span>, <span class="string">""</span>, content[<span class="number">1</span>]) <span class="comment">## keep the $ version</span></span><br><span class="line">        content[<span class="number">1</span>]= re.sub(<span class="string">"(?&lt;=\w)\*$"</span>, <span class="string">""</span>, content[<span class="number">1</span>])</span><br><span class="line">        content_tuple = tuple(content)</span><br><span class="line">        sents_revised_list.append(content_tuple)</span><br><span class="line">    sents_temp.append(sents_revised_list)</span><br><span class="line">temp = sents</span><br><span class="line">sents = sents_temp</span><br><span class="line">sents_temp = temp</span><br></pre></td></tr></table></figure></p>
<h4 id="Generate-a-set-of-tags-and-a-set-of-words"><a href="#Generate-a-set-of-tags-and-a-set-of-words" class="headerlink" title="Generate a set of tags and a set of words"></a>Generate a set of tags and a set of words</h4><ul>
<li>python can maintain a set without duplicated elements. After the sets of tags and words is built, a list of tags and a list of words which contains all the elements in the set are generated. And then dictionarys for tags index and word index are generated.  </li>
<li>The tags list and words list are used to get a tag or a word via its index. And the tags dictionanry and words dictionary are used to get their index.</li>
<li>In the tags list and tags set, the tags of start state and end state are added.  </li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">sents_list = [t <span class="keyword">for</span> s <span class="keyword">in</span> sents <span class="keyword">for</span> t <span class="keyword">in</span> s]</span><br><span class="line">tags_set = set()</span><br><span class="line">words_set = set()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(sents_list)):</span><br><span class="line">    tags_set.add(sents_list[i][<span class="number">1</span>])</span><br><span class="line">    words_set.add(sents_list[i][<span class="number">0</span>])</span><br><span class="line">tags_list = list(tags_set)</span><br><span class="line">words_list = list(words_set)</span><br><span class="line"></span><br><span class="line"><span class="comment">## add the start and end state</span></span><br><span class="line">tags_list.insert(<span class="number">0</span>, <span class="string">'START_STATE'</span>)</span><br><span class="line">tags_list.insert(len(tags_list), <span class="string">'END_STATE'</span>)</span><br><span class="line"></span><br><span class="line">tags_hash = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> index, tag <span class="keyword">in</span> enumerate(tags_list):</span><br><span class="line">    tags_hash[tag] = index</span><br><span class="line">words_hash = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> index, word <span class="keyword">in</span> enumerate(words_list):</span><br><span class="line">    words_hash[word] = index</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Tags after compressing:"</span>)</span><br><span class="line">print(tags_hash)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Tags after compressing:<br>{‘START_STATE’: 0, ‘NPS’: 1, ‘WPO’: 2, ‘PPLS’: 3, ‘BEN’: 4, ‘NNS’: 5, ‘BER’: 6, ‘VBN’: 7, ‘CS’: 8, ‘TO’: 9, ‘DTX’: 10, ‘)’: 11, ‘,’: 12, ‘NP’: 13, ‘VBD’: 14, ‘FW’: 15, ‘PP$$’: 16, ‘JJR’: 17, ‘WPS’: 18, ‘OD’: 19, ‘QLP’: 20, ‘AT’: 21, ‘EX’: 22, ‘CC’: 23, ‘HVD’: 24, ‘PPS’: 25, ‘PP$’: 26, ‘NR$’: 27, ‘``’: 28, ‘QL’: 29, ‘JJS’: 30, ‘BE’: 31, ‘HVN’: 32, ‘PN$’: 33, ‘ABN’: 34, ‘NN$’: 35, ‘AP’: 36, ‘NNS$’: 37, ‘RB$’: 38, ‘VBZ’: 39, ‘NR’: 40, ‘PPL’: 41, ‘NP$’: 42, ‘DT$’: 43, ‘BEDZ’: 44, ‘RBT’: 45, ‘MD’: 46, ‘DT’: 47, ‘NN’: 48, ‘ABL’: 49, ‘BEM’: 50, ‘BED’: 51, ‘AP$’: 52, ‘HV’: 53, ‘(‘: 54, ‘WDT’: 55, ‘DTS’: 56, ‘RP’: 57, ‘VBG’: 58, ‘HVG’: 59, ‘NPS$’: 60, ‘BEZ’: 61, ‘JJT’: 62, ‘.’: 63, “‘’”: 64, ‘DOZ’: 65, ‘ABX’: 66, ‘CD’: 67, ‘DOD’: 68, ‘RB’: 69, ‘DO’: 70, ‘BEG’: 71, ‘RBR’: 72, “‘“: 73, ‘—‘: 74, ‘PPSS’: 75, ‘WQL’: 76, ‘*’: 77, ‘IN’: 78, ‘VB’: 79, ‘PPO’: 80, ‘HVZ’: 81, ‘WP$’: 82, ‘WRB’: 83, ‘PN’: 84, ‘CD$’: 85, ‘JJ’: 86, ‘UH’: 87, ‘DTI’: 88, ‘:’: 89, ‘END_STATE’: 90}</p>
</blockquote>
<h3 id="Forward-Algorithm"><a href="#Forward-Algorithm" class="headerlink" title="Forward Algorithm"></a>Forward Algorithm</h3><h4 id="Transition-Probability-Matrix-A"><a href="#Transition-Probability-Matrix-A" class="headerlink" title="Transition Probability Matrix: A"></a>Transition Probability Matrix: A</h4><ul>
<li>The row length and column length both are the same as the length of tags</li>
<li>In order to reduce the effect of the zero values in this matrix, all elements in matrix A was added by a small value 1e-10 and then re-normalized.  </li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">A_len = len(tags_hash)</span><br><span class="line">A = np.zeros((A_len, A_len))</span><br><span class="line"></span><br><span class="line">start_state = tags_hash[<span class="string">'START_STATE'</span>]</span><br><span class="line">end_state = tags_hash[<span class="string">'END_STATE'</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(sents)):</span><br><span class="line">    </span><br><span class="line">    start_tag_index = tags_hash[sents[i][<span class="number">0</span>][<span class="number">1</span>]]</span><br><span class="line">    A[start_state][start_tag_index] += <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(len(sents[i]) - <span class="number">1</span>):</span><br><span class="line">        row_index = tags_hash[sents[i][j][<span class="number">1</span>]]</span><br><span class="line">        col_index = tags_hash[sents[i][j + <span class="number">1</span>][<span class="number">1</span>]]</span><br><span class="line">        A[row_index][col_index] += <span class="number">1</span></span><br><span class="line">    end_tag_index = tags_hash[sents[i][len(sents[i]) - <span class="number">1</span>][<span class="number">1</span>]]</span><br><span class="line">    A[end_tag_index][end_state] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">A[A_len - <span class="number">1</span>] += <span class="number">1e-10</span></span><br><span class="line">A_row_sum = np.sum(A, axis = <span class="number">1</span>)</span><br><span class="line">A_row_sum = A_row_sum.reshape(A.shape[<span class="number">0</span>],<span class="number">1</span>)</span><br><span class="line">A_normalized = A / A_row_sum</span><br><span class="line"><span class="comment">## re-normalized</span></span><br><span class="line">A_normalized += <span class="number">1e-10</span></span><br><span class="line">A_row_sum = np.sum(A_normalized, axis = <span class="number">1</span>)</span><br><span class="line">A_row_sum = A_row_sum.reshape(A_normalized.shape[<span class="number">0</span>],<span class="number">1</span>)</span><br><span class="line">A_normalized = A_normalized / A_row_sum</span><br></pre></td></tr></table></figure>
<h4 id="State-Observation-Likelihood-Matrix-B"><a href="#State-Observation-Likelihood-Matrix-B" class="headerlink" title="State Observation Likelihood Matrix: B"></a>State Observation Likelihood Matrix: B</h4><ul>
<li>The row of matrix B is the tags generated, the column of matrix B is the words in our trainnign set.  </li>
<li>In order to reduce the effect of the zero values in this matrix, all elements in matrix B was added by a small value 1e-10 and then re-normalized.  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">B_row = len(tags_hash)</span><br><span class="line">B_col = len(words_hash)</span><br><span class="line">B = np.zeros((B_row, B_col))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(sents_list)):</span><br><span class="line">    row_index = tags_hash[sents_list[i][<span class="number">1</span>]]</span><br><span class="line">    col_index = words_hash[sents_list[i][<span class="number">0</span>]]</span><br><span class="line">    B[row_index][col_index] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">B_col_sum = np.sum(B, axis = <span class="number">0</span>)</span><br><span class="line">B_col_sum = B_col_sum.reshape(<span class="number">1</span>, B_col)</span><br><span class="line">B_normalized = B / B_col_sum</span><br><span class="line"></span><br><span class="line"><span class="comment">## re-normalized</span></span><br><span class="line">B_normalized += <span class="number">1e-10</span></span><br><span class="line">B_col_sum = np.sum(B_normalized, axis = <span class="number">0</span>)</span><br><span class="line">B_col_sum = B_col_sum.reshape(<span class="number">1</span>, B_col)</span><br><span class="line">B_normalized = B_normalized / B_col_sum</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="Get-a-test-case"><a href="#Get-a-test-case" class="headerlink" title="Get a test case"></a>Get a test case</h4><p>In this post, we use a sentence from ‘alice’<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.tokenize <span class="keyword">import</span> word_tokenize</span><br><span class="line">test_case = nltk.corpus.gutenberg.raw(<span class="string">'carroll-alice.txt'</span>)[<span class="number">91</span>:<span class="number">683</span>]</span><br><span class="line">words_test = word_tokenize(test_case)</span><br></pre></td></tr></table></figure></p>
<h4 id="Forward-Algorithm-1"><a href="#Forward-Algorithm-1" class="headerlink" title="Forward Algorithm"></a>Forward Algorithm</h4><p>The cell below is the implementation of forward algorithm. The matrix ‘forward’ is the porbablity of states in the test sequence.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">num_state = len(tags_hash)</span><br><span class="line">num_obv = len(words_test)</span><br><span class="line">forward = np.zeros((num_obv, num_state))</span><br><span class="line">start_state = tags_hash[<span class="string">'START_STATE'</span>]</span><br><span class="line">end_state = tags_hash[<span class="string">'END_STATE'</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    word_index = words_hash[words_test[<span class="number">0</span>]]</span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> range(num_state):</span><br><span class="line">        forward[<span class="number">0</span>][y] = A_normalized[start_state][y] * B_normalized[y][word_index]</span><br><span class="line">        </span><br><span class="line"><span class="keyword">except</span> KeyError:</span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> range(num_state):</span><br><span class="line">        forward[<span class="number">0</span>][y] = A_normalized[start_state][y] * (<span class="number">1</span>/num_state)</span><br><span class="line">        </span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">1</span>, num_obv):</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        word_index = words_hash[words_test[t]]</span><br><span class="line">        <span class="keyword">for</span> y <span class="keyword">in</span> range(num_state):</span><br><span class="line">            <span class="keyword">for</span> y_pre <span class="keyword">in</span> range(num_state):</span><br><span class="line">                forward[t][y] += forward[t<span class="number">-1</span>][y_pre] * A_normalized[y_pre][y] * B_normalized[y][word_index]</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">except</span> KeyError:</span><br><span class="line">        <span class="keyword">for</span> y <span class="keyword">in</span> range(num_state):</span><br><span class="line">            <span class="keyword">for</span> y_pre <span class="keyword">in</span> range(num_state):</span><br><span class="line">                forward[t][y] += forward[t<span class="number">-1</span>][y_pre] * A_normalized[y_pre][y] * (<span class="number">1</span>/num_state)</span><br></pre></td></tr></table></figure></p>
<h4 id="Decoding-of-Forward-Algorithm"><a href="#Decoding-of-Forward-Algorithm" class="headerlink" title="Decoding of Forward Algorithm"></a>Decoding of Forward Algorithm</h4><p>For a given forward matrix, the state in a step can be predicted by finding a state with largest probability in this step.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">forward_output = []</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(num_obv):</span><br><span class="line">    mx = <span class="number">-1</span></span><br><span class="line">    tag_id = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> index, tags <span class="keyword">in</span> enumerate(forward[t]):</span><br><span class="line">        <span class="keyword">if</span> mx &lt; forward[t][index]:</span><br><span class="line">            mx = forward[t][index]</span><br><span class="line">            tag_id = index</span><br><span class="line">    forward_output.append((words_test[t], tags_list[tag_id]))</span><br></pre></td></tr></table></figure></p>
<h4 id="The-result-of-forward-algorithm-is-shown-as-belowed"><a href="#The-result-of-forward-algorithm-is-shown-as-belowed" class="headerlink" title="The result of forward algorithm is shown as belowed."></a>The result of forward algorithm is shown as belowed.</h4><p>Because the tags are compressed, all the tags in the results are without profix.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> (<span class="string">"The result of test case from forward algorithms is as belowed"</span>)</span><br><span class="line">print(forward_output)</span><br></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">The result of test case from forward algorithms is as belowed</span><br><span class="line">[(&apos;Alice&apos;, &apos;NP&apos;), (&apos;was&apos;, &apos;BEDZ&apos;), (&apos;beginning&apos;, &apos;NN&apos;), (&apos;to&apos;, &apos;IN&apos;), (&apos;get&apos;, &apos;VB&apos;), (&apos;very&apos;, &apos;QL&apos;), (&apos;tired&apos;, &apos;VBN&apos;), (&apos;of&apos;, &apos;IN&apos;), (&apos;sitting&apos;, &apos;VBG&apos;), (&apos;by&apos;, &apos;IN&apos;), (&apos;her&apos;, &apos;PP$&apos;), (&apos;sister&apos;, &apos;NN&apos;), (&apos;on&apos;, &apos;IN&apos;), (&apos;the&apos;, &apos;AT&apos;), (&apos;bank&apos;, &apos;NN&apos;), (&apos;,&apos;, &apos;,&apos;), (&apos;and&apos;, &apos;CC&apos;), (&apos;of&apos;, &apos;IN&apos;), (&apos;having&apos;, &apos;HVG&apos;), (&apos;nothing&apos;, &apos;PN&apos;), (&apos;to&apos;, &apos;IN&apos;), (&apos;do&apos;, &apos;DO&apos;), (&apos;:&apos;, &apos;:&apos;), (&apos;once&apos;, &apos;RB&apos;), (&apos;or&apos;, &apos;CC&apos;), (&apos;twice&apos;, &apos;RB&apos;), (&apos;she&apos;, &apos;PPS&apos;), (&apos;had&apos;, &apos;HVD&apos;), (&apos;peeped&apos;, &apos;VBN&apos;), (&apos;into&apos;, &apos;IN&apos;), (&apos;the&apos;, &apos;AT&apos;), (&apos;book&apos;, &apos;NN&apos;), (&apos;her&apos;, &apos;PP$&apos;), (&apos;sister&apos;, &apos;NN&apos;), (&apos;was&apos;, &apos;BEDZ&apos;), (&apos;reading&apos;, &apos;VBG&apos;), (&apos;,&apos;, &apos;,&apos;), (&apos;but&apos;, &apos;CC&apos;), (&apos;it&apos;, &apos;PPS&apos;), (&apos;had&apos;, &apos;HVD&apos;), (&apos;no&apos;, &apos;AT&apos;), (&apos;pictures&apos;, &apos;NNS&apos;), (&apos;or&apos;, &apos;CC&apos;), (&apos;conversations&apos;, &apos;NNS&apos;), (&apos;in&apos;, &apos;IN&apos;), (&apos;it&apos;, &apos;PPO&apos;), (&apos;,&apos;, &apos;,&apos;), (&quot;&apos;and&quot;, &apos;NP&apos;), (&apos;what&apos;, &apos;WDT&apos;), (&apos;is&apos;, &apos;BEZ&apos;), (&apos;the&apos;, &apos;AT&apos;), (&apos;use&apos;, &apos;NN&apos;), (&apos;of&apos;, &apos;IN&apos;), (&apos;a&apos;, &apos;AT&apos;), (&apos;book&apos;, &apos;NN&apos;), (&apos;,&apos;, &apos;,&apos;), (&quot;&apos;&quot;, &quot;&apos;&quot;), (&apos;thought&apos;, &apos;VBN&apos;), (&apos;Alice&apos;, &apos;NP&apos;), (&quot;&apos;without&quot;, &apos;NP&apos;), (&apos;pictures&apos;, &apos;NNS&apos;), (&apos;or&apos;, &apos;CC&apos;), (&apos;conversation&apos;, &apos;NN&apos;), (&apos;?&apos;, &apos;.&apos;), (&quot;&apos;&quot;, &quot;&apos;&quot;), (&apos;So&apos;, &apos;RB&apos;), (&apos;she&apos;, &apos;PPS&apos;), (&apos;was&apos;, &apos;BEDZ&apos;), (&apos;considering&apos;, &apos;VBG&apos;), (&apos;in&apos;, &apos;IN&apos;), (&apos;her&apos;, &apos;PP$&apos;), (&apos;own&apos;, &apos;JJ&apos;), (&apos;mind&apos;, &apos;NN&apos;), (&apos;(&apos;, &apos;(&apos;), (&apos;as&apos;, &apos;CS&apos;), (&apos;well&apos;, &apos;RB&apos;), (&apos;as&apos;, &apos;CS&apos;), (&apos;she&apos;, &apos;PPS&apos;), (&apos;could&apos;, &apos;MD&apos;), (&apos;,&apos;, &apos;,&apos;), (&apos;for&apos;, &apos;IN&apos;), (&apos;the&apos;, &apos;AT&apos;), (&apos;hot&apos;, &apos;JJ&apos;), (&apos;day&apos;, &apos;NN&apos;), (&apos;made&apos;, &apos;VBD&apos;), (&apos;her&apos;, &apos;PP$&apos;), (&apos;feel&apos;, &apos;NN&apos;), (&apos;very&apos;, &apos;QL&apos;), (&apos;sleepy&apos;, &apos;JJ&apos;), (&apos;and&apos;, &apos;CC&apos;), (&apos;stupid&apos;, &apos;NP&apos;), (&apos;)&apos;, &apos;)&apos;), (&apos;,&apos;, &apos;,&apos;), (&apos;whether&apos;, &apos;CS&apos;), (&apos;the&apos;, &apos;AT&apos;), (&apos;pleasure&apos;, &apos;NN&apos;), (&apos;of&apos;, &apos;IN&apos;), (&apos;making&apos;, &apos;VBG&apos;), (&apos;a&apos;, &apos;AT&apos;), (&apos;daisy-chain&apos;, &apos;NN&apos;), (&apos;would&apos;, &apos;MD&apos;), (&apos;be&apos;, &apos;BE&apos;), (&apos;worth&apos;, &apos;JJ&apos;), (&apos;the&apos;, &apos;AT&apos;), (&apos;trouble&apos;, &apos;NN&apos;), (&apos;of&apos;, &apos;IN&apos;), (&apos;getting&apos;, &apos;VBG&apos;), (&apos;up&apos;, &apos;RP&apos;), (&apos;and&apos;, &apos;CC&apos;), (&apos;picking&apos;, &apos;VBG&apos;), (&apos;the&apos;, &apos;AT&apos;), (&apos;daisies&apos;, &apos;NN&apos;), (&apos;,&apos;, &apos;,&apos;), (&apos;when&apos;, &apos;WRB&apos;), (&apos;suddenly&apos;, &apos;RB&apos;), (&apos;a&apos;, &apos;AT&apos;), (&apos;White&apos;, &apos;JJ&apos;), (&apos;Rabbit&apos;, &apos;NN&apos;), (&apos;with&apos;, &apos;IN&apos;), (&apos;pink&apos;, &apos;JJ&apos;), (&apos;eyes&apos;, &apos;NNS&apos;), (&apos;ran&apos;, &apos;VBD&apos;), (&apos;close&apos;, &apos;RB&apos;), (&apos;by&apos;, &apos;IN&apos;), (&apos;her&apos;, &apos;PP$&apos;), (&apos;.&apos;, &apos;.&apos;)]</span><br></pre></td></tr></table></figure>
<h3 id="Viterbi-Algorithm"><a href="#Viterbi-Algorithm" class="headerlink" title="Viterbi Algorithm"></a>Viterbi Algorithm</h3><ul>
<li>The implementation of viterbi algorithm is given as below.  </li>
<li>The Node class is a class with previous node and next node which can be used as a back pointer in viterbi algorithms.  </li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Node</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, state, observ, val, pre_node, next_node)</span>:</span></span><br><span class="line">        self.state = state</span><br><span class="line">        self.observ = observ</span><br><span class="line">        self.val = val</span><br><span class="line">        self.pre_node = pre_node</span><br><span class="line">        self.next_node = next_node</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.state + <span class="string">"--&gt;"</span> + self.observ + <span class="string">"--&gt;"</span> + str(self.val) + <span class="string">'\n'</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__lt__</span><span class="params">(self, other)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> (self.val &lt; other.val)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line">num_state = len(tags_hash)</span><br><span class="line">num_obv = len(words_test)</span><br><span class="line"><span class="comment">#viterbi = [[None for i in range(num_state)] for j in range(num_obv)]</span></span><br><span class="line">viterbi = []</span><br><span class="line">path = []</span><br><span class="line"></span><br><span class="line">start_state = tags_hash[<span class="string">'START_STATE'</span>]</span><br><span class="line">end_state = tags_hash[<span class="string">'END_STATE'</span>]</span><br><span class="line"></span><br><span class="line">start_node = Node(<span class="string">"START_STATE"</span>, <span class="string">""</span>, <span class="number">0</span>, <span class="keyword">None</span>, <span class="keyword">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    word_index = words_hash[words_test[<span class="number">0</span>]]</span><br><span class="line">    curr_step = []</span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> range(<span class="number">1</span>, num_state<span class="number">-1</span>):</span><br><span class="line">        val = A_normalized[start_state][y] * B_normalized[y][word_index]</span><br><span class="line">        curr_node = Node(tags_list[y], words_test[<span class="number">0</span>], val, start_node, <span class="keyword">None</span>)</span><br><span class="line">        <span class="comment">#print(curr_node)</span></span><br><span class="line">        curr_step.append(curr_node)</span><br><span class="line">    viterbi.append(curr_step)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">except</span> KeyError:</span><br><span class="line">    curr_step = []</span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> range(<span class="number">1</span>, num_state<span class="number">-1</span>):</span><br><span class="line">        val = A_normalized[start_state][y]  * (<span class="number">1</span>/(num_state<span class="number">-2</span>))</span><br><span class="line">        curr_node = Node(tags_list[y], words_test[<span class="number">0</span>], val, start_node, <span class="keyword">None</span>)</span><br><span class="line">        curr_step.append(curr_node)</span><br><span class="line">    viterbi.append(curr_step)</span><br><span class="line"></span><br><span class="line"><span class="comment">## </span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">1</span>, num_obv):</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        word_index = words_hash[words_test[t]]</span><br><span class="line">        curr_step = []</span><br><span class="line">        <span class="keyword">for</span> y <span class="keyword">in</span> range(<span class="number">1</span>,num_state<span class="number">-1</span>):</span><br><span class="line">            val = <span class="number">-1</span></span><br><span class="line">            prenode = <span class="keyword">None</span></span><br><span class="line">            <span class="keyword">for</span> y_pre <span class="keyword">in</span> range(<span class="number">1</span>,num_state<span class="number">-1</span>):</span><br><span class="line">                temp_val = viterbi[t<span class="number">-1</span>][y_pre<span class="number">-1</span>].val * A_normalized[y_pre][y] * B_normalized[y][word_index]</span><br><span class="line">                <span class="keyword">if</span> val &lt; temp_val :</span><br><span class="line">                    val = temp_val</span><br><span class="line">                    prenode = viterbi[t<span class="number">-1</span>][y_pre<span class="number">-1</span>]</span><br><span class="line">            curr_node = Node(tags_list[y], words_test[t], val, prenode, <span class="keyword">None</span>)</span><br><span class="line">            curr_step.append(curr_node)</span><br><span class="line">        viterbi.append(curr_step)</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">except</span> KeyError:</span><br><span class="line">        curr_step = []</span><br><span class="line">        <span class="keyword">for</span> y <span class="keyword">in</span> range(<span class="number">1</span>, num_state<span class="number">-1</span>):</span><br><span class="line">            val = <span class="number">-1</span></span><br><span class="line">            prenode = <span class="keyword">None</span></span><br><span class="line">            <span class="keyword">for</span> y_pre <span class="keyword">in</span> range(<span class="number">1</span>, num_state<span class="number">-1</span>):</span><br><span class="line">                temp_val = viterbi[t<span class="number">-1</span>][y_pre<span class="number">-1</span>].val * A_normalized[y_pre][y] * (<span class="number">1</span>/(num_state<span class="number">-2</span>))</span><br><span class="line">                <span class="keyword">if</span> val &lt; temp_val :</span><br><span class="line">                    val = temp_val</span><br><span class="line">                    prenode = viterbi[t<span class="number">-1</span>][y_pre<span class="number">-1</span>]</span><br><span class="line">            curr_node = Node(tags_list[y], words_test[t], val, prenode, <span class="keyword">None</span>)</span><br><span class="line">            curr_step.append(curr_node)</span><br><span class="line">        viterbi.append(curr_step)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span>(len(viterbi) == num_obv)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">val = <span class="number">-1</span></span><br><span class="line">prenode = <span class="keyword">None</span></span><br><span class="line"><span class="keyword">for</span> y_pre <span class="keyword">in</span> range(<span class="number">1</span>, num_state<span class="number">-1</span>):</span><br><span class="line">    temp_val = viterbi[num_obv<span class="number">-1</span>][y_pre<span class="number">-1</span>].val * A_normalized[y_pre][end_state]</span><br><span class="line">    <span class="keyword">if</span> val &lt; temp_val:</span><br><span class="line">        val = temp_val</span><br><span class="line">        prenode = viterbi[num_obv<span class="number">-1</span>][y_pre<span class="number">-1</span>]</span><br><span class="line">end_node = Node(<span class="string">"END_STATE"</span>, <span class="string">""</span>, val, prenode, <span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<h4 id="The-result-of-viterbi-algorithm-is-shown-as-belowed"><a href="#The-result-of-viterbi-algorithm-is-shown-as-belowed" class="headerlink" title="The result of viterbi algorithm is shown as belowed"></a>The result of viterbi algorithm is shown as belowed</h4><p>Because the tags are compressed, all the tags in the results are without profix.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">node = end_node</span><br><span class="line">outlist = []</span><br><span class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">    <span class="keyword">if</span> (node <span class="keyword">is</span> <span class="keyword">None</span>):</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    outlist.append((node.observ, node.state))</span><br><span class="line">    node = node.pre_node</span><br><span class="line"></span><br><span class="line">outlist.reverse()</span><br><span class="line">print(<span class="string">"The result of test case from Viterbi Algorithms is as belowed"</span>)</span><br><span class="line">print(outlist[<span class="number">1</span>:len(outlist)<span class="number">-1</span>])</span><br></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">The result of test case from forward algorithms is as belowed</span><br><span class="line">[(&apos;Alice&apos;, &apos;NP&apos;), (&apos;was&apos;, &apos;BEDZ&apos;), (&apos;beginning&apos;, &apos;NN&apos;), (&apos;to&apos;, &apos;TO&apos;), (&apos;get&apos;, &apos;VB&apos;), (&apos;very&apos;, &apos;QL&apos;), (&apos;tired&apos;, &apos;VBN&apos;), (&apos;of&apos;, &apos;IN&apos;), (&apos;sitting&apos;, &apos;VBG&apos;), (&apos;by&apos;, &apos;IN&apos;), (&apos;her&apos;, &apos;PP$&apos;), (&apos;sister&apos;, &apos;NN&apos;), (&apos;on&apos;, &apos;IN&apos;), (&apos;the&apos;, &apos;AT&apos;), (&apos;bank&apos;, &apos;NN&apos;), (&apos;,&apos;, &apos;,&apos;), (&apos;and&apos;, &apos;CC&apos;), (&apos;of&apos;, &apos;IN&apos;), (&apos;having&apos;, &apos;HVG&apos;), (&apos;nothing&apos;, &apos;PN&apos;), (&apos;to&apos;, &apos;TO&apos;), (&apos;do&apos;, &apos;DO&apos;), (&apos;:&apos;, &apos;*&apos;), (&apos;once&apos;, &apos;RB&apos;), (&apos;or&apos;, &apos;CC&apos;), (&apos;twice&apos;, &apos;RB&apos;), (&apos;she&apos;, &apos;PPS&apos;), (&apos;had&apos;, &apos;HVD&apos;), (&apos;peeped&apos;, &apos;VBN&apos;), (&apos;into&apos;, &apos;IN&apos;), (&apos;the&apos;, &apos;AT&apos;), (&apos;book&apos;, &apos;NN&apos;), (&apos;her&apos;, &apos;PP$&apos;), (&apos;sister&apos;, &apos;NN&apos;), (&apos;was&apos;, &apos;BEDZ&apos;), (&apos;reading&apos;, &apos;NN&apos;), (&apos;,&apos;, &apos;,&apos;), (&apos;but&apos;, &apos;CC&apos;), (&apos;it&apos;, &apos;PPS&apos;), (&apos;had&apos;, &apos;HVD&apos;), (&apos;no&apos;, &apos;AT&apos;), (&apos;pictures&apos;, &apos;NNS&apos;), (&apos;or&apos;, &apos;CC&apos;), (&apos;conversations&apos;, &apos;NNS&apos;), (&apos;in&apos;, &apos;IN&apos;), (&apos;it&apos;, &apos;PPO&apos;), (&apos;,&apos;, &apos;,&apos;), (&quot;&apos;and&quot;, &apos;IN&apos;), (&apos;what&apos;, &apos;WDT&apos;), (&apos;is&apos;, &apos;BEZ&apos;), (&apos;the&apos;, &apos;AT&apos;), (&apos;use&apos;, &apos;NN&apos;), (&apos;of&apos;, &apos;IN&apos;), (&apos;a&apos;, &apos;AT&apos;), (&apos;book&apos;, &apos;NN&apos;), (&apos;,&apos;, &apos;,&apos;), (&quot;&apos;&quot;, &quot;&apos;&quot;), (&apos;thought&apos;, &apos;VBN&apos;), (&apos;Alice&apos;, &apos;NP&apos;), (&quot;&apos;without&quot;, &apos;CD&apos;), (&apos;pictures&apos;, &apos;NNS&apos;), (&apos;or&apos;, &apos;CC&apos;), (&apos;conversation&apos;, &apos;NN&apos;), (&apos;?&apos;, &apos;.&apos;), (&quot;&apos;&quot;, &quot;&apos;&quot;), (&apos;So&apos;, &apos;RB&apos;), (&apos;she&apos;, &apos;PPS&apos;), (&apos;was&apos;, &apos;BEDZ&apos;), (&apos;considering&apos;, &apos;VBG&apos;), (&apos;in&apos;, &apos;IN&apos;), (&apos;her&apos;, &apos;PP$&apos;), (&apos;own&apos;, &apos;JJ&apos;), (&apos;mind&apos;, &apos;NN&apos;), (&apos;(&apos;, &apos;(&apos;), (&apos;as&apos;, &apos;CS&apos;), (&apos;well&apos;, &apos;RB&apos;), (&apos;as&apos;, &apos;CS&apos;), (&apos;she&apos;, &apos;PPS&apos;), (&apos;could&apos;, &apos;MD&apos;), (&apos;,&apos;, &apos;,&apos;), (&apos;for&apos;, &apos;IN&apos;), (&apos;the&apos;, &apos;AT&apos;), (&apos;hot&apos;, &apos;JJ&apos;), (&apos;day&apos;, &apos;NN&apos;), (&apos;made&apos;, &apos;VBD&apos;), (&apos;her&apos;, &apos;PP$&apos;), (&apos;feel&apos;, &apos;NN&apos;), (&apos;very&apos;, &apos;QL&apos;), (&apos;sleepy&apos;, &apos;JJ&apos;), (&apos;and&apos;, &apos;CC&apos;), (&apos;stupid&apos;, &apos;NP&apos;), (&apos;)&apos;, &apos;)&apos;), (&apos;,&apos;, &apos;,&apos;), (&apos;whether&apos;, &apos;CS&apos;), (&apos;the&apos;, &apos;AT&apos;), (&apos;pleasure&apos;, &apos;NN&apos;), (&apos;of&apos;, &apos;IN&apos;), (&apos;making&apos;, &apos;VBG&apos;), (&apos;a&apos;, &apos;AT&apos;), (&apos;daisy-chain&apos;, &apos;NN&apos;), (&apos;would&apos;, &apos;MD&apos;), (&apos;be&apos;, &apos;BE&apos;), (&apos;worth&apos;, &apos;JJ&apos;), (&apos;the&apos;, &apos;AT&apos;), (&apos;trouble&apos;, &apos;NN&apos;), (&apos;of&apos;, &apos;IN&apos;), (&apos;getting&apos;, &apos;VBG&apos;), (&apos;up&apos;, &apos;RP&apos;), (&apos;and&apos;, &apos;CC&apos;), (&apos;picking&apos;, &apos;VBG&apos;), (&apos;the&apos;, &apos;AT&apos;), (&apos;daisies&apos;, &apos;NN&apos;), (&apos;,&apos;, &apos;,&apos;), (&apos;when&apos;, &apos;WRB&apos;), (&apos;suddenly&apos;, &apos;RB&apos;), (&apos;a&apos;, &apos;AT&apos;), (&apos;White&apos;, &apos;JJ&apos;), (&apos;Rabbit&apos;, &apos;NN&apos;), (&apos;with&apos;, &apos;IN&apos;), (&apos;pink&apos;, &apos;JJ&apos;), (&apos;eyes&apos;, &apos;NNS&apos;), (&apos;ran&apos;, &apos;VBD&apos;), (&apos;close&apos;, &apos;RB&apos;), (&apos;by&apos;, &apos;IN&apos;), (&apos;her&apos;, &apos;PPO&apos;), (&apos;.&apos;, &apos;.&apos;)]</span><br></pre></td></tr></table></figure>

    </article>
    <!-- 前后页  -->
    <ul class="post-pager">
        
            <li class="next">
                <a href= "/2017/10/09/Word-Embedding-Syntactics-or-Semantics/" title= Word Embedding: Syntactics or Semantics >
                    <span>Next Post</span>
                    <span>Word Embedding: Syntactics or Semantics</span>
                </a>
            </li>
        
        
            <li class="previous">
                <a href= "/2017/05/02/cat-and-dog/" title= cat-and-dog >
                    <span>Previous Post</span>
                    <span>cat-and-dog</span>
                </a>
            </li>
        
    </ul>
    <!-- 评论插件 -->
    <!-- 来必力City版安装代码 -->

<!-- City版安装代码已完成 -->
    
    
<div id="container"></div>
<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script>
    var gitment = new Gitment({
        id: "Hidden Markov Model for Part of Speech Tagging", // 可选。默认为 location.href
        owner: 'wushbin',
        repo: 'wushbin.github.io',
        oauth: {
            client_id: '17eb7cee5fa4bc08355b',
            client_secret: '176f2991889fafe11f2ae7670c5f1dbe879ce0eb',
        },
    })
    gitment.render('container')

</script>

    <!--PC版-->

    <!--PC版-->


    
</main>
            <!-- profile -->
            
        </div>
        <footer class="footer footer-unloaded">
    <!-- social  -->
    
    <div class="social">
        
    
        
            
                <a href="mailto:ngshbin@gmail.com" class="iconfont-archer email" title=email ></a>
            
        
    
        
            
                <a href="//github.com/wushbin" class="iconfont-archer github" target="_blank" title="github"></a>
            
        
    
        
    
        
            
                <a href="https://www.linkedin.com/in/shengbin-wu/" class="iconfont-archer linkedin" target="_blank" title="linkedin"></a>
            
        
    
        
    

    </div>
    
    <!-- powered by Hexo  -->
    <div class="copyright">
        <span id="hexo-power">Powered by <a href="https://hexo.io/" target="_blank">Hexo</a></span><span class="iconfont-archer power">&#xe635;</span><span id="theme-info">Theme <a href="https://github.com/fi3ework/hexo-theme-archer" target="_blank">archer</a></span>
    </div>
    <!-- 不蒜子  -->
    
</footer>
    </div>
    <!-- toc -->
    
    <div class="toc-wrapper">
        <div class="toc-catalog">
            <span class="iconfont-archer catalog-icon">&#xe613;</span><span>CATALOG</span>
        </div>
        <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Introduction"><span class="toc-number">1.</span> <span class="toc-text">Introduction</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Extract-data"><span class="toc-number">1.1.</span> <span class="toc-text">Extract data</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Compress-the-tags"><span class="toc-number">1.1.1.</span> <span class="toc-text">Compress the tags</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Generate-a-set-of-tags-and-a-set-of-words"><span class="toc-number">1.2.</span> <span class="toc-text">Generate a set of tags and a set of words</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Forward-Algorithm"><span class="toc-number">2.</span> <span class="toc-text">Forward Algorithm</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Transition-Probability-Matrix-A"><span class="toc-number">2.1.</span> <span class="toc-text">Transition Probability Matrix: A</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#State-Observation-Likelihood-Matrix-B"><span class="toc-number">2.2.</span> <span class="toc-text">State Observation Likelihood Matrix: B</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Get-a-test-case"><span class="toc-number">2.3.</span> <span class="toc-text">Get a test case</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Forward-Algorithm-1"><span class="toc-number">2.4.</span> <span class="toc-text">Forward Algorithm</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Decoding-of-Forward-Algorithm"><span class="toc-number">2.5.</span> <span class="toc-text">Decoding of Forward Algorithm</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#The-result-of-forward-algorithm-is-shown-as-belowed"><span class="toc-number">2.6.</span> <span class="toc-text">The result of forward algorithm is shown as belowed.</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Viterbi-Algorithm"><span class="toc-number">3.</span> <span class="toc-text">Viterbi Algorithm</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#The-result-of-viterbi-algorithm-is-shown-as-belowed"><span class="toc-number">3.1.</span> <span class="toc-text">The result of viterbi algorithm is shown as belowed</span></a></li></ol></li></ol>
    </div>
    
    <div class="back-top">&#xe639;</div>
    <div class="sidebar">
    <div class="sidebar-header sidebar-header-show-archive">
        <div class="sidebar-category">
            <span class="sidebar-archive-link"><span class="iconfont-archer">&#xe67d;</span>Archive</span>
            <span class="sidebar-tags-link"><span class="iconfont-archer">&#xe610;</span>Tag</span>
        </div>
    </div>
    <div class="sidebar-content sidebar-content-show-archive">
          <div class="sidebar-archive">
    <!-- 在ejs中将archive按照时间排序 -->
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <div class="total-archive"> Total : 11 </div>
    
    <div class="post-archive">
    
    
    
    
    <div class="archive-year"> 2018 </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/01</span><a class="archive-post-title" href= "/2018/02/01/Deep-Neural-NetWork-Sorting/" >Deep Neural NetWork Sorting</a>
        </li>
    
    
    
    
    
        </ul>
    
    <div class="archive-year"> 2017 </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">12/23</span><a class="archive-post-title" href= "/2017/12/23/PacMan-Game-Artificial-Intellegence/" >PacMan Game: Artificial Intellegence</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">12/23</span><a class="archive-post-title" href= "/2017/12/23/Speech-Recognition/" >Speech Recognition</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">12/23</span><a class="archive-post-title" href= "/2017/12/23/Optical-Character-Recognition/" >Optical Character Recognition</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/12</span><a class="archive-post-title" href= "/2017/10/12/Sharing-On-Campus-Web-Application/" >Sharing On Campus: Web Application</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/09</span><a class="archive-post-title" href= "/2017/10/09/Word-Embedding-Syntactics-or-Semantics/" >Word Embedding: Syntactics or Semantics</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/23</span><a class="archive-post-title" href= "/2017/09/23/Hidden-Markov-Model-for-Part-of-Speech-Tagging/" >Hidden Markov Model for Part of Speech Tagging</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/02</span><a class="archive-post-title" href= "/2017/05/02/cat-and-dog/" >cat-and-dog</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/01</span><a class="archive-post-title" href= "/2017/05/01/Duke-Gather-Android-Application/" >Duke Gather: Android Application</a>
        </li>
    
    
    
    
    
        </ul>
    
    <div class="archive-year"> 2016 </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/01</span><a class="archive-post-title" href= "/2016/06/01/High-Speed-Test-Rig-Development/" >High Speed Test Rig Development</a>
        </li>
    
    
    
    
    
        </ul>
    
    <div class="archive-year"> 2013 </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/01</span><a class="archive-post-title" href= "/2013/06/01/SCARA-Robot/" >SCARA Robot</a>
        </li>
    
    </div>
  </div>
        <div class="sidebar-tags">
    <div class="sidebar-tags-name">
    
        <span class="sidebar-tag-name"><a href= "#">machine learning</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">Java</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">Mobile Application</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">natural language processing</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">mechanical design</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">experiment</a></span>
    
    </div>
    <div class="tag-load-fail" style="display: none; color: #ccc; font-size: 0.6rem;">
    缺失模块。<br/>
    1、请确保node版本大于6.2<br/>
    2、在博客根目录（注意不是archer根目录）执行以下命令：<br/>
    <span style="color: #f75357; font-size: 1rem; line-height: 2rem;">npm i hexo-generator-json-content --save</span><br/>
    3、在根目录_config.yml里添加配置：
    <pre style="color: #787878; font-size: 0.6rem;">
jsonContent:
  meta: false
  pages: false
  posts:
    title: true
    date: true
    path: true
    text: false
    raw: false
    content: false
    slug: false
    updated: false
    comments: false
    link: false
    permalink: false
    excerpt: false
    categories: false
    tags: true</pre>
    </div> 
    <div class="sidebar-tag-list"></div>
</div>
    </div>
</div> 
    <script>
    var jsInfo = {
        root: '/'
    }
</script>
    <!-- 不蒜子  -->
    
    <!-- CNZZ统计  -->
    
    </div>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>

<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

</body>
</html>


