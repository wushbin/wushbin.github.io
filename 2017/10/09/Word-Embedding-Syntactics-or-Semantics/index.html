<!DOCTYPE html>
<html>
    <head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" >
    <title>
        
        Word Embedding: Syntactics or Semantics · Shengbin&#39;s Studio
        
    </title>
    <link rel="icon" href= /assets/favicon2.ico>
    <!-- TODO: 在font-face加载完毕后改变字体  -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/webfont/1.6.28/webfontloader.js"></script>
    <!-- 提前加载place holder  -->
    <style type="text/css">
        @font-face {
            font-family: 'Oswald-Regular';
            src: url(/font/Oswald-Regular.ttf);
        }
    </style>
    <style type="text/css">
        .site-intro {
            position: relative;
            width: 100%;
            height: 50vh;
            overflow: hidden;
            box-shadow: -0.1rem 0 0.5rem 0 rgba(0, 0, 0, 0.8);
        }
        .site-intro-placeholder {
            position: absolute;
            z-index: -2;
            top: 0;
            left: 0px;
            width: calc(100% + 300px);
            height: 100%;
            background: repeating-linear-gradient(-45deg, #444 0, #444 80px, #333 80px, #333 160px);
            background-position: center center;
            transform: translate3d(-226px, 0, 0);
            animation: gradient-move 2.5s ease-out 0s 1;
        }
        @keyframes gradient-move {
        0% {
            transform: translate3d(-226px, 0, 0);
        }
        100% {
            transform: translate3d(0, 0, 0);
        }
    }
</style>
    <link rel="stylesheet" href = /css/style.css?v=20171218 />
    <script src="//cdn.staticfile.org/jquery/3.2.1/jquery.min.js" defer></script>
    
    <script src="/scripts/main.js" defer></script>
    <!-- 百度统计  -->
    
    <!-- 谷歌统计  -->
    
</head>
    
        <body class="post-body">
    
    
<header class="header">

    <div class="read-progress"></div>
    <div class="header-sidebar-menu">&#xe775;</div>
    <!-- post页的toggle banner  -->
    
    <div class="banner">
            <div class="blog-title">
                <a href="/" >Shengbin&#39;s Studio.</a>
            </div>
            <div class="post-title">
                <a href="#" class="post-name">Word Embedding: Syntactics or Semantics</a>
            </div>
    </div>
    
    <a class="home-link" href=/>Shengbin's Studio.</a>
</header>
    <div class="wrapper">
        <div class="site-intro">
    
    <!-- 主页  -->
    
    
    <!-- 404页  -->
            
    <div class="site-intro-img" style="background-image: url(http://oumn0o088.bkt.clouddn.com/post-bg.jpg)"></div>
    <div class="site-intro-placeholder"></div>
    <div class="site-intro-meta">
        <!-- 标题  -->
        <h1 class="intro-title">
            <!-- 主页  -->
            
            Word Embedding: Syntactics or Semantics
            <!-- 404 -->
            
        </h1>
        <!-- 副标题 -->
        <p class="intro-subtitle">
            <!-- 主页副标题  -->
            
            
            <!-- 404 -->
            
        </p>
        <!-- 文章页meta -->
        
            <!-- 文章页标签  -->
            
                <div class= post-intro-tags >
    
        <a class="post-tag" href="javascript:void(0);" data-href = machine learning>machine learning</a>
    
        <a class="post-tag" href="javascript:void(0);" data-href = natural language processing>natural language processing</a>
    
</div>
            
            <div class="post-intro-meta">
                <span class="post-intro-calander iconfont-archer">&#xe676;</span>
                <span class="post-intro-time">2017/10/09</span>
            </div>
        
    </div>
</div>
        <img class="loading" src="/assets/loading.svg" style="display: block; margin: 6rem auto 0 auto; width: 6rem; height: 6rem;" />
        <script>
            var browser = {
                    versions: function () {
                        var u = window.navigator.userAgent;
                        return {
                            userAgent: u,
                            trident: u.indexOf('Trident') > -1, //IE内核
                            presto: u.indexOf('Presto') > -1, //opera内核
                            webKit: u.indexOf('AppleWebKit') > -1, //苹果、谷歌内核
                            gecko: u.indexOf('Gecko') > -1 && u.indexOf('KHTML') == -1, //火狐内核
                            mobile: !!u.match(/AppleWebKit.*Mobile.*/), //是否为移动终端
                            ios: !!u.match(/\(i[^;]+;( U;)? CPU.+Mac OS X/), //ios终端
                            android: u.indexOf('Android') > -1 || u.indexOf('Linux') > -1, //android终端或者uc浏览器
                            iPhone: u.indexOf('iPhone') > -1 || u.indexOf('Mac') > -1, //是否为iPhone或者安卓QQ浏览器
                            iPad: u.indexOf('iPad') > -1, //是否为iPad
                            webApp: u.indexOf('Safari') == -1, //是否为web应用程序，没有头部与底部
                            weixin: u.indexOf('MicroMessenger') == -1, //是否为微信浏览器
                            uc: u.indexOf('UCBrowser') > -1 //是否为android下的UC浏览器
                        };
                    }()
                }

            function fontLoaded(){
                console.log('font loaded');
                if (document.getElementsByClassName('site-intro-meta')) {
                    document.getElementsByClassName('intro-title')[0].classList.add('intro-fade-in');
                    document.getElementsByClassName('intro-subtitle')[0].classList.add('intro-fade-in');
                    var postIntroTags = document.getElementsByClassName('post-intro-tags')[0],
                        postIntroMeat = document.getElementsByClassName('post-intro-meta')[0];
                        if (postIntroTags) {
                            postIntroTags.classList.add('post-fade-in');
                        }
                        if (postIntroMeat) {
                            postIntroMeat.classList.add('post-fade-in');
                        }
                    }
                }
                
            console.log("userAgent:" + browser.versions.userAgent);
            // UC不支持跨域，所以直接显示
            if (browser.versions.uc) {
                console.log("UCBrowser");
                fontLoaded();
            } else {
                WebFont.load({
                    custom: {
                        families: ['Oswald-Regular']
                    },
                    loading: function () {  //所有字体开始加载
                        // console.log('loading');
                    },
                    active: function () {  //所有字体已渲染
                        fontLoaded();
                    },
                    inactive: function () { //字体预加载失败，无效字体或浏览器不支持加载
                        console.log('inactive: timeout');
                        fontLoaded();
                    },
                    timeout: 7000 // Set the timeout to two seconds
                });
            }
        </script>
        <div class="container container-unloaded">
            <main class="main post-page">
    <article class="article-entry">
        <h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>Word embedding is a mapping from words to vector representations of the words. The method that represents a word as a vector or array of numbers related in someway to counts is called vector semantics. Ideally, the geometry of the vectors will capture the semantic and syntactic meaning of the words—for example, words similar in meaning should have representations which are close to each other in the vector space.<br>In this project, a word co-occurrence matrix of 10,000 words from Wikipedia corpus with 1.5 billion words will be used to study the two method learned from class. Entry of the matrix denotes the number of times in the corpus this the ith and jth words occur within 5 words of each other The data file in this project is downloaded from Standford CS168 course website. The dictionary in this data set is sorted by frequency, thus the first row of the co-occurrence matrix is the most frequent word.<br><a href="http://web.stanford.edu/class/cs168/co_occur.csv" target="_blank" rel="noopener">data file</a></p>
<h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><h4 id="Sparse-Vector-Representation"><a href="#Sparse-Vector-Representation" class="headerlink" title="Sparse Vector Representation"></a>Sparse Vector Representation</h4><p>The co-occurrence matrix in represented each cell by the raw frequency of the co-occurrence of two words. The raw frequency in a matrix may be skewed. Pointwise mutual information PPMI is a good measure for association between words which can tell us how much often the two words occur.<br>The pointwise mutual information is a measure of how often two events x and y occur, compared with what we would expect if they were independent:</p>
<script type="math/tex; mode=display">PMI(x,y) = log_2\frac{P(x,y)}{P(x)P(y)}</script><p>PMI between two words is:</p>
<script type="math/tex; mode=display">PMI(word_1,word_2) = log_2\frac{P(word_1,word_2)}{P(word_1)P(word_2)}</script><p>Positive PMI(PPMI) replace all negative values of PMI by 0, which eliminate the problem of unreliable negative PMI values.</p>
<script type="math/tex; mode=display">PPMI(word_1,word_2) = max(log_2\frac{P(word_1,word_2)}{P(word_1)P(word_2)}, 0)</script><p>For infrequent events, PPMI is bias which would have high PMI values for very rare word. There are two solution for this problem, one is to give rare word slightly higher probabilities, one is add-one smoothing.<br>In this project, the PPMI with add-two smoothing is used to build a model which can compute the similar target words of a given context words. In this project, the similarity of two words will be defined as the cosine-similarity between their embeddings.<br>Then, analogy analysis is performed in this model. Analogy analysis is by given two words with some relation and a third word, find the fourth word which has the similar relation with the third word as the relation between the first two words.<br>For example, if given ‘man’, ‘woman’, ‘king’, the fourth word should be ‘queen’.  </p>
<h4 id="Cosine-Similarity"><a href="#Cosine-Similarity" class="headerlink" title="Cosine Similarity"></a>Cosine Similarity</h4><p>Given two word vector, we can measure the similarity of these two vectoe by the inner product or dot product of them.  </p>
<script type="math/tex; mode=display">dotProduct(v, w) = v \cdot w = v_1w_1 + v_2w_2 + ...+ v_nw_n</script><p>The dot product is high when two vectors have large values in same dimension and it value is low when they are orthogonal vectors with complementary distributions. However, a problem exist in this metric, dot product will be longer is these two vectors are longer, the length of a vector is  </p>
<script type="math/tex; mode=display">|v| = \sqrt{\sum_{i = 1}^{N} v_i^2}</script><p>This means that more frequent words will have higher dot product. The solution for this problem is dividing the dot product by the length of two vectors,which is the $cosine$ of the angle bwtween these two vectors.  </p>
<h4 id="Dense-Vector-Representation"><a href="#Dense-Vector-Representation" class="headerlink" title="Dense Vector Representation"></a>Dense Vector Representation</h4><p>Short and dense vectors have a number of potential advantages. Short vectors may be easier    to use as features in machine learning. Dense    vectors    may    generalize    better than storing explicit counts. In addition, dense vectors may perform better in capturing synonymy than sparse vectors.<br>Singular Value Decomposition(SVD) is a classic method for generating dense vectors, which is applied to a task of generating embedding from term document matrices in a model called Latent Semantic Analysis (LSA).<br>SVD factorizes any a rectangular matrix $X$ into the product of three matrices $W, s, C$.<br>For $W$, its rows corresponding    to original matrix but columns    represents a dimension in a    new    latent space, such that the column vectors are orthogonal to each other. Columns are ordered    by the amount of variance in the dataset each new dimension accounts for.<br>$S$ is a diagonal matrix of singular values expressing    the importance of each dimension.$C$ represents documents or contexts, but each row now represents one of the new latent dimensions and the m row vectors are orthogonal to each other.<br>In this project, SVD is applied to the term-term matrix, let $M$ denote the$10000 \times 10000$ word co-occurrence matrix. After SVD, only 100 dimension is kept by keeping the top 100 singular values. Then, a low rank approximation matric is got. But only the truncated $W$ matrix is used as word embedding.<br>The word embedding is used to compute the similarity of words in the data set of this project as in the PMI method. After that, Analogy analysis is also performed in this method.  </p>
<h3 id="Result-and-Discussion"><a href="#Result-and-Discussion" class="headerlink" title="Result and Discussion"></a>Result and Discussion</h3><h4 id="Result-of-Weighted-Terms-Presentation-PPMI"><a href="#Result-of-Weighted-Terms-Presentation-PPMI" class="headerlink" title="Result of Weighted Terms Presentation: PPMI"></a>Result of Weighted Terms Presentation: PPMI</h4><p>After applying add two smooth to the co-occurrence matrix $M$, PPMI matrix is computed. Then this PMI matrix is used as the word embeddings in this data set.<br>For a given word, the corresponding vector can be retrieved from the PMI matrix by the index of this word. Then, we can compute the cosine similarity between this word and any other words. The increasing of the value of cosine similarity indicates the more relevance of two words.<br>For the similar word experiment, several word as inputs are given, and then the PPMI model is applied to search the 10 most similar words. The result of this experiment is shown as below. </p>
<table class="table table-bordered">
<thead>
<tr>
<th style="text-align:center">imput</th>
<th style="text-align:center">output(top 10 most similar words in PPMI)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">&#39;water&#39;</td>
<td style="text-align:center">&#39;water&#39;, &#39;liquid&#39;, &#39;oxygen&#39;, &#39;gas&#39;, &#39;fuel&#39;, &#39;fluid&#39;, &#39;soil&#39;, &#39;drainage&#39;, &#39;sand&#39;, &#39;waste&#39;</td>
</tr>
<tr>
<td style="text-align:center">&#39;boy&#39;</td>
<td style="text-align:center">&#39;boy&#39;, &#39;girl&#39;, &#39;kid&#39;, &#39;baby&#39;, &#39;man&#39;, &#39;crazy&#39;, &#39;woman&#39;, &#39;dog&#39;, &#39;cat&#39;, &#39;daddy&#39;</td>
</tr>
<tr>
<td style="text-align:center">&#39;apple&#39;</td>
<td style="text-align:center">&#39;apple&#39;, &#39;microsoft&#39;, &#39;ibm&#39;, &#39;linux&#39;, &#39;proprietary&#39;, &#39;os&#39;, &#39;compatible&#39;, &#39;processor&#39;, &#39;console&#39;, &#39;sony&#39;</td>
</tr>
<tr>
<td style="text-align:center">&#39;fruit&#39;</td>
<td style="text-align:center">&#39;fruit&#39;, &#39;fruits&#39;, &#39;flowers&#39;, &#39;vegetables&#39;, &#39;meat&#39;, &#39;corn&#39;, &#39;wheat&#39;, &#39;flower&#39;, &#39;seeds&#39;, &#39;milk&#39;</td>
</tr>
<tr>
<td style="text-align:center">&#39;cat&#39;</td>
<td style="text-align:center">&#39;cat&#39;, &#39;dog&#39;, &#39;rabbit&#39;, &#39;monkey&#39;, &#39;pig&#39;, &#39;rat&#39;, &#39;cow&#39;, &#39;duck&#39;, &#39;cats&#39;, &#39;frog&#39;</td>
</tr>
<tr>
<td style="text-align:center">&#39;beautiful&#39;</td>
<td style="text-align:center">&#39;beautiful&#39;, &#39;pretty&#39;, &#39;wonderful&#39;, &#39;attractive&#39;, &#39;lonely&#39;, &#39;nice&#39;, &#39;magnificent&#39;, &#39;sweet&#39;, &#39;funny&#39;, &#39;sad&#39;</td>
</tr>
<tr>
<td style="text-align:center">&#39;good&#39;</td>
<td style="text-align:center">&#39;good&#39;, &#39;bad&#39;, &#39;excellent&#39;, &#39;better&#39;, &#39;poor&#39;, &#39;nice&#39;, &#39;sure&#39;, &#39;wrong&#39;, &#39;my&#39;, &#39;wonderful&#39;</td>
</tr>
<tr>
<td style="text-align:center">&#39;many&#39;</td>
<td style="text-align:center">&#39;many&#39;, &#39;several&#39;, &#39;numerous&#39;, &#39;some&#39;, &#39;other&#39;, &#39;various&#39;, &#39;including&#39;, &#39;these&#39;, &#39;such&#39;, &#39;few&#39;</td>
</tr>
<tr>
<td style="text-align:center">&#39;blue&#39;</td>
<td style="text-align:center">&#39;blue&#39;, &#39;red&#39;, &#39;yellow&#39;, &#39;purple&#39;, &#39;black&#39;, &#39;pink&#39;, &#39;white&#39;, &#39;green&#39;, &#39;orange&#39;, &#39;colored&#39;</td>
</tr>
<tr>
<td style="text-align:center">&#39;play&#39;</td>
<td style="text-align:center">&#39;play&#39;, &#39;playing&#39;, &#39;plays&#39;, &#39;played&#39;, &#39;compete&#39;, &#39;perform&#39;, &#39;sing&#39;, &#39;participate&#39;, &#39;players&#39;, &#39;game&#39;</td>
</tr>
<tr>
<td style="text-align:center">&#39;try&#39;</td>
<td style="text-align:center">&#39;try&#39;, &#39;trying&#39;, &#39;tried&#39;, &#39;tries&#39;, &#39;attempting&#39;, &#39;attempt&#39;, &#39;able&#39;, &#39;attempts&#39;, &#39;help&#39;, &#39;attempted&#39;</td>
</tr>
<tr>
<td style="text-align:center">&#39;walk&#39;</td>
<td style="text-align:center">&#39;walk&#39;, &#39;walking&#39;, &#39;walks&#39;, &#39;walked&#39;, &#39;climb&#39;, &#39;swim&#39;, &#39;ride&#39;, &#39;throw&#39;, &#39;sit&#39;, &#39;wait&#39;</td>
</tr>
</tbody>
</table>

<p>From the result of similar words above, we can see that the PPMI do a good job in searching similar words for the given inputs. The similar words getting from PPMI are all relevant to the input word. There are some interesting output such as the output the ‘good’, the most similar word except itself is ‘bad’. Although We may think that these two words are opposite but not similar, for the given content these two words are used in most similar surrounding. That is to say they are syntactically similar. Thus, the PPMI will treat them as similar. The word embedding from PPMI matrix capture both the syntactic and semantic meaning of a word.<br>After the similarity experiment, analogy experiment was also perform by using the word embeding from PPMI. An analogy question— “man is to woman as king is to <em>__</em>”, where the goal of this analogy task is to fill in the blank space.<br>This question can be solved by finding a word whose word embedding is most closest to </p>
<p><script type="math/tex">w_{woman} - w_{man} + w_{king}</script>.<br>The result for this problem using cosine similarity and PPMI is as below.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&apos;man to woman&apos; is similar as &apos;king to prince&apos;</span><br><span class="line">&apos;boy to girl&apos; is similar as &apos;king to queen&apos;</span><br><span class="line">&apos;beijing to china&apos; is similar as &apos;paris to france&apos;</span><br><span class="line">&apos;king to queen&apos; is similar as &apos;ibm to microsoft&apos;</span><br><span class="line">&apos;korea to seoul&apos; is similar as &apos;thailand to bangkok&apos;</span><br></pre></td></tr></table></figure></p>
<p>It seems that, the PPMI does a good job in the problem except the first analogy. Then, an experiment through a test file which contains more than 5585 analogy samples to text the accurary of analogy is conducted. After a long time computation, the accuracy of PPMI in analogy is as below. It only get $41.6\%$ accuracy in this analogy test. It seems that the accuracy of word analogy by PPMI word embedding is quite low. Besides, the computation complexity of this 10000* 10000 matrix is very high, which makes the process slow dowm.  </p>
<h4 id="Result-of-Dense-vector-via-SVD"><a href="#Result-of-Dense-vector-via-SVD" class="headerlink" title="Result of Dense vector via SVD"></a>Result of Dense vector via SVD</h4><p>In this part, SVD is applied to decomposite the co-occurrence matrix in order to get a short and dense vector for word embedding. The plot of the singular value in this SVD decomposition is show as below.  </p>
<table class="table table-bordered">
<thead>
<tr>
<th>imput</th>
<th>output(top 10 most similar words in PPMI using SVD)</th>
</tr>
</thead>
<tbody>
<tr>
<td>&#39;water&#39;</td>
<td>&#39;water&#39;, &#39;liquid&#39;, &#39;oxygen&#39;, &#39;gas&#39;, &#39;fuel&#39;, &#39;fluid&#39;, &#39;soil&#39;, &#39;drainage&#39;, &#39;sand&#39;, &#39;waste&#39;</td>
</tr>
<tr>
<td>&#39;boy&#39;</td>
<td>&#39;boy&#39;, &#39;girl&#39;, &#39;kid&#39;, &#39;baby&#39;, &#39;man&#39;, &#39;crazy&#39;, &#39;woman&#39;, &#39;dog&#39;, &#39;cat&#39;, &#39;daddy&#39;</td>
</tr>
<tr>
<td>&#39;apple&#39;</td>
<td>&#39;apple&#39;, &#39;microsoft&#39;, &#39;ibm&#39;, &#39;linux&#39;, &#39;proprietary&#39;, &#39;os&#39;, &#39;compatible&#39;, &#39;processor&#39;, &#39;console&#39;, &#39;sony&#39;</td>
</tr>
<tr>
<td>&#39;fruit&#39;</td>
<td>&#39;fruit&#39;, &#39;fruits&#39;, &#39;flowers&#39;, &#39;vegetables&#39;, &#39;meat&#39;, &#39;corn&#39;, &#39;wheat&#39;, &#39;flower&#39;, &#39;seeds&#39;, &#39;milk&#39;</td>
</tr>
<tr>
<td>&#39;cat&#39;</td>
<td>&#39;cat&#39;, &#39;dog&#39;, &#39;rabbit&#39;, &#39;monkey&#39;, &#39;pig&#39;, &#39;rat&#39;, &#39;cow&#39;, &#39;duck&#39;, &#39;cats&#39;, &#39;frog&#39;</td>
</tr>
<tr>
<td>&#39;beautiful&#39;</td>
<td>&#39;beautiful&#39;, &#39;pretty&#39;, &#39;wonderful&#39;, &#39;attractive&#39;, &#39;lonely&#39;, &#39;nice&#39;, &#39;magnificent&#39;, &#39;sweet&#39;, &#39;funny&#39;, &#39;sad&#39;</td>
</tr>
<tr>
<td>&#39;good&#39;</td>
<td>&#39;good&#39;, &#39;bad&#39;, &#39;excellent&#39;, &#39;better&#39;, &#39;poor&#39;, &#39;nice&#39;, &#39;sure&#39;, &#39;wrong&#39;, &#39;my&#39;, &#39;wonderful&#39;</td>
</tr>
<tr>
<td>&#39;many&#39;</td>
<td>&#39;many&#39;, &#39;several&#39;, &#39;numerous&#39;, &#39;some&#39;, &#39;other&#39;, &#39;various&#39;, &#39;including&#39;, &#39;these&#39;, &#39;such&#39;, &#39;few&#39;</td>
</tr>
<tr>
<td>&#39;blue&#39;</td>
<td>&#39;blue&#39;, &#39;red&#39;, &#39;yellow&#39;, &#39;purple&#39;, &#39;black&#39;, &#39;pink&#39;, &#39;white&#39;, &#39;green&#39;, &#39;orange&#39;, &#39;colored&#39;</td>
</tr>
<tr>
<td>&#39;play&#39;</td>
<td>&#39;play&#39;, &#39;playing&#39;, &#39;plays&#39;, &#39;played&#39;, &#39;compete&#39;, &#39;perform&#39;, &#39;sing&#39;, &#39;participate&#39;, &#39;players&#39;, &#39;game&#39;</td>
</tr>
<tr>
<td>&#39;try&#39;</td>
<td>&#39;try&#39;, &#39;trying&#39;, &#39;tried&#39;, &#39;tries&#39;, &#39;attempting&#39;, &#39;attempt&#39;, &#39;able&#39;, &#39;attempts&#39;, &#39;help&#39;, &#39;attempted&#39;</td>
</tr>
<tr>
<td>&#39;walk&#39;</td>
<td>&#39;walk&#39;, &#39;walking&#39;, &#39;walks&#39;, &#39;walked&#39;, &#39;climb&#39;, &#39;swim&#39;, &#39;ride&#39;, &#39;throw&#39;, &#39;sit&#39;, &#39;wait&#39;</td>
</tr>
</tbody>
</table>

<p>From the result, we can see that co-occurrence matrix with SVD also get a well job in searching the similar words.<br>After the similarity experiment, analogy experiment was also perform by using the word embedding from truncated co-occurrence matrix.<br>The result for this problem using cosine similarity and Dense vector is as below.<br>From the result, we can see that the dense vector perform well in this analogy problem. It gets the right result for the analogy problem given.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&apos;man to woman&apos; is similar as &apos;king to queen&apos;</span><br><span class="line">&apos;boy to girl&apos; is similar as &apos;king to queen&apos;</span><br><span class="line">&apos;beijing to china&apos; is similar as &apos;paris to france&apos;</span><br><span class="line">&apos;king to queen&apos; is similar as &apos;ibm to microsoft&apos;</span><br><span class="line">&apos;korea to seoul&apos; is similar as &apos;thailand to bangkok&apos;</span><br></pre></td></tr></table></figure></p>
<p>As the PPMI model, an experiment through a test file which contains more than 5585 analogy samples to text the accurary of analogy is conducted. The accuracy using dense vector is $54\%$, which is much better than the sparse vector PPMI method.</p>
<h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>From all the result of the two method, we know that the dense vector method get a better result than the sparse PPMI method in analogy analysis and similar word search. In addition, the computational efficiency of the dense vector is also better than the PPMI. Short vectors may be easier to use as features in machine learning. Dense vectors    may    generalize better than storing explicit counts. In addition, dense vectors may perform better in capturing synonymy than sparse vectors.  </p>

    </article>
    <!-- 前后页  -->
    <ul class="post-pager">
        
            <li class="next">
                <a href= "/2017/10/12/Sharing-On-Campus-Web-Application/" title= Sharing On Campus: Web Application >
                    <span>Next Post</span>
                    <span>Sharing On Campus: Web Application</span>
                </a>
            </li>
        
        
            <li class="previous">
                <a href= "/2017/05/02/cat-and-dog/" title= cat-and-dog >
                    <span>Previous Post</span>
                    <span>cat-and-dog</span>
                </a>
            </li>
        
    </ul>
    <!-- 评论插件 -->
    <!-- 来必力City版安装代码 -->

<!-- City版安装代码已完成 -->
    
    
<div id="container"></div>
<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script>
    var gitment = new Gitment({
        // id: "Word Embedding: Syntactics or Semantics", // 可选。默认为 location.href
        owner: 'wushbin',
        repo: 'https://wushbin.github.io/',
        oauth: {
            client_id: '17eb7cee5fa4bc08355b',
            client_secret: '176f2991889fafe11f2ae7670c5f1dbe879ce0eb',
        },
    })
    gitment.render('container')

</script>

    <!--PC版-->

    <!--PC版-->


    
</main>
            <!-- profile -->
            
        </div>
        <footer class="footer footer-unloaded">
    <!-- social  -->
    
    <div class="social">
        
    
        
            
                <a href="mailto:ngshbin@gmail.com" class="iconfont-archer email" title=email ></a>
            
        
    
        
            
                <a href="//github.com/wushbin" class="iconfont-archer github" target="_blank" title="github"></a>
            
        
    
        
    
        
            
                <a href="https://www.linkedin.com/in/shengbin-wu/" class="iconfont-archer linkedin" target="_blank" title="linkedin"></a>
            
        
    
        
            
                <a href="/atom.xml" class="iconfont-archer rss" target="_blank" title="rss"></a>
            
        
    

    </div>
    
    <!-- powered by Hexo  -->
    <div class="copyright">
        <span id="hexo-power">Powered by <a href="https://hexo.io/" target="_blank">Hexo</a></span><span class="iconfont-archer power">&#xe635;</span><span id="theme-info">Theme <a href="https://github.com/fi3ework/hexo-theme-archer" target="_blank">archer</a></span>
    </div>
    <!-- 不蒜子  -->
    
</footer>
    </div>
    <!-- toc -->
    
    <div class="toc-wrapper">
        <div class="toc-catalog">
            <span class="iconfont-archer catalog-icon">&#xe613;</span><span>CATALOG</span>
        </div>
        <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Introduction"><span class="toc-number">1.</span> <span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Method"><span class="toc-number">2.</span> <span class="toc-text">Method</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Sparse-Vector-Representation"><span class="toc-number">2.1.</span> <span class="toc-text">Sparse Vector Representation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Cosine-Similarity"><span class="toc-number">2.2.</span> <span class="toc-text">Cosine Similarity</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Dense-Vector-Representation"><span class="toc-number">2.3.</span> <span class="toc-text">Dense Vector Representation</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Result-and-Discussion"><span class="toc-number">3.</span> <span class="toc-text">Result and Discussion</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Result-of-Weighted-Terms-Presentation-PPMI"><span class="toc-number">3.1.</span> <span class="toc-text">Result of Weighted Terms Presentation: PPMI</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Result-of-Dense-vector-via-SVD"><span class="toc-number">3.2.</span> <span class="toc-text">Result of Dense vector via SVD</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Conclusion"><span class="toc-number">4.</span> <span class="toc-text">Conclusion</span></a></li></ol>
    </div>
    
    <div class="back-top">&#xe639;</div>
    <div class="sidebar">
    <div class="sidebar-header sidebar-header-show-archive">
        <div class="sidebar-category">
            <span class="sidebar-archive-link"><span class="iconfont-archer">&#xe67d;</span>Archive</span>
            <span class="sidebar-tags-link"><span class="iconfont-archer">&#xe610;</span>Tag</span>
        </div>
    </div>
    <div class="sidebar-content sidebar-content-show-archive">
          <div class="sidebar-archive">
    <!-- 在ejs中将archive按照时间排序 -->
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <div class="total-archive"> Total : 10 </div>
    
    <div class="post-archive">
    
    
    
    
    <div class="archive-year"> 2017 </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">12/23</span><a class="archive-post-title" href= "/2017/12/23/Speech-Recognition/" >Speech Recognition</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">12/23</span><a class="archive-post-title" href= "/2017/12/23/PacMan-Game-Artificial-Intellegence/" >PacMan Game: Artificial Intellegence</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">12/23</span><a class="archive-post-title" href= "/2017/12/23/Optical-Character-Recognition/" >Optical Character Recognition</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">12/23</span><a class="archive-post-title" href= "/2017/12/23/Hidden-Markov-Model-for-Part-of-Speech-Tagging/" >Hidden Markov Model for Part of Speech Tagging</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/12</span><a class="archive-post-title" href= "/2017/10/12/Sharing-On-Campus-Web-Application/" >Sharing On Campus: Web Application</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/09</span><a class="archive-post-title" href= "/2017/10/09/Word-Embedding-Syntactics-or-Semantics/" >Word Embedding: Syntactics or Semantics</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/02</span><a class="archive-post-title" href= "/2017/05/02/cat-and-dog/" >cat-and-dog</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/01</span><a class="archive-post-title" href= "/2017/05/01/Duke-Gather-Android-Application/" >Duke Gather: Android Application</a>
        </li>
    
    
    
    
    
        </ul>
    
    <div class="archive-year"> 2016 </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/01</span><a class="archive-post-title" href= "/2016/06/01/High-Speed-Test-Rig-Development/" >High Speed Test Rig Development</a>
        </li>
    
    
    
    
    
        </ul>
    
    <div class="archive-year"> 2013 </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/01</span><a class="archive-post-title" href= "/2013/06/01/SCARA-Robot/" >SCARA Robot</a>
        </li>
    
    </div>
  </div>
        <div class="sidebar-tags">
    <div class="sidebar-tags-name">
    
        <span class="sidebar-tag-name"><a href= "#">machine learning</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">Java</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">Mobile Application</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">natural language processing</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">mechanical design</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">experiment</a></span>
    
    </div>
    <div class="tag-load-fail" style="display: none; color: #ccc; font-size: 0.6rem;">
    缺失模块。<br/>
    1、请确保node版本大于6.2<br/>
    2、在博客根目录（注意不是archer根目录）执行以下命令：<br/>
    <span style="color: #f75357; font-size: 1rem; line-height: 2rem;">npm i hexo-generator-json-content --save</span><br/>
    3、在根目录_config.yml里添加配置：
    <pre style="color: #787878; font-size: 0.6rem;">
jsonContent:
  meta: false
  pages: false
  posts:
    title: true
    date: true
    path: true
    text: false
    raw: false
    content: false
    slug: false
    updated: false
    comments: false
    link: false
    permalink: false
    excerpt: false
    categories: false
    tags: true</pre>
    </div> 
    <div class="sidebar-tag-list"></div>
</div>
    </div>
</div> 
    <script>
    var jsInfo = {
        root: '/'
    }
</script>
    <!-- 不蒜子  -->
    
    <!-- CNZZ统计  -->
    
    </div>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>

<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

</body>
</html>


