<!DOCTYPE html>
<html>
    <head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" >
    <title>
        
        Decision Tree and Random Forest · Shengbin&#39;s Studio
        
    </title>
    <link rel="icon" href= /assets/favicon2.ico>
    <!-- TODO: 在font-face加载完毕后改变字体  -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/webfont/1.6.28/webfontloader.js"></script>
    <!-- 提前加载place holder  -->
    <style type="text/css">
        @font-face {
            font-family: 'Oswald-Regular';
            src: url(/font/Oswald-Regular.ttf);
        }
    </style>
    <style type="text/css">
        .site-intro {
            position: relative;
            width: 100%;
            height: 50vh;
            overflow: hidden;
            box-shadow: -0.1rem 0 0.5rem 0 rgba(0, 0, 0, 0.8);
        }
        .site-intro-placeholder {
            position: absolute;
            z-index: -2;
            top: 0;
            left: 0px;
            width: calc(100% + 300px);
            height: 100%;
            background: repeating-linear-gradient(-45deg, #444 0, #444 80px, #333 80px, #333 160px);
            background-position: center center;
            transform: translate3d(-226px, 0, 0);
            animation: gradient-move 2.5s ease-out 0s 1;
        }
        @keyframes gradient-move {
        0% {
            transform: translate3d(-226px, 0, 0);
        }
        100% {
            transform: translate3d(0, 0, 0);
        }
    }
</style>
    <link rel="stylesheet" href = /css/style.css?v=20171218 />
    <script src="//cdn.staticfile.org/jquery/3.2.1/jquery.min.js" defer></script>
    
    <script src="/scripts/main.js" defer></script>
    <!-- 百度统计  -->
    
    <!-- 谷歌统计  -->
    
</head>
    
        <body class="post-body">
    
    
<header class="header">

    <div class="read-progress"></div>
    <div class="header-sidebar-menu">&#xe775;</div>
    <!-- post页的toggle banner  -->
    
    <div class="banner">
            <div class="blog-title">
                <a href="/" >Shengbin&#39;s Studio.</a>
            </div>
            <div class="post-title">
                <a href="#" class="post-name">Decision Tree and Random Forest</a>
            </div>
    </div>
    
    <a class="home-link" href=/>Shengbin's Studio.</a>
</header>
    <div class="wrapper">
        <div class="site-intro">
    
    <!-- 主页  -->
    
    
    <!-- 404页  -->
            
    <div class="site-intro-img" style="background-image: url(http://oumn0o088.bkt.clouddn.com/post-bg.jpg)"></div>
    <div class="site-intro-placeholder"></div>
    <div class="site-intro-meta">
        <!-- 标题  -->
        <h1 class="intro-title">
            <!-- 主页  -->
            
            Decision Tree and Random Forest
            <!-- 404 -->
            
        </h1>
        <!-- 副标题 -->
        <p class="intro-subtitle">
            <!-- 主页副标题  -->
            
            
            <!-- 404 -->
            
        </p>
        <!-- 文章页meta -->
        
            <!-- 文章页标签  -->
            
            <div class="post-intro-meta">
                <span class="post-intro-calander iconfont-archer">&#xe676;</span>
                <span class="post-intro-time">2018/02/02</span>
            </div>
        
    </div>
</div>
        <img class="loading" src="/assets/loading.svg" style="display: block; margin: 6rem auto 0 auto; width: 6rem; height: 6rem;" />
        <script>
            var browser = {
                    versions: function () {
                        var u = window.navigator.userAgent;
                        return {
                            userAgent: u,
                            trident: u.indexOf('Trident') > -1, //IE内核
                            presto: u.indexOf('Presto') > -1, //opera内核
                            webKit: u.indexOf('AppleWebKit') > -1, //苹果、谷歌内核
                            gecko: u.indexOf('Gecko') > -1 && u.indexOf('KHTML') == -1, //火狐内核
                            mobile: !!u.match(/AppleWebKit.*Mobile.*/), //是否为移动终端
                            ios: !!u.match(/\(i[^;]+;( U;)? CPU.+Mac OS X/), //ios终端
                            android: u.indexOf('Android') > -1 || u.indexOf('Linux') > -1, //android终端或者uc浏览器
                            iPhone: u.indexOf('iPhone') > -1 || u.indexOf('Mac') > -1, //是否为iPhone或者安卓QQ浏览器
                            iPad: u.indexOf('iPad') > -1, //是否为iPad
                            webApp: u.indexOf('Safari') == -1, //是否为web应用程序，没有头部与底部
                            weixin: u.indexOf('MicroMessenger') == -1, //是否为微信浏览器
                            uc: u.indexOf('UCBrowser') > -1 //是否为android下的UC浏览器
                        };
                    }()
                }

            function fontLoaded(){
                console.log('font loaded');
                if (document.getElementsByClassName('site-intro-meta')) {
                    document.getElementsByClassName('intro-title')[0].classList.add('intro-fade-in');
                    document.getElementsByClassName('intro-subtitle')[0].classList.add('intro-fade-in');
                    var postIntroTags = document.getElementsByClassName('post-intro-tags')[0],
                        postIntroMeat = document.getElementsByClassName('post-intro-meta')[0];
                        if (postIntroTags) {
                            postIntroTags.classList.add('post-fade-in');
                        }
                        if (postIntroMeat) {
                            postIntroMeat.classList.add('post-fade-in');
                        }
                    }
                }
                
            console.log("userAgent:" + browser.versions.userAgent);
            // UC不支持跨域，所以直接显示
            if (browser.versions.uc) {
                console.log("UCBrowser");
                fontLoaded();
            } else {
                WebFont.load({
                    custom: {
                        families: ['Oswald-Regular']
                    },
                    loading: function () {  //所有字体开始加载
                        // console.log('loading');
                    },
                    active: function () {  //所有字体已渲染
                        fontLoaded();
                    },
                    inactive: function () { //字体预加载失败，无效字体或浏览器不支持加载
                        console.log('inactive: timeout');
                        fontLoaded();
                    },
                    timeout: 7000 // Set the timeout to two seconds
                });
            }
        </script>
        <div class="container container-unloaded">
            <main class="main post-page">
    <article class="article-entry">
        <p>This post is to implement a Binary Decision Tree and a Random Forest with Decision Stump</p>
<h3 id="Implementation-of-A-Decision-Tree-Node"><a href="#Implementation-of-A-Decision-Tree-Node" class="headerlink" title="Implementation of A Decision Tree Node"></a>Implementation of A Decision Tree Node</h3><p>The Code of implementation is shown as below. </p>
<ul>
<li>‘feature_id’: which feature is used to split data in this decision tree node</li>
<li>threshold: threshold applied to the chosen feature to split data</li>
<li>gini: the gini value of this three node</li>
<li>d_gini: gini reduction from its parent node to this node</li>
<li>f_space: range of features this the subtree with this node as the root</li>
<li>label: majority vote of samples’ label</li>
<li>left: left child</li>
<li>right: right child<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecisionNode</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, f_id, thres, gini, d_gini, f_s= list<span class="params">()</span>, label=None)</span>:</span></span><br><span class="line">        self.feature_id = f_id</span><br><span class="line">        self.threshold = thres</span><br><span class="line">        self.gini = gini <span class="comment">## add gini to the tree node</span></span><br><span class="line">        self.d_gini = d_gini</span><br><span class="line">        self.f_space = f_s <span class="comment">## a filed for decision boundary polting</span></span><br><span class="line">        self.label = label</span><br><span class="line">        self.left = <span class="keyword">None</span></span><br><span class="line">        self.right = <span class="keyword">None</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">"self.feature_id: &#123;0&#125;\nself.threshold: &#123;1&#125;\nself.gini: &#123;2&#125;\nself.label: &#123;3&#125;\nself.space: &#123;4&#125;\nself.d_gini: &#123;5&#125;"</span>.format(self.feature_id, self.threshold, self.gini, self.label, self.f_space, self.d_gini)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="Implementation-of-A-Decision-Tree"><a href="#Implementation-of-A-Decision-Tree" class="headerlink" title="Implementation of A Decision Tree"></a>Implementation of A Decision Tree</h3><p>In this post, a decision tree is implemented as a binary tree, which can only deal with binary classification problems. The method used to fully grow a decision tree in this post is to split the tree in the feature with a threshold that maximize the Gini reduction. Thus, the code below will iterate all the features with all threshold of this features to find the maximum Gini reduction and then choose this feature the split the tree. Repeat the split method recursively until the decision tree can not be split any more.<br>The code below in the Decision tree implementation in Python.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecisionTree</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.best_node = <span class="keyword">None</span></span><br><span class="line">        </span><br><span class="line">    <span class="comment">## a buggy self version decision tree, implemented by a binary tree</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, train_data, f_space)</span>:</span></span><br><span class="line">        self.best_node = self.buildDecisionTree(train_data, f_space)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">buildDecisionTree</span><span class="params">(self, train_data, f_space)</span>:</span></span><br><span class="line">        train_x = train_data[<span class="number">0</span>]</span><br><span class="line">        train_y = train_data[<span class="number">1</span>]</span><br><span class="line">        <span class="comment">#f is number of features</span></span><br><span class="line">        <span class="comment">#n is the number of training sample</span></span><br><span class="line">        n, f = train_x.shape</span><br><span class="line">        <span class="keyword">if</span> n == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">        train_y = train_y.reshape(n, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        p = np.sum(train_y == <span class="number">1</span>) / n</span><br><span class="line">        label = <span class="keyword">None</span></span><br><span class="line">        gini_input = <span class="number">2</span> * p * (<span class="number">1</span> - p)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> p &gt; <span class="number">0.5</span>:</span><br><span class="line">            label = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            label = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> p == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> DecisionNode(f_id=<span class="number">-1</span>, thres=<span class="keyword">None</span>, gini=gini_input, d_gini=<span class="number">0</span>, f_s=f_space, label=label) <span class="comment">## this is a leaf with label negative</span></span><br><span class="line">        <span class="keyword">if</span> p == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> DecisionNode(f_id=<span class="number">-1</span>, thres=<span class="keyword">None</span>, gini=gini_input, d_gini=<span class="number">0</span>, f_s=f_space, label=label) <span class="comment">## this is a leaf with label positive</span></span><br><span class="line">        <span class="comment">## generate thresholds for each features</span></span><br><span class="line">        possibilities = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(f):</span><br><span class="line">            x_sorted = np.sort(train_x[:,i])</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>, n):</span><br><span class="line">                p1 = x_sorted[j<span class="number">-1</span>]</span><br><span class="line">                p2 = x_sorted[j]</span><br><span class="line">                <span class="keyword">if</span> p1 == p2:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                threshold = p1 + (p2 - p1)/<span class="number">2</span></span><br><span class="line">                child0_id = np.where(train_x[:,i] &lt; threshold)</span><br><span class="line">                child1_id = np.where(train_x[:,i] &gt;= threshold)</span><br><span class="line"></span><br><span class="line">                x_child0 = train_x[child0_id]</span><br><span class="line">                y_child0 = train_y[child0_id]</span><br><span class="line">                n_child0 = y_child0.shape[<span class="number">0</span>]</span><br><span class="line">                p0 = np.sum(y_child0 == <span class="number">1</span>) / n_child0</span><br><span class="line">                gini0 = <span class="number">2</span> * p0 * (<span class="number">1</span> - p0)</span><br><span class="line"></span><br><span class="line">                x_child1 = train_x[child1_id]</span><br><span class="line">                y_child1 = train_y[child1_id]</span><br><span class="line">                n_child1 = y_child1.shape[<span class="number">0</span>]</span><br><span class="line">                p1 = np.sum(y_child1 == <span class="number">1</span>) / n_child1</span><br><span class="line">                gini1 = <span class="number">2</span> * p1 * (<span class="number">1</span> - p1)</span><br><span class="line"></span><br><span class="line">                child_gini = n_child0/n * gini0 + n_child1/n * gini1 <span class="comment">## weighted average of children gini</span></span><br><span class="line"></span><br><span class="line">                possibilities.append((gini_input-child_gini, i, threshold))</span><br><span class="line">        <span class="comment">## generate the list of (gini reduction, feature index, threshold)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">## all the feature are the same, can not split</span></span><br><span class="line">        <span class="keyword">if</span> len(possibilities) == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> DecisionNode(f_id=<span class="number">-1</span>, thres=<span class="keyword">None</span>, gini=gini_input, d_gini=<span class="number">0</span>, f_s=f_space, label=label)</span><br><span class="line"></span><br><span class="line">        (gini_reduction, f_id, thres) = max(possibilities) <span class="comment">## find the max gini reduction, that is to find the minimum gini value</span></span><br><span class="line">        <span class="comment">#print((gini_reduction, f_id, thres))</span></span><br><span class="line"></span><br><span class="line">        curr_node = DecisionNode(f_id=f_id, thres=thres, gini=gini_input, d_gini=gini_reduction, f_s=f_space, label=label)</span><br><span class="line"></span><br><span class="line">        left_id = np.where(train_x[:,f_id] &lt; thres)</span><br><span class="line">        right_id = np.where(train_x[:,f_id] &gt;= thres)</span><br><span class="line">        f_selected_min = f_space[f_id][<span class="number">0</span>]</span><br><span class="line">        f_selected_max = f_space[f_id][<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        left_f_s = list(f_space)</span><br><span class="line">        left_f_s[f_id] = (f_selected_min, thres)</span><br><span class="line"></span><br><span class="line">        right_f_s = list(f_space)</span><br><span class="line">        right_f_s[f_id] = (thres, f_selected_max)</span><br><span class="line"></span><br><span class="line">        curr_node.left = self.buildDecisionTree((train_x[left_id], train_y[left_id]), left_f_s)</span><br><span class="line">        curr_node.right = self.buildDecisionTree((train_x[right_id], train_y[right_id]), right_f_s)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> curr_node</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">evaluateSample</span><span class="params">(self, x_input)</span>:</span></span><br><span class="line">        </span><br><span class="line">        curr_node = self.best_node</span><br><span class="line">        <span class="keyword">while</span> curr_node <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            f_id = curr_node.feature_id</span><br><span class="line">            thres = curr_node.threshold</span><br><span class="line">            <span class="keyword">if</span> curr_node.left <span class="keyword">is</span> <span class="keyword">None</span> <span class="keyword">and</span> curr_node.right <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">                <span class="keyword">return</span> curr_node.label</span><br><span class="line">            <span class="keyword">if</span> x_input[f_id] &lt; thres <span class="keyword">and</span> curr_node.left <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                curr_node = curr_node.left</span><br><span class="line">            <span class="keyword">if</span> x_input[f_id] &gt;= thres <span class="keyword">and</span> curr_node.right <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                curr_node = curr_node.right</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, x_input)</span>:</span></span><br><span class="line">        result = []</span><br><span class="line">        <span class="keyword">for</span> xs <span class="keyword">in</span> x_input:</span><br><span class="line">            result.append(self.evaluateSample(xs))</span><br><span class="line"></span><br><span class="line">        pre = np.array(result)</span><br><span class="line">        pre = pre.reshape((len(result), <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> pre</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">computeError</span><span class="params">(self, x_input, y_input, error=<span class="string">'missclassification'</span>)</span>:</span></span><br><span class="line">        result = []</span><br><span class="line">        <span class="keyword">for</span> xs <span class="keyword">in</span> x_input:</span><br><span class="line">            result.append(self.evaluateSample(xs))</span><br><span class="line">            </span><br><span class="line">        pre = np.array(result)</span><br><span class="line">        pre = pre.reshape((len(result), <span class="number">1</span>))</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> error == <span class="string">'missclassification'</span>:</span><br><span class="line">            res = np.absolute(pre - y_input)</span><br><span class="line">            err = np.sum(res == <span class="number">1</span>) / len(result)</span><br><span class="line">            <span class="keyword">return</span> err</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> error == <span class="string">'leastsquares'</span>:</span><br><span class="line">            res = np.sum(np.power((pre - y_input), <span class="number">2</span>))</span><br><span class="line">            err = res / len(result)</span><br><span class="line">            <span class="keyword">return</span> err</span><br></pre></td></tr></table></figure></p>
<h3 id="Implementation-of-Decision-Stump"><a href="#Implementation-of-Decision-Stump" class="headerlink" title="Implementation of Decision Stump"></a>Implementation of Decision Stump</h3><p>Decision Stump is a decision tree with depth 2, that is to say a decision stump is a decision tree with only one root node and two child node.<br>The code below is the implementation of decision stump. It is similar as the decision tree implementation, the only difference is that this tree is not constructed recursively because it only have depth 2.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecisionStump</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.best_node = <span class="keyword">None</span></span><br><span class="line">        self.surrogate_node = <span class="keyword">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, train_data, feature_dict=[])</span>:</span></span><br><span class="line">        </span><br><span class="line">        self.best_node, self.surrogate_node =self.buildStump(train_data, feature_dict)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">buildStump</span><span class="params">(self, train_data, feature_dict=[])</span>:</span></span><br><span class="line">        train_x = train_data[<span class="number">0</span>]</span><br><span class="line">        train_y = train_data[<span class="number">1</span>]</span><br><span class="line">        <span class="comment">#f is number of features</span></span><br><span class="line">        <span class="comment">#n is the number of training sample</span></span><br><span class="line">        n, f = train_x.shape</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> len(feature_dict) == <span class="number">0</span>:</span><br><span class="line">            feature_dict = np.arange(f)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> n == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">None</span>, <span class="keyword">None</span></span><br><span class="line">        train_y = train_y.reshape(n, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">## recursion exit conditions</span></span><br><span class="line">        p = np.sum(train_y == <span class="number">1</span>) / n</span><br><span class="line">        label = <span class="number">0</span></span><br><span class="line">        gini_input = <span class="number">2</span> * p * (<span class="number">1</span> - p)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> p &gt; <span class="number">0.5</span>:</span><br><span class="line">            label = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            label = <span class="number">0</span></span><br><span class="line">        <span class="comment">## all samples with same label, return</span></span><br><span class="line">        <span class="keyword">if</span> p == <span class="number">0</span> <span class="keyword">or</span> p == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> DecisionNode(f_id=<span class="number">-1</span>, thres=<span class="keyword">None</span>, gini=gini_input,d_gini=<span class="number">0</span>, label=label), <span class="keyword">None</span></span><br><span class="line">        <span class="comment">## recursion exit conditions above</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">## generate thresholds for each features</span></span><br><span class="line">        possibilities = []</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> feature_dict:</span><br><span class="line">            x_sorted = np.sort(train_x[:,i])</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>, n):</span><br><span class="line">                p1 = x_sorted[j<span class="number">-1</span>]</span><br><span class="line">                p2 = x_sorted[j]</span><br><span class="line">                <span class="keyword">if</span> p1 == p2:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                threshold = p1 + (p2 - p1)/<span class="number">2</span></span><br><span class="line">                child0_id = np.where(train_x[:,i] &lt; threshold)</span><br><span class="line">                child1_id = np.where(train_x[:,i] &gt;= threshold)</span><br><span class="line"></span><br><span class="line">                x_child0 = train_x[child0_id]</span><br><span class="line">                y_child0 = train_y[child0_id]</span><br><span class="line">                n_child0 = y_child0.shape[<span class="number">0</span>]</span><br><span class="line">                p0 = np.sum(y_child0 == <span class="number">1</span>) / n_child0</span><br><span class="line">                gini0 = <span class="number">2</span> * p0 * (<span class="number">1</span> - p0)</span><br><span class="line"></span><br><span class="line">                x_child1 = train_x[child1_id]</span><br><span class="line">                y_child1 = train_y[child1_id]</span><br><span class="line">                n_child1 = y_child1.shape[<span class="number">0</span>]</span><br><span class="line">                p1 = np.sum(y_child1 == <span class="number">1</span>) / n_child1</span><br><span class="line">                gini1 = <span class="number">2</span> * p1 * (<span class="number">1</span> - p1)</span><br><span class="line"></span><br><span class="line">                child_gini = n_child0/n * gini0 + n_child1/n * gini1</span><br><span class="line"></span><br><span class="line">                possibilities.append((gini_input - child_gini, i, threshold))</span><br><span class="line">        <span class="comment">## generate the list of (gini reduction, feature index, threshold)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">## all the feature are the same, can not split</span></span><br><span class="line">        <span class="keyword">if</span> len(possibilities) == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> DecisionNode(f_id=<span class="number">-1</span>, thres=<span class="keyword">None</span>, gini=gini_input, d_gini=<span class="number">0</span>, label=label), <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">## pick one feature and theshold with largest Gini Reduction</span></span><br><span class="line">        <span class="comment">## Best split</span></span><br><span class="line">        (gini_r_best, f_id_best, thres_best) = max(possibilities)</span><br><span class="line"></span><br><span class="line">        <span class="comment">## delete the possibilities with feature that are used</span></span><br><span class="line">        remain_possibilities = list(filter(<span class="keyword">lambda</span> x: x[<span class="number">1</span>] != f_id_best, possibilities))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> len(remain_possibilities)==<span class="number">0</span>: <span class="comment">## no other possibility</span></span><br><span class="line">            b_node = DecisionNode(f_id=f_id_best,thres=thres_best,gini=gini_input, d_gini=gini_r_best, label=label)</span><br><span class="line">            b_node.left, b_node.right = self.split(train_data, b_node)</span><br><span class="line">            <span class="keyword">return</span> b_node, <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">## compute PL, PR with the best split</span></span><br><span class="line">        left_id = np.where(train_x[:,f_id_best] &lt; thres_best)</span><br><span class="line">        right_id = np.where(train_x[:,f_id_best] &gt;= thres_best)</span><br><span class="line"></span><br><span class="line">        left_x = train_x[left_id]</span><br><span class="line">        left_y = train_y[left_id]</span><br><span class="line"></span><br><span class="line">        right_x = train_x[right_id]</span><br><span class="line">        right_y = train_y[right_id]</span><br><span class="line"></span><br><span class="line">        pL = left_id[<span class="number">0</span>].size/n</span><br><span class="line">        pR = right_id[<span class="number">0</span>].size/n</span><br><span class="line"></span><br><span class="line">        <span class="comment">## find the best surrogate split by finding the maximum predictive similarity measurement:</span></span><br><span class="line">        lamb = []</span><br><span class="line">        <span class="keyword">for</span> j, (g, idx, t) <span class="keyword">in</span> enumerate(remain_possibilities):</span><br><span class="line">            l_id = np.where(left_x[:,idx] &lt; t)</span><br><span class="line">            pLL = l_id[<span class="number">0</span>].size / n</span><br><span class="line">            <span class="comment">#print(l_id[0].size)</span></span><br><span class="line">            r_id = np.where(right_x[:, idx] &gt;= t)</span><br><span class="line">            pRR = r_id[<span class="number">0</span>].size / n</span><br><span class="line">            <span class="comment">#print(r_id[0].size)</span></span><br><span class="line">            lamb_temp = (min(pL, pR) - (<span class="number">1</span> - pLL - pRR))/min(pL, pR)</span><br><span class="line">            lamb.append((lamb_temp, j))</span><br><span class="line"></span><br><span class="line">        lambd_max, idx_max = max(lamb)</span><br><span class="line">        <span class="comment">## Best surrogate node</span></span><br><span class="line">        (gini_r_surg, f_id_surg, thres_surg) = remain_possibilities[idx_max]</span><br><span class="line">        </span><br><span class="line">        b_node = DecisionNode(f_id=f_id_best,thres=thres_best,gini=gini_input,d_gini=gini_r_best, label=label)</span><br><span class="line"></span><br><span class="line">        b_node.left, b_node.right = self.split(train_data, b_node)</span><br><span class="line"></span><br><span class="line">        s_node = DecisionNode(f_id=f_id_surg, thres=thres_surg, gini=gini_input, d_gini=gini_r_surg, label=label)</span><br><span class="line">        s_node.left, s_node.right = self.split(train_data, s_node)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> b_node, s_node</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">split</span><span class="params">(self, train_data, node)</span>:</span></span><br><span class="line">        train_x = train_data[<span class="number">0</span>]</span><br><span class="line">        train_y = train_data[<span class="number">1</span>]</span><br><span class="line">        </span><br><span class="line">        f_id = node.feature_id</span><br><span class="line">        thres = node.threshold</span><br><span class="line"></span><br><span class="line">        <span class="comment">## node's left child and right child index</span></span><br><span class="line">        left_id = np.where(train_x[:,f_id] &lt; thres)</span><br><span class="line">        right_id = np.where(train_x[:,f_id] &gt;= thres)</span><br><span class="line"></span><br><span class="line">        <span class="comment">## data in left child</span></span><br><span class="line">        left_x = train_x[left_id]</span><br><span class="line">        left_y = train_y[left_id]</span><br><span class="line">        left_p = np.sum(left_y == <span class="number">1</span>) / left_y.shape[<span class="number">0</span>]</span><br><span class="line">        left_gini = <span class="number">2</span>*left_p*(<span class="number">1</span>-left_p)</span><br><span class="line">        left_lb = <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> left_p &gt; <span class="number">0.5</span>:</span><br><span class="line">            left_lb = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            left_lb = <span class="number">0</span></span><br><span class="line">        left_node = DecisionNode(f_id=<span class="number">-1</span>, thres=<span class="keyword">None</span>, gini=left_gini, d_gini=<span class="number">0</span>, label=left_lb)</span><br><span class="line"></span><br><span class="line">        <span class="comment">## data in right child</span></span><br><span class="line">        right_x = train_x[right_id]</span><br><span class="line">        right_y = train_y[right_id]</span><br><span class="line">        right_p = np.sum(right_y == <span class="number">1</span>) / right_y.shape[<span class="number">0</span>]</span><br><span class="line">        right_gini = <span class="number">2</span>*right_p*(<span class="number">1</span>-right_p)</span><br><span class="line">        right_lb = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> right_p &gt; <span class="number">0.5</span>:</span><br><span class="line">            right_lb = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            right_lb = <span class="number">0</span></span><br><span class="line">        right_node = DecisionNode(f_id=<span class="number">-1</span>, thres=<span class="keyword">None</span>, gini=right_gini, d_gini=<span class="number">0</span>, label=right_lb)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> left_node, right_node</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">evaluateSample</span><span class="params">(self, x_input, node_type=<span class="string">'best node'</span>)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> node_type == <span class="string">'best node'</span>:</span><br><span class="line">            curr_node = self.best_node</span><br><span class="line">        <span class="keyword">elif</span> node_type == <span class="string">'surrogate node'</span>:</span><br><span class="line">            curr_node = self.surrogate_node</span><br><span class="line">        <span class="keyword">while</span> curr_node <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            f_id = curr_node.feature_id</span><br><span class="line">            thres = curr_node.threshold</span><br><span class="line">            <span class="keyword">if</span> curr_node.left <span class="keyword">is</span> <span class="keyword">None</span> <span class="keyword">and</span> curr_node.right <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">                <span class="keyword">return</span> curr_node.label</span><br><span class="line">            <span class="keyword">if</span> x_input[f_id] &lt; thres <span class="keyword">and</span> curr_node.left <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                curr_node = curr_node.left</span><br><span class="line">            <span class="keyword">if</span> x_input[f_id] &gt;= thres <span class="keyword">and</span> curr_node.right <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                curr_node = curr_node.right</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, x_input)</span>:</span></span><br><span class="line">        result = []</span><br><span class="line">        <span class="keyword">for</span> xs <span class="keyword">in</span> x_input:</span><br><span class="line">            result.append(self.evaluateSample(xs))</span><br><span class="line"></span><br><span class="line">        pre = np.array(result)</span><br><span class="line">        pre = pre.reshape((len(result),))</span><br><span class="line">        <span class="keyword">return</span> pre</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">computeError</span><span class="params">(self, x_input, y_input, error=<span class="string">'missclassification'</span>, node_type=<span class="string">'best node'</span>)</span>:</span></span><br><span class="line">        result = []</span><br><span class="line">        <span class="keyword">for</span> xs <span class="keyword">in</span> x_input:</span><br><span class="line">            result.append(self.evaluateSample(xs, node_type))</span><br><span class="line">            </span><br><span class="line">        pre = np.array(result)</span><br><span class="line">        pre = pre.reshape((len(result), <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> error == <span class="string">'missclassification'</span>:</span><br><span class="line">            res = np.absolute(pre - y_input)</span><br><span class="line">            err = np.sum(res == <span class="number">1</span>) / len(result)</span><br><span class="line">            <span class="keyword">return</span> err</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> error == <span class="string">'leastsquares'</span>:</span><br><span class="line">            res = np.sum(np.power((pre - y_input), <span class="number">2</span>))</span><br><span class="line">            <span class="comment">#res = np.dot((pre - y_input).T, (pre - y_input))</span></span><br><span class="line">            err = res / len(result)</span><br><span class="line">            <span class="keyword">return</span> err</span><br></pre></td></tr></table></figure></p>
<h3 id="Implementation-of-a-Random-Forest-with-Decision-Stump"><a href="#Implementation-of-a-Random-Forest-with-Decision-Stump" class="headerlink" title="Implementation of a Random Forest with Decision Stump"></a>Implementation of a Random Forest with Decision Stump</h3><p>The pseudo-code is<br>For t=1 to T:<br>    Draw a bootstrap sample of size n from the training data.<br>    Grow a tree using this splitting and stopping procedure:<br>        – Choose m features at random (out of total feature of p)<br>        – Evaluate the splitting criteria(Gini Reduction) on all of them<br>        – Split on the best feature<br>        – If the node has less than the minimum count we set, then stop splitting.<br>    Output all the trees.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## a decision forest contains with decision stump</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecisionForest</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.trainX = <span class="keyword">None</span></span><br><span class="line">        self.trainY = <span class="keyword">None</span></span><br><span class="line">        self.forest = <span class="keyword">None</span> <span class="comment">## it should be a list of decisiom stumps </span></span><br><span class="line">        self.samples = <span class="keyword">None</span> <span class="comment">## it should be a list a sample indexes for a corresponding stump above</span></span><br><span class="line">        self.features = <span class="keyword">None</span> <span class="comment">## it should be a list a feature indexes for a corresponding stump above</span></span><br><span class="line">        self.M = <span class="number">0</span> <span class="comment">## the numnber of stump </span></span><br><span class="line">        self.n = <span class="number">0</span> <span class="comment">## the number of samples</span></span><br><span class="line">        self.f = <span class="number">0</span> <span class="comment">## the number of sample features</span></span><br><span class="line">        self.k = <span class="number">0</span> <span class="comment">## number of feture to pick</span></span><br><span class="line">        self.q = <span class="number">0</span> <span class="comment">## rate the samples to pick</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, trainX, trainY, M, k=<span class="number">5</span>, q=<span class="number">0.8</span>)</span>:</span></span><br><span class="line">        self.trainX = trainX</span><br><span class="line">        self.trainY = trainY</span><br><span class="line">        </span><br><span class="line">        self.M = M</span><br><span class="line">        self.n = trainY.shape[<span class="number">0</span>] <span class="comment">## sample number</span></span><br><span class="line">        self.f = trainX.shape[<span class="number">1</span>] <span class="comment">## sample feature count</span></span><br><span class="line">        <span class="keyword">assert</span>(k &lt;= self.f)</span><br><span class="line">        self.k = k <span class="comment">## number of feature number to pick</span></span><br><span class="line">        self.q = q <span class="comment">## rate of sample to pick</span></span><br><span class="line">        </span><br><span class="line">        self.forest, self.samples, self.features = self.buildForest()</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">buildForest</span><span class="params">(self)</span>:</span></span><br><span class="line">        train_x = self.trainX</span><br><span class="line">        train_y = self.trainY</span><br><span class="line">        M, n, f, q = self.M, self.n, self.f, self.q</span><br><span class="line">        </span><br><span class="line">        sample_size = (int)(q * n)</span><br><span class="line">        forest = []</span><br><span class="line">        samples = []</span><br><span class="line">        features = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(M):</span><br><span class="line">            feature_selected = np.random.choice(f, size=self.k, replace=<span class="keyword">False</span>)</span><br><span class="line">            <span class="comment">#feature_selected.sort()</span></span><br><span class="line">            sample_selected = np.random.choice(n, size=sample_size, replace=<span class="keyword">True</span>)</span><br><span class="line">            sample_selected.sort()</span><br><span class="line">            <span class="comment">## pass the samples with all the features and a feature index dictionary </span></span><br><span class="line">            <span class="comment">## take samples</span></span><br><span class="line">            x_sampled = train_x[sample_selected, :]</span><br><span class="line">            y_sampled = train_y[sample_selected, :]</span><br><span class="line">            <span class="comment">## create a new stump</span></span><br><span class="line">            </span><br><span class="line">            temp_stump = DecisionStump()</span><br><span class="line">            temp_stump.fit((x_sampled, y_sampled), feature_dict = feature_selected)</span><br><span class="line">            forest.append(temp_stump)</span><br><span class="line">            samples.append(sample_selected)</span><br><span class="line">            features.append(feature_selected)</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> forest, samples, features</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">splitCount</span><span class="params">(self)</span>:</span></span><br><span class="line">        best_count = np.zeros(self.f)</span><br><span class="line">        surrogate_count = np.zeros(self.f)</span><br><span class="line">        <span class="keyword">for</span> stump <span class="keyword">in</span> self.forest:</span><br><span class="line">            <span class="keyword">if</span> stump.best_node <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                best_count[stump.best_node.feature_id] += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> stump.surrogate_node <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                surrogate_count[stump.surrogate_node.feature_id] +=<span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> best_count, surrogate_count</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">queryFeatureInTrees</span><span class="params">(self)</span>:</span></span><br><span class="line">        feature_choosen = np.zeros((self.f, self.M), dtype=<span class="string">'int'</span>)</span><br><span class="line">        <span class="keyword">for</span> i, feature_select <span class="keyword">in</span> enumerate(self.features):</span><br><span class="line">            feature_choosen[:,i][feature_select] += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> feature_choosen</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">variableImportance</span><span class="params">(self, stat=<span class="string">'mean'</span>)</span>:</span></span><br><span class="line">        trees_with_feature = self.queryFeatureInTrees()</span><br><span class="line">        feature_in_trees = np.sum(trees_with_feature, axis=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> stat == <span class="string">'mean'</span>:</span><br><span class="line">            imprt = np.zeros(self.f, dtype=<span class="string">'float32'</span>)</span><br><span class="line">            <span class="keyword">for</span> stump <span class="keyword">in</span> self.forest:</span><br><span class="line">                <span class="keyword">if</span> stump.best_node <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                    imprt[stump.best_node.feature_id] += stump.best_node.d_gini</span><br><span class="line">            imprt_mean = np.divide(imprt, feature_in_trees)</span><br><span class="line">            <span class="keyword">return</span> imprt_mean</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> stat == <span class="string">'standard_deviation'</span>:</span><br><span class="line">            imprt = np.zeros((self.f, self.M), dtype=<span class="string">'float32'</span>)</span><br><span class="line">            imprt_std = np.zeros(self.f, dtype=<span class="string">'float32'</span>)</span><br><span class="line">            <span class="keyword">for</span> j, stump <span class="keyword">in</span> enumerate(self.forest):</span><br><span class="line">                <span class="keyword">if</span> stump.best_node <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                    imprt[stump.best_node.feature_id][j] += stump.best_node.d_gini</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(self.f):</span><br><span class="line">                mask = trees_with_feature[i,:]</span><br><span class="line">                idx = np.where(mask == <span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">                imprt_i = imprt[i,:][idx]</span><br><span class="line">                imprt_std[i] = np.std(imprt_i)</span><br><span class="line">                </span><br><span class="line">            <span class="keyword">return</span> imprt_std</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">variableImportanceOOB</span><span class="params">(self, stat=<span class="string">'mean'</span>)</span>:</span></span><br><span class="line">        trees_with_feature = self.queryFeatureInTrees()</span><br><span class="line">        feature_in_trees = np.sum(trees_with_feature, axis=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> stat == <span class="string">'mean'</span>:</span><br><span class="line">            imprt = np.zeros(self.f, dtype=<span class="string">'float32'</span>)</span><br><span class="line">            <span class="keyword">for</span> f_id <span class="keyword">in</span> range(self.f):</span><br><span class="line">                trees = trees_with_feature[f_id,:]</span><br><span class="line">                trees_idx = np.where(trees == <span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">                sum_err = <span class="number">0</span></span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> trees_idx:</span><br><span class="line">                    stump, sample = self.forest[j], self.samples[j]</span><br><span class="line">                    mask = np.ones(self.n, np.bool)</span><br><span class="line">                    mask[sample] = <span class="number">0</span></span><br><span class="line">                    OOBx = train_x[mask]</span><br><span class="line">                    OOBy = train_y[mask]</span><br><span class="line">                    OOBx_p = np.array(OOBx)</span><br><span class="line">                    np.random.shuffle(OOBx_p[:, f_id])</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">if</span> stump.best_node <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                        err = stump.computeError(x_input=OOBx, y_input=OOBy, error=<span class="string">'leastsquares'</span>, node_type=<span class="string">'best node'</span>)</span><br><span class="line">                        err_p = stump.computeError(x_input=OOBx_p, y_input=OOBy, error=<span class="string">'leastsquares'</span>, node_type=<span class="string">'best node'</span>)</span><br><span class="line">                        sum_err += (err_p - err)</span><br><span class="line">                        </span><br><span class="line">                imprt[f_id] = np.divide(sum_err, feature_in_trees[f_id])</span><br><span class="line">            <span class="keyword">return</span> imprt</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> stat == <span class="string">'standard_deviation'</span>:</span><br><span class="line">            imprt_std = np.zeros(self.f, dtype=<span class="string">'float32'</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">for</span> f_id <span class="keyword">in</span> range(self.f):</span><br><span class="line">                trees = trees_with_feature[f_id,:]</span><br><span class="line">                trees_idx = np.where(trees == <span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">                imprt_fid = np.zeros(feature_in_trees[f_id], dtype=<span class="string">'float32'</span>) </span><br><span class="line">                </span><br><span class="line">                <span class="keyword">for</span> k, j <span class="keyword">in</span> enumerate(trees_idx):</span><br><span class="line">                    stump, sample = self.forest[j], self.samples[j]</span><br><span class="line">                    mask = np.ones(self.n, np.bool)</span><br><span class="line">                    mask[sample] = <span class="number">0</span></span><br><span class="line">                    OOBx = train_x[mask]</span><br><span class="line">                    OOBy = train_y[mask]</span><br><span class="line">                    OOBx_p = np.array(OOBx)</span><br><span class="line">                    np.random.shuffle(OOBx_p[:, f_id])</span><br><span class="line">                    </span><br><span class="line">                    <span class="keyword">if</span> stump.best_node <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                        err = stump.computeError(x_input=OOBx, y_input=OOBy, error=<span class="string">'leastsquares'</span>, node_type=<span class="string">'best node'</span>)</span><br><span class="line">                        err_p = stump.computeError(x_input=OOBx_p, y_input=OOBy, error=<span class="string">'leastsquares'</span>, node_type=<span class="string">'best node'</span>)</span><br><span class="line">                        imprt_fid[k] += (err_p - err)</span><br><span class="line">                <span class="comment">#print(feature_in_trees[f_id])</span></span><br><span class="line">                imprt_std[f_id] = np.std(imprt_fid, axis=<span class="number">0</span>)</span><br><span class="line">                </span><br><span class="line">            <span class="keyword">return</span> imprt_std      </span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">                    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">computeError</span><span class="params">(self, x_input, y_input, error=<span class="string">'major_vote'</span>)</span>:</span></span><br><span class="line">        n_input, f_input = x_input.shape</span><br><span class="line">        <span class="keyword">assert</span>(f_input == self.f)</span><br><span class="line">        </span><br><span class="line">        result = np.zeros((n_input, self.M), dtype=<span class="string">'int'</span>)</span><br><span class="line">        <span class="keyword">for</span> i, stump <span class="keyword">in</span> enumerate(self.forest):</span><br><span class="line">            pre = stump.predict(x_input)</span><br><span class="line">            result[:, i] = pre</span><br><span class="line">            </span><br><span class="line">        err = <span class="keyword">None</span></span><br><span class="line">        <span class="keyword">if</span> error == <span class="string">'major_vote'</span>:</span><br><span class="line">            result_voted = np.zeros((n_input, <span class="number">1</span>), dtype=<span class="string">'int'</span>)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(n_input):</span><br><span class="line">                neg_vote = np.sum(result[i,:] == <span class="number">0</span>)</span><br><span class="line">                pos_vote = np.sum(result[i,:] == <span class="number">1</span>)</span><br><span class="line">                <span class="keyword">if</span> pos_vote &gt; neg_vote:</span><br><span class="line">                    result_voted[i] = <span class="number">1</span></span><br><span class="line">                <span class="keyword">if</span> pos_vote &lt; neg_vote:</span><br><span class="line">                    result_voted[i] = <span class="number">0</span></span><br><span class="line">                    </span><br><span class="line">            res = np.sum(np.power((result_voted - y_input), <span class="number">2</span>))</span><br><span class="line">            err = res / n_input</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">if</span> error == <span class="string">'individual'</span>:</span><br><span class="line">            res = np.sum(np.power((result - y_input), <span class="number">2</span>))</span><br><span class="line">            err = res/(n_input*self.M)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> err</span><br></pre></td></tr></table></figure></p>

    </article>
    <!-- 前后页  -->
    <ul class="post-pager">
        
            <li class="next">
                <a href= "/2018/03/01/Support-Vector-Machine-Implementation/" title= Support Vector Machine Implementation With Python >
                    <span>Next Post</span>
                    <span>Support Vector Machine Implementation With Python</span>
                </a>
            </li>
        
        
            <li class="previous">
                <a href= "/2018/02/01/Deep-Neural-NetWork-Sorting/" title= Deep Neural NetWork Sorting >
                    <span>Previous Post</span>
                    <span>Deep Neural NetWork Sorting</span>
                </a>
            </li>
        
    </ul>
    <!-- 评论插件 -->
    <!-- 来必力City版安装代码 -->

<!-- City版安装代码已完成 -->
    
    
<div id="container"></div>
<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script>
    var gitment = new Gitment({
        id: "Decision Tree and Random Forest", // 可选。默认为 location.href
        owner: 'wushbin',
        repo: 'wushbin.github.io',
        oauth: {
            client_id: '17eb7cee5fa4bc08355b',
            client_secret: '176f2991889fafe11f2ae7670c5f1dbe879ce0eb',
        },
    })
    gitment.render('container')

</script>

    <!--PC版-->

    <!--PC版-->


    
</main>
            <!-- profile -->
            
        </div>
        <footer class="footer footer-unloaded">
    <!-- social  -->
    
    <div class="social">
        
    
        
            
                <a href="mailto:ngshbin@gmail.com" class="iconfont-archer email" title=email ></a>
            
        
    
        
            
                <a href="//github.com/wushbin" class="iconfont-archer github" target="_blank" title="github"></a>
            
        
    
        
    
        
            
                <a href="https://www.linkedin.com/in/shengbin-wu/" class="iconfont-archer linkedin" target="_blank" title="linkedin"></a>
            
        
    
        
    

    </div>
    
    <!-- powered by Hexo  -->
    <div class="copyright">
        <span id="hexo-power">Powered by <a href="https://hexo.io/" target="_blank">Hexo</a></span><span class="iconfont-archer power">&#xe635;</span><span id="theme-info">Theme <a href="https://github.com/fi3ework/hexo-theme-archer" target="_blank">archer</a></span>
    </div>
    <!-- 不蒜子  -->
    
</footer>
    </div>
    <!-- toc -->
    
    <div class="toc-wrapper">
        <div class="toc-catalog">
            <span class="iconfont-archer catalog-icon">&#xe613;</span><span>CATALOG</span>
        </div>
        <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Implementation-of-A-Decision-Tree-Node"><span class="toc-number">1.</span> <span class="toc-text">Implementation of A Decision Tree Node</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Implementation-of-A-Decision-Tree"><span class="toc-number">2.</span> <span class="toc-text">Implementation of A Decision Tree</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Implementation-of-Decision-Stump"><span class="toc-number">3.</span> <span class="toc-text">Implementation of Decision Stump</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Implementation-of-a-Random-Forest-with-Decision-Stump"><span class="toc-number">4.</span> <span class="toc-text">Implementation of a Random Forest with Decision Stump</span></a></li></ol>
    </div>
    
    <div class="back-top">&#xe639;</div>
    <div class="sidebar">
    <div class="sidebar-header sidebar-header-show-archive">
        <div class="sidebar-category">
            <span class="sidebar-archive-link"><span class="iconfont-archer">&#xe67d;</span>Archive</span>
            <span class="sidebar-tags-link"><span class="iconfont-archer">&#xe610;</span>Tag</span>
        </div>
    </div>
    <div class="sidebar-content sidebar-content-show-archive">
          <div class="sidebar-archive">
    <!-- 在ejs中将archive按照时间排序 -->
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <div class="total-archive"> Total : 12 </div>
    
    <div class="post-archive">
    
    
    
    
    <div class="archive-year"> 2018 </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/01</span><a class="archive-post-title" href= "/2018/03/01/Support-Vector-Machine-Implementation/" >Support Vector Machine Implementation With Python</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/02</span><a class="archive-post-title" href= "/2018/02/02/Decision-Tree/" >Decision Tree and Random Forest</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/01</span><a class="archive-post-title" href= "/2018/02/01/Deep-Neural-NetWork-Sorting/" >Deep Neural NetWork Sorting</a>
        </li>
    
    
    
    
    
        </ul>
    
    <div class="archive-year"> 2017 </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">12/23</span><a class="archive-post-title" href= "/2017/12/23/Speech-Recognition/" >Speech Recognition</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">12/23</span><a class="archive-post-title" href= "/2017/12/23/Optical-Character-Recognition/" >Optical Character Recognition</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/12</span><a class="archive-post-title" href= "/2017/10/12/Sharing-On-Campus-Web-Application/" >Sharing On Campus: Web Application</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/09</span><a class="archive-post-title" href= "/2017/10/09/Word-Embedding-Syntactics-or-Semantics/" >Word Embedding: Syntactics or Semantics</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/23</span><a class="archive-post-title" href= "/2017/09/23/Hidden-Markov-Model-for-Part-of-Speech-Tagging/" >Hidden Markov Model for Part of Speech Tagging</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/02</span><a class="archive-post-title" href= "/2017/05/02/cat-and-dog/" >cat-and-dog</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/01</span><a class="archive-post-title" href= "/2017/05/01/Duke-Gather-Android-Application/" >Duke Gather: Android Application</a>
        </li>
    
    
    
    
    
        </ul>
    
    <div class="archive-year"> 2016 </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/01</span><a class="archive-post-title" href= "/2016/06/01/High-Speed-Test-Rig-Development/" >High Speed Test Rig Development</a>
        </li>
    
    
    
    
    
        </ul>
    
    <div class="archive-year"> 2013 </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/01</span><a class="archive-post-title" href= "/2013/06/01/SCARA-Robot/" >SCARA Robot</a>
        </li>
    
    </div>
  </div>
        <div class="sidebar-tags">
    <div class="sidebar-tags-name">
    
        <span class="sidebar-tag-name"><a href= "#">machine learning</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">Java</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">Mobile Application</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">natural language processing</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">mechanical design</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">experiment</a></span>
    
    </div>
    <div class="tag-load-fail" style="display: none; color: #ccc; font-size: 0.6rem;">
    缺失模块。<br/>
    1、请确保node版本大于6.2<br/>
    2、在博客根目录（注意不是archer根目录）执行以下命令：<br/>
    <span style="color: #f75357; font-size: 1rem; line-height: 2rem;">npm i hexo-generator-json-content --save</span><br/>
    3、在根目录_config.yml里添加配置：
    <pre style="color: #787878; font-size: 0.6rem;">
jsonContent:
  meta: false
  pages: false
  posts:
    title: true
    date: true
    path: true
    text: false
    raw: false
    content: false
    slug: false
    updated: false
    comments: false
    link: false
    permalink: false
    excerpt: false
    categories: false
    tags: true</pre>
    </div> 
    <div class="sidebar-tag-list"></div>
</div>
    </div>
</div> 
    <script>
    var jsInfo = {
        root: '/'
    }
</script>
    <!-- 不蒜子  -->
    
    <!-- CNZZ统计  -->
    
    </div>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>

<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

</body>
</html>


