<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>wushbin&#39;s studio</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://wushbin.github.io/"/>
  <updated>2017-12-23T13:54:37.995Z</updated>
  <id>https://wushbin.github.io/</id>
  
  <author>
    <name>shengbin wu</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>PacMan Game: Artificial Intellegence</title>
    <link href="https://wushbin.github.io/2017/12/23/PacMan-Game-Artificial-Intellegence/"/>
    <id>https://wushbin.github.io/2017/12/23/PacMan-Game-Artificial-Intellegence/</id>
    <published>2017-12-24T02:54:37.000Z</published>
    <updated>2017-12-23T13:54:37.995Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Hidden Markov Model for Part of Speech Tagging</title>
    <link href="https://wushbin.github.io/2017/12/23/Hidden-Markov-Model-for-Part-of-Speech-Tagging/"/>
    <id>https://wushbin.github.io/2017/12/23/Hidden-Markov-Model-for-Part-of-Speech-Tagging/</id>
    <published>2017-12-24T02:53:54.000Z</published>
    <updated>2018-02-13T19:29:58.514Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>In this post, we will train a Hidden Markov Model for part of speech tagging. The first 10K tagged sentence of ‘news’ in the brown corpus will be used to train the Hidden Markov Model. A sentence from ‘alice’ will be infered by this Hidden Markov Model. </p><h4 id="Extract-data"><a href="#Extract-data" class="headerlink" title="Extract data"></a>Extract data</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> nltk.tag.sequential <span class="keyword">import</span> UnigramTagger</span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> brown</span><br><span class="line"><span class="keyword">from</span> nltk.tokenize <span class="keyword">import</span> TreebankWordTokenizer</span><br><span class="line"></span><br><span class="line">alice = nltk.corpus.gutenberg.raw(<span class="string">'carroll-alice.txt'</span>)</span><br><span class="line">tagger = UnigramTagger(brown.tagged_sents(categories=<span class="string">'news'</span>)[:<span class="number">10000</span>])</span><br><span class="line">tokens = TreebankWordTokenizer().tokenize(alice[:<span class="number">1000</span>])</span><br><span class="line">tags = tagger.tag(tokens)</span><br><span class="line">sents = brown.tagged_sents(categories=<span class="string">'news'</span>)[:<span class="number">10000</span>]</span><br><span class="line">words = [w[<span class="number">0</span>] <span class="keyword">for</span> s <span class="keyword">in</span> sents <span class="keyword">for</span> w <span class="keyword">in</span> s]</span><br></pre></td></tr></table></figure><h5 id="Compress-the-tags"><a href="#Compress-the-tags" class="headerlink" title="Compress the tags"></a>Compress the tags</h5><p>The tags provided by brown corpus is to detail, it can be compressed in some extent.<br>Detelte the postfix of the generated tags, as followed. All the dashes and everything behind them are removed and all the asterisks are also removed, while the $ is kept as its original way.  </p><blockquote><p>NN-TL -&gt; NN<br>  BEZ<em> -&gt; BEZ<br>  FW-</em> -&gt; FW<br>  :-HL -&gt; :  </p></blockquote><p>After this process, the tags like “NN-TL” and “NN-JJ” become “NN” without its postfix and “JJ-TL” will become “JJ”.<br>Before compressing the tags, the total number of tags is 218 and after compressing,  the number of tags decrease to 88. Although it will have some effect on the precision of tags but it will improve the accurary of the output prediction.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">sents_temp = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(sents)):</span><br><span class="line">    sents_revised_list = []</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(len(sents[i])):</span><br><span class="line">        content = list(sents[i][j])</span><br><span class="line">        <span class="comment">#content[1]= re.sub("([\$]*[\-\+]+[\w\$\*]+)+$|\$+$|(?&lt;=\w)\*+$", "", content[1]) ## remove the $ version</span></span><br><span class="line">        content[<span class="number">1</span>]= re.sub(<span class="string">"([\-\+]+[\w\$\*]+)+$|(?&lt;=\w)\*+$"</span>, <span class="string">""</span>, content[<span class="number">1</span>]) <span class="comment">## keep the $ version</span></span><br><span class="line">        content[<span class="number">1</span>]= re.sub(<span class="string">"(?&lt;=\w)\*$"</span>, <span class="string">""</span>, content[<span class="number">1</span>])</span><br><span class="line">        content_tuple = tuple(content)</span><br><span class="line">        sents_revised_list.append(content_tuple)</span><br><span class="line">    sents_temp.append(sents_revised_list)</span><br><span class="line">temp = sents</span><br><span class="line">sents = sents_temp</span><br><span class="line">sents_temp = temp</span><br></pre></td></tr></table></figure></p><h4 id="Generate-a-set-of-tags-and-a-set-of-words"><a href="#Generate-a-set-of-tags-and-a-set-of-words" class="headerlink" title="Generate a set of tags and a set of words"></a>Generate a set of tags and a set of words</h4><ul><li>python can maintain a set without duplicated elements. After the sets of tags and words is built, a list of tags and a list of words which contains all the elements in the set are generated. And then dictionarys for tags index and word index are generated.  </li><li>The tags list and words list are used to get a tag or a word via its index. And the tags dictionanry and words dictionary are used to get their index.</li><li>In the tags list and tags set, the tags of start state and end state are added.  </li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">sents_list = [t <span class="keyword">for</span> s <span class="keyword">in</span> sents <span class="keyword">for</span> t <span class="keyword">in</span> s]</span><br><span class="line">tags_set = set()</span><br><span class="line">words_set = set()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(sents_list)):</span><br><span class="line">    tags_set.add(sents_list[i][<span class="number">1</span>])</span><br><span class="line">    words_set.add(sents_list[i][<span class="number">0</span>])</span><br><span class="line">tags_list = list(tags_set)</span><br><span class="line">words_list = list(words_set)</span><br><span class="line"></span><br><span class="line"><span class="comment">## add the start and end state</span></span><br><span class="line">tags_list.insert(<span class="number">0</span>, <span class="string">'START_STATE'</span>)</span><br><span class="line">tags_list.insert(len(tags_list), <span class="string">'END_STATE'</span>)</span><br><span class="line"></span><br><span class="line">tags_hash = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> index, tag <span class="keyword">in</span> enumerate(tags_list):</span><br><span class="line">    tags_hash[tag] = index</span><br><span class="line">words_hash = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> index, word <span class="keyword">in</span> enumerate(words_list):</span><br><span class="line">    words_hash[word] = index</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Tags after compressing:"</span>)</span><br><span class="line">print(tags_hash)</span><br></pre></td></tr></table></figure><blockquote><p>Tags after compressing:<br>{‘START_STATE’: 0, ‘NPS’: 1, ‘WPO’: 2, ‘PPLS’: 3, ‘BEN’: 4, ‘NNS’: 5, ‘BER’: 6, ‘VBN’: 7, ‘CS’: 8, ‘TO’: 9, ‘DTX’: 10, ‘)’: 11, ‘,’: 12, ‘NP’: 13, ‘VBD’: 14, ‘FW’: 15, ‘PP$$’: 16, ‘JJR’: 17, ‘WPS’: 18, ‘OD’: 19, ‘QLP’: 20, ‘AT’: 21, ‘EX’: 22, ‘CC’: 23, ‘HVD’: 24, ‘PPS’: 25, ‘PP$’: 26, ‘NR$’: 27, ‘``’: 28, ‘QL’: 29, ‘JJS’: 30, ‘BE’: 31, ‘HVN’: 32, ‘PN$’: 33, ‘ABN’: 34, ‘NN$’: 35, ‘AP’: 36, ‘NNS$’: 37, ‘RB$’: 38, ‘VBZ’: 39, ‘NR’: 40, ‘PPL’: 41, ‘NP$’: 42, ‘DT$’: 43, ‘BEDZ’: 44, ‘RBT’: 45, ‘MD’: 46, ‘DT’: 47, ‘NN’: 48, ‘ABL’: 49, ‘BEM’: 50, ‘BED’: 51, ‘AP$’: 52, ‘HV’: 53, ‘(‘: 54, ‘WDT’: 55, ‘DTS’: 56, ‘RP’: 57, ‘VBG’: 58, ‘HVG’: 59, ‘NPS$’: 60, ‘BEZ’: 61, ‘JJT’: 62, ‘.’: 63, “‘’”: 64, ‘DOZ’: 65, ‘ABX’: 66, ‘CD’: 67, ‘DOD’: 68, ‘RB’: 69, ‘DO’: 70, ‘BEG’: 71, ‘RBR’: 72, “‘“: 73, ‘—‘: 74, ‘PPSS’: 75, ‘WQL’: 76, ‘*’: 77, ‘IN’: 78, ‘VB’: 79, ‘PPO’: 80, ‘HVZ’: 81, ‘WP$’: 82, ‘WRB’: 83, ‘PN’: 84, ‘CD$’: 85, ‘JJ’: 86, ‘UH’: 87, ‘DTI’: 88, ‘:’: 89, ‘END_STATE’: 90}</p></blockquote><h3 id="Forward-Algorithm"><a href="#Forward-Algorithm" class="headerlink" title="Forward Algorithm"></a>Forward Algorithm</h3><h4 id="Transition-Probability-Matrix-A"><a href="#Transition-Probability-Matrix-A" class="headerlink" title="Transition Probability Matrix: A"></a>Transition Probability Matrix: A</h4><ul><li>The row length and column length both are the same as the length of tags</li><li>In order to reduce the effect of the zero values in this matrix, all elements in matrix A was added by a small value 1e-10 and then re-normalized.  </li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">A_len = len(tags_hash)</span><br><span class="line">A = np.zeros((A_len, A_len))</span><br><span class="line"></span><br><span class="line">start_state = tags_hash[<span class="string">'START_STATE'</span>]</span><br><span class="line">end_state = tags_hash[<span class="string">'END_STATE'</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(sents)):</span><br><span class="line">    </span><br><span class="line">    start_tag_index = tags_hash[sents[i][<span class="number">0</span>][<span class="number">1</span>]]</span><br><span class="line">    A[start_state][start_tag_index] += <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(len(sents[i]) - <span class="number">1</span>):</span><br><span class="line">        row_index = tags_hash[sents[i][j][<span class="number">1</span>]]</span><br><span class="line">        col_index = tags_hash[sents[i][j + <span class="number">1</span>][<span class="number">1</span>]]</span><br><span class="line">        A[row_index][col_index] += <span class="number">1</span></span><br><span class="line">    end_tag_index = tags_hash[sents[i][len(sents[i]) - <span class="number">1</span>][<span class="number">1</span>]]</span><br><span class="line">    A[end_tag_index][end_state] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">A[A_len - <span class="number">1</span>] += <span class="number">1e-10</span></span><br><span class="line">A_row_sum = np.sum(A, axis = <span class="number">1</span>)</span><br><span class="line">A_row_sum = A_row_sum.reshape(A.shape[<span class="number">0</span>],<span class="number">1</span>)</span><br><span class="line">A_normalized = A / A_row_sum</span><br><span class="line"><span class="comment">## re-normalized</span></span><br><span class="line">A_normalized += <span class="number">1e-10</span></span><br><span class="line">A_row_sum = np.sum(A_normalized, axis = <span class="number">1</span>)</span><br><span class="line">A_row_sum = A_row_sum.reshape(A_normalized.shape[<span class="number">0</span>],<span class="number">1</span>)</span><br><span class="line">A_normalized = A_normalized / A_row_sum</span><br></pre></td></tr></table></figure><h4 id="State-Observation-Likelihood-Matrix-B"><a href="#State-Observation-Likelihood-Matrix-B" class="headerlink" title="State Observation Likelihood Matrix: B"></a>State Observation Likelihood Matrix: B</h4><ul><li>The row of matrix B is the tags generated, the column of matrix B is the words in our trainnign set.  </li><li>In order to reduce the effect of the zero values in this matrix, all elements in matrix B was added by a small value 1e-10 and then re-normalized.  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">B_row = len(tags_hash)</span><br><span class="line">B_col = len(words_hash)</span><br><span class="line">B = np.zeros((B_row, B_col))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(sents_list)):</span><br><span class="line">    row_index = tags_hash[sents_list[i][<span class="number">1</span>]]</span><br><span class="line">    col_index = words_hash[sents_list[i][<span class="number">0</span>]]</span><br><span class="line">    B[row_index][col_index] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">B_col_sum = np.sum(B, axis = <span class="number">0</span>)</span><br><span class="line">B_col_sum = B_col_sum.reshape(<span class="number">1</span>, B_col)</span><br><span class="line">B_normalized = B / B_col_sum</span><br><span class="line"></span><br><span class="line"><span class="comment">## re-normalized</span></span><br><span class="line">B_normalized += <span class="number">1e-10</span></span><br><span class="line">B_col_sum = np.sum(B_normalized, axis = <span class="number">0</span>)</span><br><span class="line">B_col_sum = B_col_sum.reshape(<span class="number">1</span>, B_col)</span><br><span class="line">B_normalized = B_normalized / B_col_sum</span><br></pre></td></tr></table></figure></li></ul><h4 id="Get-a-test-case"><a href="#Get-a-test-case" class="headerlink" title="Get a test case"></a>Get a test case</h4><p>In this post, we use a sentence from ‘alice’<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.tokenize <span class="keyword">import</span> word_tokenize</span><br><span class="line">test_case = nltk.corpus.gutenberg.raw(<span class="string">'carroll-alice.txt'</span>)[<span class="number">91</span>:<span class="number">683</span>]</span><br><span class="line">words_test = word_tokenize(test_case)</span><br></pre></td></tr></table></figure></p><h4 id="Forward-Algorithm-1"><a href="#Forward-Algorithm-1" class="headerlink" title="Forward Algorithm"></a>Forward Algorithm</h4><p>The cell below is the implementation of forward algorithm. The matrix ‘forward’ is the porbablity of states in the test sequence.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">num_state = len(tags_hash)</span><br><span class="line">num_obv = len(words_test)</span><br><span class="line">forward = np.zeros((num_obv, num_state))</span><br><span class="line">start_state = tags_hash[<span class="string">'START_STATE'</span>]</span><br><span class="line">end_state = tags_hash[<span class="string">'END_STATE'</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    word_index = words_hash[words_test[<span class="number">0</span>]]</span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> range(num_state):</span><br><span class="line">        forward[<span class="number">0</span>][y] = A_normalized[start_state][y] * B_normalized[y][word_index]</span><br><span class="line">        </span><br><span class="line"><span class="keyword">except</span> KeyError:</span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> range(num_state):</span><br><span class="line">        forward[<span class="number">0</span>][y] = A_normalized[start_state][y] * (<span class="number">1</span>/num_state)</span><br><span class="line">        </span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">1</span>, num_obv):</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        word_index = words_hash[words_test[t]]</span><br><span class="line">        <span class="keyword">for</span> y <span class="keyword">in</span> range(num_state):</span><br><span class="line">            <span class="keyword">for</span> y_pre <span class="keyword">in</span> range(num_state):</span><br><span class="line">                forward[t][y] += forward[t<span class="number">-1</span>][y_pre] * A_normalized[y_pre][y] * B_normalized[y][word_index]</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">except</span> KeyError:</span><br><span class="line">        <span class="keyword">for</span> y <span class="keyword">in</span> range(num_state):</span><br><span class="line">            <span class="keyword">for</span> y_pre <span class="keyword">in</span> range(num_state):</span><br><span class="line">                forward[t][y] += forward[t<span class="number">-1</span>][y_pre] * A_normalized[y_pre][y] * (<span class="number">1</span>/num_state)</span><br></pre></td></tr></table></figure></p><h4 id="Decoding-of-Forward-Algorithm"><a href="#Decoding-of-Forward-Algorithm" class="headerlink" title="Decoding of Forward Algorithm"></a>Decoding of Forward Algorithm</h4><p>For a given forward matrix, the state in a step can be predicted by finding a state with largest probability in this step.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">forward_output = []</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(num_obv):</span><br><span class="line">    mx = <span class="number">-1</span></span><br><span class="line">    tag_id = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> index, tags <span class="keyword">in</span> enumerate(forward[t]):</span><br><span class="line">        <span class="keyword">if</span> mx &lt; forward[t][index]:</span><br><span class="line">            mx = forward[t][index]</span><br><span class="line">            tag_id = index</span><br><span class="line">    forward_output.append((words_test[t], tags_list[tag_id]))</span><br></pre></td></tr></table></figure></p><h4 id="The-result-of-forward-algorithm-is-shown-as-belowed"><a href="#The-result-of-forward-algorithm-is-shown-as-belowed" class="headerlink" title="The result of forward algorithm is shown as belowed."></a>The result of forward algorithm is shown as belowed.</h4><p>Because the tags are compressed, all the tags in the results are without profix.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> (<span class="string">"The result of test case from forward algorithms is as belowed"</span>)</span><br><span class="line">print(forward_output)</span><br></pre></td></tr></table></figure></p><blockquote><p>The result of test case from forward algorithms is as belowed<br>[(‘Alice’, ‘NP’), (‘was’, ‘BEDZ’), (‘beginning’, ‘NN’), (‘to’, ‘IN’), (‘get’, ‘VB’), (‘very’, ‘QL’), (‘tired’, ‘VBN’), (‘of’, ‘IN’), (‘sitting’, ‘VBG’), (‘by’, ‘IN’), (‘her’, ‘PP$’), (‘sister’, ‘NN’), (‘on’, ‘IN’), (‘the’, ‘AT’), (‘bank’, ‘NN’), (‘,’, ‘,’), (‘and’, ‘CC’), (‘of’, ‘IN’), (‘having’, ‘HVG’), (‘nothing’, ‘PN’), (‘to’, ‘IN’), (‘do’, ‘DO’), (‘:’, ‘:’), (‘once’, ‘RB’), (‘or’, ‘CC’), (‘twice’, ‘RB’), (‘she’, ‘PPS’), (‘had’, ‘HVD’), (‘peeped’, ‘VBN’), (‘into’, ‘IN’), (‘the’, ‘AT’), (‘book’, ‘NN’), (‘her’, ‘PP$’), (‘sister’, ‘NN’), (‘was’, ‘BEDZ’), (‘reading’, ‘VBG’), (‘,’, ‘,’), (‘but’, ‘CC’), (‘it’, ‘PPS’), (‘had’, ‘HVD’), (‘no’, ‘AT’), (‘pictures’, ‘NNS’), (‘or’, ‘CC’), (‘conversations’, ‘NNS’), (‘in’, ‘IN’), (‘it’, ‘PPO’), (‘,’, ‘,’), (“‘and”, ‘NP’), (‘what’, ‘WDT’), (‘is’, ‘BEZ’), (‘the’, ‘AT’), (‘use’, ‘NN’), (‘of’, ‘IN’), (‘a’, ‘AT’), (‘book’, ‘NN’), (‘,’, ‘,’), (“‘“, “‘“), (‘thought’, ‘VBN’), (‘Alice’, ‘NP’), (“‘without”, ‘NP’), (‘pictures’, ‘NNS’), (‘or’, ‘CC’), (‘conversation’, ‘NN’), (‘?’, ‘.’), (“‘“, “‘“), (‘So’, ‘RB’), (‘she’, ‘PPS’), (‘was’, ‘BEDZ’), (‘considering’, ‘VBG’), (‘in’, ‘IN’), (‘her’, ‘PP$’), (‘own’, ‘JJ’), (‘mind’, ‘NN’), (‘(‘, ‘(‘), (‘as’, ‘CS’), (‘well’, ‘RB’), (‘as’, ‘CS’), (‘she’, ‘PPS’), (‘could’, ‘MD’), (‘,’, ‘,’), (‘for’, ‘IN’), (‘the’, ‘AT’), (‘hot’, ‘JJ’), (‘day’, ‘NN’), (‘made’, ‘VBD’), (‘her’, ‘PP$’), (‘feel’, ‘NN’), (‘very’, ‘QL’), (‘sleepy’, ‘JJ’), (‘and’, ‘CC’), (‘stupid’, ‘NP’), (‘)’, ‘)’), (‘,’, ‘,’), (‘whether’, ‘CS’), (‘the’, ‘AT’), (‘pleasure’, ‘NN’), (‘of’, ‘IN’), (‘making’, ‘VBG’), (‘a’, ‘AT’), (‘daisy-chain’, ‘NN’), (‘would’, ‘MD’), (‘be’, ‘BE’), (‘worth’, ‘JJ’), (‘the’, ‘AT’), (‘trouble’, ‘NN’), (‘of’, ‘IN’), (‘getting’, ‘VBG’), (‘up’, ‘RP’), (‘and’, ‘CC’), (‘picking’, ‘VBG’), (‘the’, ‘AT’), (‘daisies’, ‘NN’), (‘,’, ‘,’), (‘when’, ‘WRB’), (‘suddenly’, ‘RB’), (‘a’, ‘AT’), (‘White’, ‘JJ’), (‘Rabbit’, ‘NN’), (‘with’, ‘IN’), (‘pink’, ‘JJ’), (‘eyes’, ‘NNS’), (‘ran’, ‘VBD’), (‘close’, ‘RB’), (‘by’, ‘IN’), (‘her’, ‘PP$’), (‘.’, ‘.’)]</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h3&gt;&lt;p&gt;In this post, we will train a 
      
    
    </summary>
    
    
      <category term="machine learning" scheme="https://wushbin.github.io/tags/machine-learning/"/>
    
      <category term="natural language processing" scheme="https://wushbin.github.io/tags/natural-language-processing/"/>
    
  </entry>
  
  <entry>
    <title>Optical Character Recognition</title>
    <link href="https://wushbin.github.io/2017/12/23/Optical-Character-Recognition/"/>
    <id>https://wushbin.github.io/2017/12/23/Optical-Character-Recognition/</id>
    <published>2017-12-24T02:53:38.000Z</published>
    <updated>2017-12-23T13:53:38.203Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Speech Recognition</title>
    <link href="https://wushbin.github.io/2017/12/23/Speech-Recognition/"/>
    <id>https://wushbin.github.io/2017/12/23/Speech-Recognition/</id>
    <published>2017-12-24T02:51:56.000Z</published>
    <updated>2017-12-23T13:51:56.572Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Sharing On Campus: Web Application</title>
    <link href="https://wushbin.github.io/2017/10/12/Sharing-On-Campus-Web-Application/"/>
    <id>https://wushbin.github.io/2017/10/12/Sharing-On-Campus-Web-Application/</id>
    <published>2017-10-13T01:41:29.000Z</published>
    <updated>2018-01-13T21:58:42.165Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://github.com/wushbin/sharingoncampus" target="_blank" rel="noopener">link to this project in github</a><br><a href="https://sharingoncampus.herokuapp.com/" target="_blank" rel="noopener">link to webapplication</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://github.com/wushbin/sharingoncampus&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;link to this project in github&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https:
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Word Embedding: Syntactics or Semantics</title>
    <link href="https://wushbin.github.io/2017/10/09/Word-Embedding-Syntactics-or-Semantics/"/>
    <id>https://wushbin.github.io/2017/10/09/Word-Embedding-Syntactics-or-Semantics/</id>
    <published>2017-10-10T01:39:09.000Z</published>
    <updated>2018-02-13T18:25:21.581Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>Word embedding is a mapping from words to vector representations of the words. The method that represents a word as a vector or array of numbers related in someway to counts is called vector semantics. Ideally, the geometry of the vectors will capture the semantic and syntactic meaning of the words—for example, words similar in meaning should have representations which are close to each other in the vector space.<br>In this project, a word co-occurrence matrix of 10,000 words from Wikipedia corpus with 1.5 billion words will be used to study the two method learned from class. Entry of the matrix denotes the number of times in the corpus this the ith and jth words occur within 5 words of each other The data file in this project is downloaded from Standford CS168 course website. The dictionary in this data set is sorted by frequency, thus the first row of the co-occurrence matrix is the most frequent word.<br><a href="http://web.stanford.edu/class/cs168/co_occur.csv" target="_blank" rel="noopener">data file</a></p><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><h4 id="Sparse-Vector-Representation"><a href="#Sparse-Vector-Representation" class="headerlink" title="Sparse Vector Representation"></a>Sparse Vector Representation</h4><p>The co-occurrence matrix in represented each cell by the raw frequency of the co-occurrence of two words. The raw frequency in a matrix may be skewed. Pointwise mutual information PPMI is a good measure for association between words which can tell us how much often the two words occur.<br>The pointwise mutual information is a measure of how often two events x and y occur, compared with what we would expect if they were independent:</p><script type="math/tex; mode=display">PMI(x,y) = log_2\frac{P(x,y)}{P(x)P(y)}</script><p>PMI between two words is:</p><script type="math/tex; mode=display">PMI(word_1,word_2) = log_2\frac{P(word_1,word_2)}{P(word_1)P(word_2)}</script><p>Positive PMI(PPMI) replace all negative values of PMI by 0, which eliminate the problem of unreliable negative PMI values.</p><script type="math/tex; mode=display">PPMI(word_1,word_2) = max(log_2\frac{P(word_1,word_2)}{P(word_1)P(word_2)}, 0)</script><p>For infrequent events, PPMI is bias which would have high PMI values for very rare word. There are two solution for this problem, one is to give rare word slightly higher probabilities, one is add-one smoothing.<br>In this project, the PPMI with add-two smoothing is used to build a model which can compute the similar target words of a given context words. In this project, the similarity of two words will be defined as the cosine-similarity between their embeddings.<br>Then, analogy analysis is performed in this model. Analogy analysis is by given two words with some relation and a third word, find the fourth word which has the similar relation with the third word as the relation between the first two words.<br>For example, if given ‘man’, ‘woman’, ‘king’, the fourth word should be ‘queen’.  </p><h4 id="Cosine-Similarity"><a href="#Cosine-Similarity" class="headerlink" title="Cosine Similarity"></a>Cosine Similarity</h4><p>Given two word vector, we can measure the similarity of these two vectoe by the inner product or dot product of them.  </p><script type="math/tex; mode=display">dotProduct(v, w) = v \cdot w = v_1w_1 + v_2w_2 + ...+ v_nw_n</script><p>The dot product is high when two vectors have large values in same dimension and it value is low when they are orthogonal vectors with complementary distributions. However, a problem exist in this metric, dot product will be longer is these two vectors are longer, the length of a vector is  </p><script type="math/tex; mode=display">|v| = \sqrt{\sum_{i = 1}^{N} v_i^2}</script><p>This means that more frequent words will have higher dot product. The solution for this problem is dividing the dot product by the length of two vectors,which is the $cosine$ of the angle bwtween these two vectors.  </p><h4 id="Dense-Vector-Representation"><a href="#Dense-Vector-Representation" class="headerlink" title="Dense Vector Representation"></a>Dense Vector Representation</h4><p>Short and dense vectors have a number of potential advantages. Short vectors may be easier    to use as features in machine learning. Dense    vectors    may    generalize    better than storing explicit counts. In addition, dense vectors may perform better in capturing synonymy than sparse vectors.<br>Singular Value Decomposition(SVD) is a classic method for generating dense vectors, which is applied to a task of generating embedding from term document matrices in a model called Latent Semantic Analysis (LSA).<br>SVD factorizes any a rectangular matrix $X$ into the product of three matrices $W, s, C$.<br>For $W$, its rows corresponding    to original matrix but columns    represents a dimension in a    new    latent space, such that the column vectors are orthogonal to each other. Columns are ordered    by the amount of variance in the dataset each new dimension accounts for.<br>$S$ is a diagonal matrix of singular values expressing    the importance of each dimension.$C$ represents documents or contexts, but each row now represents one of the new latent dimensions and the m row vectors are orthogonal to each other.<br>In this project, SVD is applied to the term-term matrix, let $M$ denote the$10000 \times 10000$ word co-occurrence matrix. After SVD, only 100 dimension is kept by keeping the top 100 singular values. Then, a low rank approximation matric is got. But only the truncated $W$ matrix is used as word embedding.<br>The word embedding is used to compute the similarity of words in the data set of this project as in the PMI method. After that, Analogy analysis is also performed in this method.  </p><h3 id="Result-and-Discussion"><a href="#Result-and-Discussion" class="headerlink" title="Result and Discussion"></a>Result and Discussion</h3><h4 id="Result-of-Weighted-Terms-Presentation-PPMI"><a href="#Result-of-Weighted-Terms-Presentation-PPMI" class="headerlink" title="Result of Weighted Terms Presentation: PPMI"></a>Result of Weighted Terms Presentation: PPMI</h4><p>After applying add two smooth to the co-occurrence matrix $M$, PPMI matrix is computed. Then this PMI matrix is used as the word embeddings in this data set.<br>For a given word, the corresponding vector can be retrieved from the PMI matrix by the index of this word. Then, we can compute the cosine similarity between this word and any other words. The increasing of the value of cosine similarity indicates the more relevance of two words.<br>For the similar word experiment, several word as inputs are given, and then the PPMI model is applied to search the 10 most similar words. The result of this experiment is shown as below. </p><table class="table table-bordered"><thead><tr><th style="text-align:center">imput</th><th style="text-align:center">output(top 10 most similar words in PPMI)</th></tr></thead><tbody><tr><td style="text-align:center">&#39;water&#39;</td><td style="text-align:center">&#39;water&#39;, &#39;liquid&#39;, &#39;oxygen&#39;, &#39;gas&#39;, &#39;fuel&#39;, &#39;fluid&#39;, &#39;soil&#39;, &#39;drainage&#39;, &#39;sand&#39;, &#39;waste&#39;</td></tr><tr><td style="text-align:center">&#39;boy&#39;</td><td style="text-align:center">&#39;boy&#39;, &#39;girl&#39;, &#39;kid&#39;, &#39;baby&#39;, &#39;man&#39;, &#39;crazy&#39;, &#39;woman&#39;, &#39;dog&#39;, &#39;cat&#39;, &#39;daddy&#39;</td></tr><tr><td style="text-align:center">&#39;apple&#39;</td><td style="text-align:center">&#39;apple&#39;, &#39;microsoft&#39;, &#39;ibm&#39;, &#39;linux&#39;, &#39;proprietary&#39;, &#39;os&#39;, &#39;compatible&#39;, &#39;processor&#39;, &#39;console&#39;, &#39;sony&#39;</td></tr><tr><td style="text-align:center">&#39;fruit&#39;</td><td style="text-align:center">&#39;fruit&#39;, &#39;fruits&#39;, &#39;flowers&#39;, &#39;vegetables&#39;, &#39;meat&#39;, &#39;corn&#39;, &#39;wheat&#39;, &#39;flower&#39;, &#39;seeds&#39;, &#39;milk&#39;</td></tr><tr><td style="text-align:center">&#39;cat&#39;</td><td style="text-align:center">&#39;cat&#39;, &#39;dog&#39;, &#39;rabbit&#39;, &#39;monkey&#39;, &#39;pig&#39;, &#39;rat&#39;, &#39;cow&#39;, &#39;duck&#39;, &#39;cats&#39;, &#39;frog&#39;</td></tr><tr><td style="text-align:center">&#39;beautiful&#39;</td><td style="text-align:center">&#39;beautiful&#39;, &#39;pretty&#39;, &#39;wonderful&#39;, &#39;attractive&#39;, &#39;lonely&#39;, &#39;nice&#39;, &#39;magnificent&#39;, &#39;sweet&#39;, &#39;funny&#39;, &#39;sad&#39;</td></tr><tr><td style="text-align:center">&#39;good&#39;</td><td style="text-align:center">&#39;good&#39;, &#39;bad&#39;, &#39;excellent&#39;, &#39;better&#39;, &#39;poor&#39;, &#39;nice&#39;, &#39;sure&#39;, &#39;wrong&#39;, &#39;my&#39;, &#39;wonderful&#39;</td></tr><tr><td style="text-align:center">&#39;many&#39;</td><td style="text-align:center">&#39;many&#39;, &#39;several&#39;, &#39;numerous&#39;, &#39;some&#39;, &#39;other&#39;, &#39;various&#39;, &#39;including&#39;, &#39;these&#39;, &#39;such&#39;, &#39;few&#39;</td></tr><tr><td style="text-align:center">&#39;blue&#39;</td><td style="text-align:center">&#39;blue&#39;, &#39;red&#39;, &#39;yellow&#39;, &#39;purple&#39;, &#39;black&#39;, &#39;pink&#39;, &#39;white&#39;, &#39;green&#39;, &#39;orange&#39;, &#39;colored&#39;</td></tr><tr><td style="text-align:center">&#39;play&#39;</td><td style="text-align:center">&#39;play&#39;, &#39;playing&#39;, &#39;plays&#39;, &#39;played&#39;, &#39;compete&#39;, &#39;perform&#39;, &#39;sing&#39;, &#39;participate&#39;, &#39;players&#39;, &#39;game&#39;</td></tr><tr><td style="text-align:center">&#39;try&#39;</td><td style="text-align:center">&#39;try&#39;, &#39;trying&#39;, &#39;tried&#39;, &#39;tries&#39;, &#39;attempting&#39;, &#39;attempt&#39;, &#39;able&#39;, &#39;attempts&#39;, &#39;help&#39;, &#39;attempted&#39;</td></tr><tr><td style="text-align:center">&#39;walk&#39;</td><td style="text-align:center">&#39;walk&#39;, &#39;walking&#39;, &#39;walks&#39;, &#39;walked&#39;, &#39;climb&#39;, &#39;swim&#39;, &#39;ride&#39;, &#39;throw&#39;, &#39;sit&#39;, &#39;wait&#39;</td></tr></tbody></table><p>From the result of similar words above, we can see that the PPMI do a good job in searching similar words for the given inputs. The similar words getting from PPMI are all relevant to the input word. There are some interesting output such as the output the ‘good’, the most similar word except itself is ‘bad’. Although We may think that these two words are opposite but not similar, for the given content these two words are used in most similar surrounding. That is to say they are syntactically similar. Thus, the PPMI will treat them as similar. The word embedding from PPMI matrix capture both the syntactic and semantic meaning of a word.<br>After the similarity experiment, analogy experiment was also perform by using the word embeding from PPMI. An analogy question— “man is to woman as king is to <em>__</em>”, where the goal of this analogy task is to fill in the blank space.<br>This question can be solved by finding a word whose word embedding is most closest to </p><p><script type="math/tex">w_{woman} - w_{man} + w_{king}</script>.<br>The result for this problem using cosine similarity and PPMI is as below.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&apos;man to woman&apos; is similar as &apos;king to prince&apos;</span><br><span class="line">&apos;boy to girl&apos; is similar as &apos;king to queen&apos;</span><br><span class="line">&apos;beijing to china&apos; is similar as &apos;paris to france&apos;</span><br><span class="line">&apos;king to queen&apos; is similar as &apos;ibm to microsoft&apos;</span><br><span class="line">&apos;korea to seoul&apos; is similar as &apos;thailand to bangkok&apos;</span><br></pre></td></tr></table></figure></p><p>It seems that, the PPMI does a good job in the problem except the first analogy. Then, an experiment through a test file which contains more than 5585 analogy samples to text the accurary of analogy is conducted. After a long time computation, the accuracy of PPMI in analogy is as below. It only get $41.6\%$ accuracy in this analogy test. It seems that the accuracy of word analogy by PPMI word embedding is quite low. Besides, the computation complexity of this 10000* 10000 matrix is very high, which makes the process slow dowm.  </p><h4 id="Result-of-Dense-vector-via-SVD"><a href="#Result-of-Dense-vector-via-SVD" class="headerlink" title="Result of Dense vector via SVD"></a>Result of Dense vector via SVD</h4><p>In this part, SVD is applied to decomposite the co-occurrence matrix in order to get a short and dense vector for word embedding. The plot of the singular value in this SVD decomposition is show as below.  </p><table class="table table-bordered"><thead><tr><th>imput</th><th>output(top 10 most similar words in PPMI using SVD)</th></tr></thead><tbody><tr><td>&#39;water&#39;</td><td>&#39;water&#39;, &#39;liquid&#39;, &#39;oxygen&#39;, &#39;gas&#39;, &#39;fuel&#39;, &#39;fluid&#39;, &#39;soil&#39;, &#39;drainage&#39;, &#39;sand&#39;, &#39;waste&#39;</td></tr><tr><td>&#39;boy&#39;</td><td>&#39;boy&#39;, &#39;girl&#39;, &#39;kid&#39;, &#39;baby&#39;, &#39;man&#39;, &#39;crazy&#39;, &#39;woman&#39;, &#39;dog&#39;, &#39;cat&#39;, &#39;daddy&#39;</td></tr><tr><td>&#39;apple&#39;</td><td>&#39;apple&#39;, &#39;microsoft&#39;, &#39;ibm&#39;, &#39;linux&#39;, &#39;proprietary&#39;, &#39;os&#39;, &#39;compatible&#39;, &#39;processor&#39;, &#39;console&#39;, &#39;sony&#39;</td></tr><tr><td>&#39;fruit&#39;</td><td>&#39;fruit&#39;, &#39;fruits&#39;, &#39;flowers&#39;, &#39;vegetables&#39;, &#39;meat&#39;, &#39;corn&#39;, &#39;wheat&#39;, &#39;flower&#39;, &#39;seeds&#39;, &#39;milk&#39;</td></tr><tr><td>&#39;cat&#39;</td><td>&#39;cat&#39;, &#39;dog&#39;, &#39;rabbit&#39;, &#39;monkey&#39;, &#39;pig&#39;, &#39;rat&#39;, &#39;cow&#39;, &#39;duck&#39;, &#39;cats&#39;, &#39;frog&#39;</td></tr><tr><td>&#39;beautiful&#39;</td><td>&#39;beautiful&#39;, &#39;pretty&#39;, &#39;wonderful&#39;, &#39;attractive&#39;, &#39;lonely&#39;, &#39;nice&#39;, &#39;magnificent&#39;, &#39;sweet&#39;, &#39;funny&#39;, &#39;sad&#39;</td></tr><tr><td>&#39;good&#39;</td><td>&#39;good&#39;, &#39;bad&#39;, &#39;excellent&#39;, &#39;better&#39;, &#39;poor&#39;, &#39;nice&#39;, &#39;sure&#39;, &#39;wrong&#39;, &#39;my&#39;, &#39;wonderful&#39;</td></tr><tr><td>&#39;many&#39;</td><td>&#39;many&#39;, &#39;several&#39;, &#39;numerous&#39;, &#39;some&#39;, &#39;other&#39;, &#39;various&#39;, &#39;including&#39;, &#39;these&#39;, &#39;such&#39;, &#39;few&#39;</td></tr><tr><td>&#39;blue&#39;</td><td>&#39;blue&#39;, &#39;red&#39;, &#39;yellow&#39;, &#39;purple&#39;, &#39;black&#39;, &#39;pink&#39;, &#39;white&#39;, &#39;green&#39;, &#39;orange&#39;, &#39;colored&#39;</td></tr><tr><td>&#39;play&#39;</td><td>&#39;play&#39;, &#39;playing&#39;, &#39;plays&#39;, &#39;played&#39;, &#39;compete&#39;, &#39;perform&#39;, &#39;sing&#39;, &#39;participate&#39;, &#39;players&#39;, &#39;game&#39;</td></tr><tr><td>&#39;try&#39;</td><td>&#39;try&#39;, &#39;trying&#39;, &#39;tried&#39;, &#39;tries&#39;, &#39;attempting&#39;, &#39;attempt&#39;, &#39;able&#39;, &#39;attempts&#39;, &#39;help&#39;, &#39;attempted&#39;</td></tr><tr><td>&#39;walk&#39;</td><td>&#39;walk&#39;, &#39;walking&#39;, &#39;walks&#39;, &#39;walked&#39;, &#39;climb&#39;, &#39;swim&#39;, &#39;ride&#39;, &#39;throw&#39;, &#39;sit&#39;, &#39;wait&#39;</td></tr></tbody></table><p>From the result, we can see that co-occurrence matrix with SVD also get a well job in searching the similar words.<br>After the similarity experiment, analogy experiment was also perform by using the word embedding from truncated co-occurrence matrix.<br>The result for this problem using cosine similarity and Dense vector is as below.<br>From the result, we can see that the dense vector perform well in this analogy problem. It gets the right result for the analogy problem given.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&apos;man to woman&apos; is similar as &apos;king to queen&apos;</span><br><span class="line">&apos;boy to girl&apos; is similar as &apos;king to queen&apos;</span><br><span class="line">&apos;beijing to china&apos; is similar as &apos;paris to france&apos;</span><br><span class="line">&apos;king to queen&apos; is similar as &apos;ibm to microsoft&apos;</span><br><span class="line">&apos;korea to seoul&apos; is similar as &apos;thailand to bangkok&apos;</span><br></pre></td></tr></table></figure></p><p>As the PPMI model, an experiment through a test file which contains more than 5585 analogy samples to text the accurary of analogy is conducted. The accuracy using dense vector is $54\%$, which is much better than the sparse vector PPMI method.</p><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>From all the result of the two method, we know that the dense vector method get a better result than the sparse PPMI method in analogy analysis and similar word search. In addition, the computational efficiency of the dense vector is also better than the PPMI. Short vectors may be easier to use as features in machine learning. Dense vectors    may    generalize better than storing explicit counts. In addition, dense vectors may perform better in capturing synonymy than sparse vectors.  </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h3&gt;&lt;p&gt;Word embedding is a mapping fr
      
    
    </summary>
    
    
      <category term="machine learning" scheme="https://wushbin.github.io/tags/machine-learning/"/>
    
      <category term="natural language processing" scheme="https://wushbin.github.io/tags/natural-language-processing/"/>
    
  </entry>
  
  <entry>
    <title>cat-and-dog</title>
    <link href="https://wushbin.github.io/2017/05/02/cat-and-dog/"/>
    <id>https://wushbin.github.io/2017/05/02/cat-and-dog/</id>
    <published>2017-05-03T00:42:35.000Z</published>
    <updated>2017-12-23T13:49:09.193Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>In the note, a classifier of SVM(Support Vector Machine) and a classifier of CNN(Convolutional Neural Network) using transfer learning will be explored to experiment the accuracy of automatically classifying the image set of cats and dogs.  </p><h4 id="Method-One"><a href="#Method-One" class="headerlink" title="Method One"></a>Method One</h4><p>For the first method, an SVM classifier was trained to classify the images by families(family dog or family cat). SIFT will be applied to extracted the features from each image in the training set. Then the bag of word model will be applied to create a dictionary for these extracted features. This dictionary is formed by a clustering algorithm K-means. One cluster of features is viewed as a visual word in this dictionary. After the dictionary is created, images are represented by frequency vectors which represent the proportion of features belong to a visual word. Then, a SVM classifier is trained based on these frequency features.  </p><h4 id="Method-Two"><a href="#Method-Two" class="headerlink" title="Method Two"></a>Method Two</h4><p>For the second method, we used CNN model to extracted features from the images. For this project, keras application are used. These applications are deep learning models with pre-trained weights. They can be used for prediction, feature extraction and fine tuning. The ResNet50, InceptionV3 and Xception model are used for extracting features from the images. Then these features will be used to trained a CNN model.  </p><h4 id="Data-Set"><a href="#Data-Set" class="headerlink" title="Data Set"></a>Data Set</h4><p>The data set for this project is the provided by Microsoft Research and Kaggle. The training set consists of 25,000 images with half cat images and half dog images. The training set contains 12,500 images without labels. The size of these images are about $350\times 350$.<br>After downloading the image set from Kaggle, the images are separated into two folder, one for cat images, one for dog images. For dogs, the corresponding label is 1, for cats, the corresponding label is 0. The sample images in the data set are shown in the <strong><em>Figure 1</em></strong>.<br><img src="/images/catAndDog.png" alt="data set images">“Figure 1: images in data set”</p><h3 id="Process"><a href="#Process" class="headerlink" title="Process"></a>Process</h3><h4 id="Method-One-Traditional-Machine-Learning-Approach"><a href="#Method-One-Traditional-Machine-Learning-Approach" class="headerlink" title="Method One: Traditional Machine Learning Approach"></a>Method One: Traditional Machine Learning Approach</h4><p>For traditional image classification problems, features extracted by human are chosen to train classifiers. Then feature descriptors are use to represent the images. SIFT, HoG, RGB and HSV are the common features that are used to represent images.<br>For this project, SIFT are used to extracted the features and compute the feature descriptors. Because we know that the shapes of images of cat and dog are different with each other. Features extracted by SIFT will play an important role in the classification of the image set.<br>After implementing the SIFT algorithms to extract features, we get the features descriptors of all the images in the training set. K-means clustering algorithms is applied to generate a dictionary of visual words for the features descriptors in the training set. All the images are then represented by by frequency vectors which represent the proportion of features belong to a visual word.<br>Based on the frequency vectors generated by the BOW method, a SVM classifier was trained to make classification. The visualize process of this method is shown in the <strong><em>Figure 2</em></strong>.<br><img src="/images/SIFT_proc.jpeg" alt="method one">“Figure 2: Process of Method One”</p><p><a href="https://github.com/wushbin/DogAndCat/blob/master/catVsDog.ipynb" target="_blank" rel="noopener">Source code for this method</a><br>The accuracy of this method is about $62\%$ when the images were compressed to $128 \times 128$, which is quite dissatisfactory for a binary classification problem. While when the images were compressed to $256 \times 256$, the accuracy only increase to $65.46\%$.<br><img src="/images/sift_result.png" alt="method one result">“Result using Method One”</p><h4 id="Method-Two-Deep-Neural-Netword-CNN-with-transfer-learning"><a href="#Method-Two-Deep-Neural-Netword-CNN-with-transfer-learning" class="headerlink" title="Method Two: Deep Neural Netword(CNN with transfer learning)"></a>Method Two: Deep Neural Netword(CNN with transfer learning)</h4><p><em>Method two is refer to Peiwen Yang’s Post in Zhihu</em>.  </p><p>In Method Two, image features will be extracted by a Convolutional Neural Network model. After that, we can simply use dropout to classify the validation set and test set. Compared with the feature extraction by SIFT, convolutional neural network learn features from the images.<br>Several pre-trained model in keras application are used in this method to learn the features from the cat and dog image set. The ResNet50, InceptionV3 and Xception models are chosen to learn the features,which are object detection model in image recognition provided by keras. The weights of these models are pre-trained on ImageNet.<br>In order to improve the performance of the classification model, these three models are used together to learn the features from the images. These three models build up a huge network. If a fully connected layer is added directly after this huge network to train the classification model, the computation cost will be extremely large. Thus, the features extraction and classifier training are conducted separately. The pre-trained models are used to extract features. And then, these features are used to train the classifier.<br>For the trainning process, a simple neural network was used as the classification model, this model includes an input layer, a hidden layer with dropout rate $0.5$, and an output layer with sigmoid as activation function. The features number learned by each model is 2048, and then 6144 features was learned from the feature extraction process. The number of nodes in the input layer is 6144. For the hidden layer, there is also 6144 nodes, but they are not fully connected because dropout was applied in this layer. For the output layer, there is only 1 node because it is a binary classification problem.\newline<br>With fine features learning by pre-trained models, a simple model can make a good classification. The visualized process of this model is shown is <strong><em>Figure 3</em></strong>.<br><img src="/images/cnnModel.jpeg" alt="method two">“Figure 3: Model of Method Two”</p><p><a href="https://github.com/wushbin/DogAndCat/blob/master/cnnCatVsDog.ipynb" target="_blank" rel="noopener">Source code for this method</a> </p><p><img src="/images/cnn_result.png" alt="method two result">“Result using Method two”<br>In addition, the cost time of feature extraction for each model(ResNet50, InceptionV3 and Xception) is less than 20 minutes, which is also more efficient than the method one. Moreover, it only take less than 10 minutes to train the classifier.   </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h3&gt;&lt;p&gt;In the note, a classifier of S
      
    
    </summary>
    
    
      <category term="machine learning" scheme="https://wushbin.github.io/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>Duke Gather: Android Application</title>
    <link href="https://wushbin.github.io/2017/05/01/Duke-Gather-Android-Application/"/>
    <id>https://wushbin.github.io/2017/05/01/Duke-Gather-Android-Application/</id>
    <published>2017-05-02T01:40:30.000Z</published>
    <updated>2018-01-13T21:56:48.781Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://github.com/wushbin/GatherApp" target="_blank" rel="noopener">link to this project in github</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://github.com/wushbin/GatherApp&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;link to this project in github&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Java" scheme="https://wushbin.github.io/tags/Java/"/>
    
      <category term="Mobile Application" scheme="https://wushbin.github.io/tags/Mobile-Application/"/>
    
  </entry>
  
  <entry>
    <title>High Speed Test Rig Development</title>
    <link href="https://wushbin.github.io/2016/06/01/High-Speed-Test-Rig-Development/"/>
    <id>https://wushbin.github.io/2016/06/01/High-Speed-Test-Rig-Development/</id>
    <published>2016-06-02T01:53:03.000Z</published>
    <updated>2018-01-13T21:22:44.023Z</updated>
    
    <content type="html"><![CDATA[<h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>这是一个失败的试验台设计，试验过程及数据详见文末论文链接。<br>本项目主要是搭建一个综合实验台,该实验台用以研究一款气体动压轴承和一款气体静压轴承的在高速旋转状态下的动态特性。  </p><ol><li>设计开发一个用以研究气体箔片止推轴承和多孔质静压气体轴承的实验台,实验台主要包括高速旋转部分,加载部分、供气系统和数据采集系统。其中高速旋转部分采用多孔质静压气体轴承作为转子径向支撑,实验台的轴向限位采用气体箔片止推轴承。</li><li>尝试并开发气体箔片止推轴承的制作工艺。</li><li>对实验台高速旋转部分的轴承-转子系统进行了动平衡实验和转子动力学实验。动平衡实验包括单面动平衡和双面动平衡实验。转子动力学实验研究多孔质静压气体轴承对高速转子性能的影响。</li></ol><h3 id="试验台设计"><a href="#试验台设计" class="headerlink" title="试验台设计"></a>试验台设计</h3><p>试验台主要由加载部分和高速旋转部分两部分组成<br><img src="/images/High-Speed-Test-Rig-Development/Picture1.png" style="width: 800px;"></p><h4 id="试验台加载部分"><a href="#试验台加载部分" class="headerlink" title="试验台加载部分"></a>试验台加载部分</h4><p><img src="/images/High-Speed-Test-Rig-Development/Picture2.jpg" style="width: 800px;"></p><h4 id="试验台高速旋转部分"><a href="#试验台高速旋转部分" class="headerlink" title="试验台高速旋转部分"></a>试验台高速旋转部分</h4><p><img src="/images/High-Speed-Test-Rig-Development/Picture3.jpg" style="width: 800px;"></p><h4 id="试验台供气系统"><a href="#试验台供气系统" class="headerlink" title="试验台供气系统"></a>试验台供气系统</h4><p><img src="/images/High-Speed-Test-Rig-Development/Picture4.png" style="width: 500px;"></p><h4 id="试验台整体"><a href="#试验台整体" class="headerlink" title="试验台整体"></a>试验台整体</h4><p><img src="/images/High-Speed-Test-Rig-Development/Picture5.png" style="width: 800px;"></p><h4 id="转子动平衡试验台"><a href="#转子动平衡试验台" class="headerlink" title="转子动平衡试验台"></a>转子动平衡试验台</h4><p><img src="/images/High-Speed-Test-Rig-Development/Picture6.png" style="width: 500px;"><br><img src="/images/High-Speed-Test-Rig-Development/Picture7.png" style="width: 500px;"></p><h3 id="试验台加工制造以及试验部分（见论文）"><a href="#试验台加工制造以及试验部分（见论文）" class="headerlink" title="试验台加工制造以及试验部分（见论文）"></a>试验台加工制造以及试验部分（见论文）</h3><p><a href="http://kns.cnki.net/KCMS/detail/detail.aspx?dbcode=CMFD&amp;dbname=CMFD201701&amp;filename=1016251956.nh&amp;v=MDAwMzZyV00xRnJDVVJMS2ZadWRyRmkzaFdyM0xWRjI2R0xHOUg5akpxWkViUElSOGVYMUx1eFlTN0RoMVQzcVQ=" target="_blank" rel="noopener">论文链接</a> </p><h4 id="气体箔片止推轴承制造工艺（见论文）"><a href="#气体箔片止推轴承制造工艺（见论文）" class="headerlink" title="气体箔片止推轴承制造工艺（见论文）"></a>气体箔片止推轴承制造工艺（见论文）</h4><h4 id="箔片止推轴承性能测试（见论文）"><a href="#箔片止推轴承性能测试（见论文）" class="headerlink" title="箔片止推轴承性能测试（见论文）"></a>箔片止推轴承性能测试（见论文）</h4><h4 id="高速转自动平衡试验（见论文）"><a href="#高速转自动平衡试验（见论文）" class="headerlink" title="高速转自动平衡试验（见论文）"></a>高速转自动平衡试验（见论文）</h4><h4 id="多孔质轴承-转子系统试验（见论文）"><a href="#多孔质轴承-转子系统试验（见论文）" class="headerlink" title="多孔质轴承-转子系统试验（见论文）"></a>多孔质轴承-转子系统试验（见论文）</h4><hr><p>于2016年夏天<br>导师：没有导师<br><a href="">试验台UG三维模型及详细工程图</a><br><a href="http://kns.cnki.net/KCMS/detail/detail.aspx?dbcode=CMFD&amp;dbname=CMFD201701&amp;filename=1016251956.nh&amp;v=MDAwMzZyV00xRnJDVVJMS2ZadWRyRmkzaFdyM0xWRjI2R0xHOUg5akpxWkViUElSOGVYMUx1eFlTN0RoMVQzcVQ=" target="_blank" rel="noopener">论文链接</a> </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h3&gt;&lt;p&gt;这是一个失败的试验台设计，试验过程及数据详见文末论文链接。&lt;br&gt;本项目主要是搭建一个综合实验台,该实验台用以研究一款气体动压轴承和一款气体
      
    
    </summary>
    
    
      <category term="mechanical design" scheme="https://wushbin.github.io/tags/mechanical-design/"/>
    
      <category term="experiment" scheme="https://wushbin.github.io/tags/experiment/"/>
    
  </entry>
  
  <entry>
    <title>SCARA Robot</title>
    <link href="https://wushbin.github.io/2013/06/01/SCARA-Robot/"/>
    <id>https://wushbin.github.io/2013/06/01/SCARA-Robot/</id>
    <published>2013-06-02T01:52:32.000Z</published>
    <updated>2018-01-13T21:47:44.585Z</updated>
    
    <content type="html"><![CDATA[<h3 id="SCARA机器人"><a href="#SCARA机器人" class="headerlink" title="SCARA机器人"></a>SCARA机器人</h3><p>SCARA平面关节式机器人是目前使用较为广泛的通用型机器人。在动作相对简单，而又需要有高产量的环境中，SCARA机器人相比六轴机器人而言很有优势的。SCARA机器人在点对点的运动中是最好的机器人，常用于分配、搬运、装载、包装、安放以及装配等作业之中。<br><img src="/images/SCARA-Robot/Picture0.jpg" style="width: 600px;"></p><h3 id="机器人本体机械设计"><a href="#机器人本体机械设计" class="headerlink" title="机器人本体机械设计"></a>机器人本体机械设计</h3><h4 id="机器人构型"><a href="#机器人构型" class="headerlink" title="机器人构型"></a>机器人构型</h4><p>两种比较具有代表性的SCARA机器人的构型，初步选择了两种方案，如下图所示：</p><ol><li>RRRT型SCARA机器人</li><li>TRRR型SCARA机器人<br><img src="/images/SCARA-Robot/Picture1.jpg" style="width: 400px;"><h4 id="机器人技术参数"><a href="#机器人技术参数" class="headerlink" title="机器人技术参数"></a>机器人技术参数</h4>该款SCARA机器人的关键设计参数如下图所示。<br><img src="/images/SCARA-Robot/Picture2.jpg" style="width: 600px;"></li></ol><h4 id="机器人传动方式"><a href="#机器人传动方式" class="headerlink" title="机器人传动方式"></a>机器人传动方式</h4><p>该四自由度关节型工业机器人各个轴的传动方案确定如下：</p><ul><li>Ｘ轴回转：底座→伺服电机→谐波减速器→大臂回转</li><li>Ｙ轴回转：大臂→伺服电机→谐波减速器→小臂回转 </li><li>Ｚ轴移动：小臂→伺服电机→同步带→丝杆螺母副→滚珠花键副上下平动</li><li>Ｒ轴回转：小臂→伺服电机→同步带→谐波减速器→滚珠花键轴套→滚珠花键副<br><img src="/images/SCARA-Robot/Picture3.jpg" style="width: 600px;"></li></ul><h4 id="机器人尺寸及工作空间"><a href="#机器人尺寸及工作空间" class="headerlink" title="机器人尺寸及工作空间"></a>机器人尺寸及工作空间</h4><p>机器人的整体设计结构如下图所示<br><img src="/images/SCARA-Robot/Picture4.jpg" style="width: 800px;"></p><p>机器人的整体尺寸以及工作空间如下图所示</p><p><img src="/images/SCARA-Robot/Picture5.jpg" style="width: 800px;"></p><p>机器人整体渲染效果图</p><div class="video-container"><iframe src="//www.youtube.com/embed/AxYgaXBQnxo" frameborder="0" allowfullscreen></iframe></div><p><img src="/images/SCARA-Robot/Picture6.jpg" style="width: 800px;"></p><h3 id="机器人关键结构设计"><a href="#机器人关键结构设计" class="headerlink" title="机器人关键结构设计"></a>机器人关键结构设计</h3><h4 id="机械臂一结构设计"><a href="#机械臂一结构设计" class="headerlink" title="机械臂一结构设计"></a>机械臂一结构设计</h4><p>电机固定在底座上，有利于减少机械臂的惯量。所选的谐波减速器为日本哈默纳科简易型谐波减速器SHG/SHF类型，该类型谐波属于简易型谐波减速器，内部置有用于支撑外部负载的精密、具有高刚性的交叉滚子轴承，不用再在外部安装用于支承负载的轴承，所以只需将刚轮、柔轮分别与底座和机械臂一固定，就能实现X轴的转动。<br><img src="/images/SCARA-Robot/Picture7.jpg" style="width: 400px;"></p><h4 id="机械臂手腕结构设计"><a href="#机械臂手腕结构设计" class="headerlink" title="机械臂手腕结构设计"></a>机械臂手腕结构设计</h4><p>由于主轴位于机器人小臂的末端，对重量和惯量比较敏感，所以要求整个结构紧凑、重量轻，同时考虑到控制系统设计的相对简单和成本的相对低廉，采用滚珠花键和滚珠螺杆组合的方式。目前SCARA机器人最新的结构是采用滚珠花键-丝杆一体的结构，但这样的结构需要两个电机耦合控制来实现末端的旋转和上下移动，使控制系统较为复杂。</p><p><img src="/images/SCARA-Robot/Picture8.jpg" style="width: 600px;"></p><h3 id="机器人运动学分析"><a href="#机器人运动学分析" class="headerlink" title="机器人运动学分析"></a>机器人运动学分析</h3><h4 id="工业机器人运动学方程"><a href="#工业机器人运动学方程" class="headerlink" title="工业机器人运动学方程"></a>工业机器人运动学方程</h4><p><img src="/images/SCARA-Robot/Picture9.jpg" style="width: 400px;"><br><img src="/images/SCARA-Robot/Picture10.jpg" style="width: 400px;"></p><h4 id="该SCARAb机器人D-H模型"><a href="#该SCARAb机器人D-H模型" class="headerlink" title="该SCARAb机器人D-H模型"></a>该SCARAb机器人D-H模型</h4><p>根据所设计的SCARA机器人结构，机器人D-H坐标系如下图所示<br><img src="/images/SCARA-Robot/Picture11.jpg" style="width: 400px;"><br>其连杆参数表如下图所示<br><img src="/images/SCARA-Robot/Picture12.jpg" style="width: 600px;"></p><h4 id="正运动学分析"><a href="#正运动学分析" class="headerlink" title="正运动学分析"></a>正运动学分析</h4><p>各连杆变换矩阵连乘，便能得到机器人末端连杆的位姿方程，也就是正运动学方程。<br><img src="/images/SCARA-Robot/Picture13.jpg" style="width: 600px;"></p><h4 id="逆运动学分析"><a href="#逆运动学分析" class="headerlink" title="逆运动学分析"></a>逆运动学分析</h4><p>在四自由度关节型机器人基坐标系中，机械手末端执行器的位姿矢量设为已知<br><img src="/images/SCARA-Robot/Picture15.jpg" style="width: 200px;"><br>也就是<br><img src="/images/SCARA-Robot/Picture16.jpg" style="width: 800px;"><br>最终可求得<br><img src="/images/SCARA-Robot/Picture17.jpg" style="width: 800px;"></p><hr><p>于2013年<br>导师：王念峰<br>华南理工大大学<br>源文件下载链接<br><a href="">solidworks模型及工程图</a>  </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;SCARA机器人&quot;&gt;&lt;a href=&quot;#SCARA机器人&quot; class=&quot;headerlink&quot; title=&quot;SCARA机器人&quot;&gt;&lt;/a&gt;SCARA机器人&lt;/h3&gt;&lt;p&gt;SCARA平面关节式机器人是目前使用较为广泛的通用型机器人。在动作相对简单，而又需要有高产量
      
    
    </summary>
    
    
      <category term="mechanical design" scheme="https://wushbin.github.io/tags/mechanical-design/"/>
    
      <category term="experiment" scheme="https://wushbin.github.io/tags/experiment/"/>
    
  </entry>
  
</feed>
