<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>wushbin&#39;s studio</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://wushbin.github.io/"/>
  <updated>2018-03-01T21:15:30.988Z</updated>
  <id>https://wushbin.github.io/</id>
  
  <author>
    <name>shengbin wu</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>My Understanding of Kernel in SVM</title>
    <link href="https://wushbin.github.io/2018/03/01/kernel/"/>
    <id>https://wushbin.github.io/2018/03/01/kernel/</id>
    <published>2018-03-01T21:15:04.000Z</published>
    <updated>2018-03-01T21:15:30.988Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Support Vector Machine Implementation With Python</title>
    <link href="https://wushbin.github.io/2018/03/01/Support-Vector-Machine-Implementation/"/>
    <id>https://wushbin.github.io/2018/03/01/Support-Vector-Machine-Implementation/</id>
    <published>2018-03-01T16:16:03.000Z</published>
    <updated>2018-03-01T23:40:51.514Z</updated>
    
    <content type="html"><![CDATA[<p>In this post, a SVM classifier is implemented. The SVM is implemented with “Hard Margin” and “Soft Margin”.  The “Hard Margin” is used to classify separable data, while the soft margin is used to classifier inseparable data. In addition, kernel can be used in this SVM classifier. In this post, the SVM is implemented with linear kernel and Gaussian Kernel(RBF kernel).  </p><h3 id="SVM-Classifier"><a href="#SVM-Classifier" class="headerlink" title="SVM Classifier"></a>SVM Classifier</h3><p>We will use a Quadratic Program Solver CVXOPT to solve the Lagrangian of SVM. A tutorial of CVXOPT can be found here (<a href="https://courses.csail.mit.edu/6.867/wiki/images/a/a7/Qp-cvxopt.pdf" target="_blank" rel="noopener">cvsopt</a>).<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> cvxopt</span><br><span class="line"><span class="keyword">from</span> cvxopt <span class="keyword">import</span> matrix</span><br><span class="line"><span class="keyword">from</span> cvxopt <span class="keyword">import</span> solvers</span><br></pre></td></tr></table></figure></p><h4 id="Hard-Margin-SVM"><a href="#Hard-Margin-SVM" class="headerlink" title="Hard Margin SVM"></a>Hard Margin SVM</h4><p>The Primal problem of SVM is </p><script type="math/tex; mode=display">min_{\lambda, \lambda_0} \frac{1}{2}||\lambda||^2_2  \text{ }\text{ }\text{  s.t.  } y_i(\lambda^Tx_i + \lambda_0) - 1 \geq 0</script><p>The Lagrangian of SVM is </p><script type="math/tex; mode=display">\mathcal{L}([\lambda, \lambda_0], \alpha) = \frac{1}{2} \sum_{j=1}^n \lambda_j^2 + \sum_{i=1}^n \alpha_i[-y_i(\lambda^Tx_i + \lambda_0) + 1]</script><p>The $KKT$ condition is </p><script type="math/tex; mode=display">\begin{cases}\nabla_{\lambda}\mathcal{L} = \lambda - \sum_{i=1}^n\alpha_iy_ix_i = 0 \implies \lambda = \sum_{i=1}^n \alpha_iy_ix_i\\\frac{\partial}{\partial \lambda_0}\mathcal{L} = -\sum_{i=1}^n\alpha_iy_i = 0\\\alpha_i \geq 0\\\alpha_i[-y_i(\lambda^Tx_i + \lambda_0) + 1] = 0\\-y_i(\lambda^Tx_i + \lambda_0) + 1 \leq 0\\\end{cases}</script><!-- $$\nabla_{\lambda}\mathcal{L} = \lambda - \sum_{i=1}^n\alpha_iy_ix_i = 0 \implies \lambda = \sum_{i=1}^n \alpha_iy_ix_i$$$$\frac{\partial}{\partial \lambda_0}\mathcal{L} = -\sum_{i=1}^n\alpha_iy_i = 0$$$$\alpha_i \geq 0$$$$\alpha_i[-y_i(\lambda^Tx_i + \lambda_0) + 1] = 0$$$$-y_i(\lambda^Tx_i + \lambda_0) + 1 \leq 0$$ --><p>The Dual Problem is </p><script type="math/tex; mode=display">max_{\alpha}\mathcal{L}(\alpha) = \sum_{i=1}^n\alpha_i - \frac{1}{2}\sum_{i,j}\alpha_i\alpha_ky_iy_kx_i^Tx_k</script><script type="math/tex; mode=display">s.t. \text{     }\alpha_i \geq 0 \text{  }\forall i  \text{   and   } \sum_{i=1}^n \alpha_iy_i = 0</script><p>This dual problem can be solved by a quadratic program solver.<br>That is </p><script type="math/tex; mode=display">min_{\alpha}: \frac{1}{2}\sum_{i,j}\alpha_i\alpha_ky_iy_kx_i^Tx_k - \sum_{i=1}^n\alpha_i</script><script type="math/tex; mode=display">min_{\mathbf{\alpha}}: \frac{1}{2} \mathbf{\alpha}^T \mathbf{P} \mathbf{\alpha} + \mathbf{q}^T\mathbf{\alpha}</script><script type="math/tex; mode=display">\mathbf{P} = \begin{bmatrix}y_1y_1\mathbf{x_1}^T\mathbf{x_1} & \dots & y_1y_n\mathbf{x_1}^T\mathbf{x_n} \\\dots & \dots & \dots \\y_ny_1\mathbf{x_n}^T\mathbf{x_1} & \dots & y_ny_n\mathbf{x_n}^T\mathbf{x_n} \\\end{bmatrix}</script><script type="math/tex; mode=display">\mathbf{q} = [-1,-1,-1,\dots, -1]^T</script><script type="math/tex; mode=display">s.t. \begin{cases}\mathbf{G}\mathbf{\alpha} \leq \mathbf{h} \\\mathbf{A} \mathbf{\alpha} = \mathbf{b}\\ \end{cases}</script><script type="math/tex; mode=display">\mathbf{G} = -1 \times \mathbf{I_{n\times n}}</script><script type="math/tex; mode=display">\mathbf{h} = [0,0,0,\dots,0]^T</script><script type="math/tex; mode=display">A = [y_1, y_2, y_3, \dots, y_n]^T</script><script type="math/tex; mode=display">b = 0</script><p>The optimal value of $\alpha_i$ can be obtained by solving the above quadratic problem using CVXOPT.</p><h5 id="Making-prediction-using-Linear-Kernel"><a href="#Making-prediction-using-Linear-Kernel" class="headerlink" title="Making prediction using Linear Kernel"></a>Making prediction using Linear Kernel</h5><p>For linear kernel, we make prediction by </p><script type="math/tex; mode=display">f(x_{new}) = \mathbf{\lambda}^{\star T}\mathbf{x_{new} + \lambda_0^{\star}}</script><p>we can compute the $\lambda^{\star}$ by </p><script type="math/tex; mode=display">\lambda^{\star} = \sum_{i=1}^n \alpha_i^{\star} y_i \mathbf{x_i}</script><p>Then we can use the $\lambda^{\star}$ and support vectors to compute $\lambda_0^{\star}$ by</p><script type="math/tex; mode=display">1 = y_i(\lambda^{\star T}\mathbf{x_i} + \lambda_0^{\star})</script><p>For this question, all the support vectors are used to compute the $\lambda_0^{\star}$ and then take the average as the final $\lambda_0^{\star}$</p><h5 id="Gaussian-Kernel-RBF"><a href="#Gaussian-Kernel-RBF" class="headerlink" title="Gaussian Kernel(RBF)"></a>Gaussian Kernel(RBF)</h5><p>For a non-linear kernel, if we do not know how the kernel map features to a new feature space, the $\lambda^{\star}$ can not be computed by $\lambda^{\star} = \sum_{i=1}^n \alpha_i^{\star} y_i \mathbf{x_i^{\mathcal{H_k}}}$ if we do not know the $\mathbf{x_i^{\mathcal{H_k}}}$<br>However, all we need to know is the value of inner product a $\mathbf{x_i^{\mathcal{H_k}}}$ in the new feature space.<br>For solving the Dual problem, </p><script type="math/tex; mode=display">max_{\alpha}\mathcal{L}(\alpha) = \sum_{i=1}^n\alpha_i - \frac{1}{2}\sum_{i,j}\alpha_i\alpha_ky_iy_k\langle x_i, x_k \rangle_{\mathcal{H_k}}</script><p>we just need to know the value of $\langle x_i, x_k \rangle$<br>For making prediction,</p><script type="math/tex; mode=display">\begin{align}f(x_{new}) & = \sum_j^p \lambda_j^{\star}x_{new(j)} + \lambda_0\\& = \sum_{j}^p \sum_{i}^n \alpha_i^{\star}y_ix_{i,j} x_{new(j)} + \lambda_0^{\star}\\& = \sum_i^n \alpha_i^{\star}y_i \sum_j^p x_{i,j} x_{new(j)} + \lambda_0^{\star}\\& = \sum_i^n \alpha_i^{\star}y_i \langle \mathbf{x_i}, \mathbf{x_{new}}\rangle_{\mathcal{H_k}} + \lambda_0^{\star}\\& = \sum_i^n \alpha_i^{\star}y_i k(\mathbf{x_i}, \mathbf{x_{new}}) + \lambda_0^{\star}\\\end{align}</script><p>For this equation, we know that we just need to know the inner product value of $x$ in the new feature space to make a prediction.<br>Thus, for the SVM using Gaussian kernel, all we need is the value of two vector in the Gaussian kernel. </p><h4 id="Soft-Margin"><a href="#Soft-Margin" class="headerlink" title="Soft Margin"></a>Soft Margin</h4><p>The Primal problem of SVM is </p><script type="math/tex; mode=display">min_{\lambda, \lambda_0} \frac{1}{2}||\lambda||^2_2 + C\sum_{i=1}^n \xi_i</script><p>s.t.$\begin{cases}<br>y_i(\lambda^Tx_i + \lambda_0) \geq  1 - \xi_i\<br>\xi_i \geq 0\end{cases}$<br>The Lagrangian form of this prime is</p><script type="math/tex; mode=display">\mathcal{L}([\lambda, \lambda_0], \xi, \alpha, \gamma) = \frac{1}{2} \sum_{j=1}^p \lambda_j^2 + C\sum_{i=1}^n \xi_i -\sum_{i=1}^n \alpha_i[y_i(\lambda^Tx_i + \lambda_0) - 1 + \xi_i] - \sum_{i=1}^n \gamma_i\xi_i</script><p>The Dual problem is </p><script type="math/tex; mode=display">max_{\alpha}\mathcal{L}(\alpha) = \sum_{i=1}^n\alpha_i - \frac{1}{2}\sum_{i,j}\alpha_i\alpha_ky_iy_kx_i^Tx_k</script><script type="math/tex; mode=display">s.t. \begin{cases}0 \leq \alpha_i \geq C \text{  }\forall i \\\sum_{i=1}^n \alpha_iy_i = 0\\\end{cases}</script><p>The only difference is $\leq \alpha_i \geq C \text{  }\forall i$. To solve the quadratic problem, all we need to change is the matrix $\mathbf{P}$, $\mathbf{G}$ and $\mathbf{h}$. </p><script type="math/tex; mode=display">\mathbf{P} = \begin{bmatrix}y_1y_1k(\mathbf{x_1}, \mathbf{x_1}) & \dots & y_1y_nk(\mathbf{x_1}, \mathbf{x_n}) \\\dots & \dots & \dots \\y_ny_1k(\mathbf{x_n}, \mathbf{x_1}) & \dots & y_ny_nk(\mathbf{x_n}, \mathbf{x_n}) \\\end{bmatrix}</script><script type="math/tex; mode=display">\mathbf{G} = \begin{bmatrix}-\mathbf{I} \\\mathbf{I} \\\end{bmatrix}</script><script type="math/tex; mode=display">\mathbf{h} = \begin{bmatrix}\mathbf{0_{n\times 1}}\\\mathbf{C_{n\times 1}}\\\end{bmatrix}</script><p>The code of SVM implemented in Python is shown as below.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SVM</span>:</span></span><br><span class="line">    <span class="comment">## default kernel is linear</span></span><br><span class="line">    <span class="comment">## default sigma for gaussian kernel is 5.0</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, kernel=<span class="string">'linear'</span>, margin=<span class="string">'hard'</span>, C=<span class="number">1</span>, sigma=<span class="number">5</span>)</span>:</span></span><br><span class="line">        self.k = kernel</span><br><span class="line">        self.a = <span class="keyword">None</span></span><br><span class="line">        self.sv_x = <span class="keyword">None</span></span><br><span class="line">        self.sv_y = <span class="keyword">None</span></span><br><span class="line">        self.w = <span class="keyword">None</span></span><br><span class="line">        self.w_0 = <span class="keyword">None</span></span><br><span class="line">        self.sigma = sigma</span><br><span class="line">        self.margin = margin</span><br><span class="line">        self.C = C</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">kernel</span><span class="params">(self, x1, x2)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.k == <span class="string">'linear'</span>:</span><br><span class="line">            <span class="keyword">return</span> np.dot(x1, x2)</span><br><span class="line">        <span class="keyword">elif</span> self.k == <span class="string">'guassian'</span>:</span><br><span class="line">            <span class="keyword">return</span> np.exp(-np.linalg.norm(x1-x2)**<span class="number">2</span> / (<span class="number">2</span> * (self.sigma ** <span class="number">2</span>)))</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">        caches = &#123;&#125;</span><br><span class="line">        <span class="comment">## y is (1,-1)</span></span><br><span class="line">        self.train_x = X <span class="comment">## point to the training data</span></span><br><span class="line">        self.train_y = y <span class="comment">## point to the training data</span></span><br><span class="line">        n, f = X.shape</span><br><span class="line">        P = np.zeros((n, n))</span><br><span class="line">        K = np.zeros((n,n))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">                k_ij = self.kernel(X[i], X[j])</span><br><span class="line">                P[i,j] = y[i] * y[j] * k_ij</span><br><span class="line">                K[i, j] = k_ij</span><br><span class="line">        P = matrix(P)</span><br><span class="line">        q = matrix(<span class="number">-1</span> * np.ones((n, <span class="number">1</span>)))</span><br><span class="line">        <span class="keyword">if</span> self.margin == <span class="string">'hard'</span>:</span><br><span class="line">            G = matrix(<span class="number">-1</span> * np.identity(n))</span><br><span class="line">            h = matrix(np.zeros((n, <span class="number">1</span>)))</span><br><span class="line">        <span class="keyword">if</span> self.margin == <span class="string">'soft'</span>:</span><br><span class="line">            G = matrix(np.vstack((<span class="number">-1</span> * np.identity(n), <span class="number">1</span> * np.identity(n))))</span><br><span class="line">            h = matrix(np.vstack((np.zeros((n,<span class="number">1</span>)), self.C * np.ones((n, <span class="number">1</span>)))))</span><br><span class="line">        </span><br><span class="line">        A = matrix(y.reshape(<span class="number">1</span>, n))</span><br><span class="line">        b = matrix(<span class="number">0.0</span>)</span><br><span class="line">        sol = solvers.qp(P,q,G,h,A,b)</span><br><span class="line">        a = np.ravel(sol[<span class="string">'x'</span>])</span><br><span class="line">        </span><br><span class="line">        sv_idx = np.where(a &gt; <span class="number">1e-5</span>)</span><br><span class="line">        self.sv_x = X[sv_idx]</span><br><span class="line">        self.sv_y = y[sv_idx]</span><br><span class="line">        self.a = a[sv_idx]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> self.k == <span class="string">'linear'</span>:</span><br><span class="line">            self.w = np.dot((self.a * self.sv_y), self.sv_x)</span><br><span class="line">            self.w_0 = np.mean(self.sv_y - np.dot(self.sv_x, self.w))</span><br><span class="line">            print(<span class="string">"The computed intercept term is &#123;0:f&#125;"</span>.format(self.w_0))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment">## can not compute w</span></span><br><span class="line">            w_0s = np.zeros_like(self.a)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(len(self.a)):</span><br><span class="line">                first_term = <span class="number">0</span></span><br><span class="line">                first_term = np.dot(self.a * self.sv_y, K[sv_idx, sv_idx[<span class="number">0</span>][i]].T)</span><br><span class="line">                w_0s[i] = self.sv_y[i] - first_term</span><br><span class="line">            self.w_0 = np.mean(w_0s)</span><br><span class="line">            print(<span class="string">"The computed intercept term is &#123;0:f&#125;"</span>.format(self.w_0))</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> self.margin == <span class="string">'hard'</span> <span class="keyword">and</span> self.evaluate_acc(X, y) &lt; <span class="number">1.0</span>:</span><br><span class="line">            <span class="keyword">raise</span> Exception(<span class="string">'Hard margin, but data not separable'</span>)</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"The rate of support vectors: &#123;0:0.6f&#125;"</span>.format(float(len(self.a))/n))</span><br><span class="line">        caches[<span class="string">'sol'</span>] = sol</span><br><span class="line">        caches[<span class="string">'K'</span>] = K</span><br><span class="line">        caches[<span class="string">'sv_idx'</span>] = sv_idx</span><br><span class="line">        <span class="keyword">return</span> caches</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict_no_bias</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.k == <span class="string">'linear'</span>:</span><br><span class="line">            <span class="keyword">return</span> np.dot(x, self.w)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            n, f = x.shape</span><br><span class="line">            preds = []</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">                pred = <span class="number">0</span></span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> range(len(self.a)):</span><br><span class="line">                    pred += self.sv_y[j] * self.a[j] * self.kernel(self.sv_x[j], x[i])</span><br><span class="line">                preds.append(pred)</span><br><span class="line">            <span class="keyword">return</span> np.array(preds)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment">## add bias to the prediction</span></span><br><span class="line">        pred = self.predict_no_bias(x)</span><br><span class="line">        <span class="keyword">return</span> np.sign(pred + self.w_0)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">evaluate_acc</span><span class="params">(self, x, y)</span>:</span></span><br><span class="line">        n , f = x.shape</span><br><span class="line">        pred = self.predict(x)</span><br><span class="line">        acc = np.sum(pred == y) / len(y)</span><br><span class="line">        <span class="keyword">return</span> acc</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">plot_Roc</span><span class="params">(self, x_input, y_input)</span>:</span></span><br><span class="line">        n, f = x_input.shape</span><br><span class="line">        pred = self.predict_no_bias(x_input)</span><br><span class="line">        <span class="comment">## argsort</span></span><br><span class="line">        p = np.argsort(pred, axis=<span class="number">0</span>)</span><br><span class="line">        x = x_input[p] </span><br><span class="line">        y = y_input[p] <span class="comment">## labels after sorting</span></span><br><span class="line">        pre = pred[p] <span class="comment">## prediction without bias after sorting</span></span><br><span class="line">        min_dis = min(pred)</span><br><span class="line">        max_dis = max(pred)</span><br><span class="line">        </span><br><span class="line">        true_lbs = np.sum(y == <span class="number">1</span>)</span><br><span class="line">        false_lbs = np.sum(y == <span class="number">-1</span>)</span><br><span class="line">        </span><br><span class="line">        b = np.linspace(min_dis, max_dis, <span class="number">200</span>)</span><br><span class="line">        b = b[::<span class="number">-1</span>]</span><br><span class="line">        tprs = []</span><br><span class="line">        fprs = []</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i, t <span class="keyword">in</span> enumerate(b):</span><br><span class="line">            TP = <span class="number">0</span></span><br><span class="line">            FP = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">                <span class="keyword">if</span> pre[j] &gt; t:</span><br><span class="line">                    <span class="keyword">if</span> y[j] == <span class="number">1</span>:</span><br><span class="line">                        TP += <span class="number">1</span></span><br><span class="line">                    <span class="keyword">if</span> y[j] == <span class="number">-1</span>:</span><br><span class="line">                        FP += <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">            tprs.append(TP/(true_lbs))</span><br><span class="line">            fprs.append(FP/(false_lbs))</span><br><span class="line">        auc = np.trapz(tprs, fprs)</span><br><span class="line">        plt.figure(figsize=(<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line">        plt.plot(fprs, tprs, color=<span class="string">'darkorange'</span>, label=<span class="string">'ROC curve: AUC=&#123;0:0.8f&#125;'</span>.format(auc))</span><br><span class="line">        plt.xlabel(<span class="string">'False Positive Rate'</span>,fontsize=<span class="number">12</span>)</span><br><span class="line">        plt.ylabel(<span class="string">'True Positive Rate'</span>,fontsize=<span class="number">12</span>)</span><br><span class="line">        plt.legend(loc=<span class="string">"lower right"</span>, fontsize=<span class="number">12</span>)</span><br><span class="line">        plt.show()  </span><br><span class="line">        <span class="keyword">return</span> tprs, fprs</span><br></pre></td></tr></table></figure><p>That’s it. </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;In this post, a SVM classifier is implemented. The SVM is implemented with “Hard Margin” and “Soft Margin”.  The “Hard Margin” is used to
      
    
    </summary>
    
    
      <category term="machine learning" scheme="https://wushbin.github.io/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>Deep Neural NetWork Sorting</title>
    <link href="https://wushbin.github.io/2018/02/01/Deep-Neural-NetWork-Sorting/"/>
    <id>https://wushbin.github.io/2018/02/01/Deep-Neural-NetWork-Sorting/</id>
    <published>2018-02-01T22:07:42.000Z</published>
    <updated>2018-03-01T16:29:20.134Z</updated>
    
    <content type="html"><![CDATA[<p>In this example, I will train a deep neural network to sort an array of 5 data. </p><h3 id="Dara-Generation"><a href="#Dara-Generation" class="headerlink" title="Dara Generation"></a>Dara Generation</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><h4 id="data-generation-function"><a href="#data-generation-function" class="headerlink" title="data generation function"></a>data generation function</h4><p>Generate data set, the input of training data is arrays of integer, the length is set to 5 for this particular example. The label of a training data is 5 independent vectors which indicate sorted indexes from the input array.<br>For example, for an array like [10, 5, 3, 6, 7], the output label is [2, 1, 3, 4, 0]. This label will be transformed to 5 independent vectors.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generateData</span><span class="params">(train, n = <span class="number">100</span>, d = <span class="number">5</span>)</span>:</span></span><br><span class="line">    dataSet = np.arange(<span class="number">-255</span>, <span class="number">256</span>, <span class="number">1</span>)</span><br><span class="line">    length = len(dataSet)</span><br><span class="line">    <span class="keyword">if</span> train:</span><br><span class="line">        np.random.seed(<span class="number">2018</span>)</span><br><span class="line">    sampleIndex = np.random.randint(length, size=(n, d))</span><br><span class="line">    </span><br><span class="line">    train_x = dataSet[sampleIndex]</span><br><span class="line">    train_y = np.argsort(sampleIndex)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#train_y = train_y[:,0]</span></span><br><span class="line">    I = np.eye(d)</span><br><span class="line">    train_label = I[train_y]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#train_label = train_label.reshape(n,d,d)</span></span><br><span class="line">    train_x_normalized = train_x / <span class="number">511.0</span></span><br><span class="line">    </span><br><span class="line">    y_0 = train_label[:,<span class="number">0</span>,:]</span><br><span class="line">    y_1 = train_label[:,<span class="number">1</span>,:]</span><br><span class="line">    y_2 = train_label[:,<span class="number">2</span>,:]</span><br><span class="line">    y_3 = train_label[:,<span class="number">3</span>,:]</span><br><span class="line">    y_4 = train_label[:,<span class="number">4</span>,:]</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> train_x, train_x_normalized, [y_0, y_1, y_2, y_3, y_4]</span><br></pre></td></tr></table></figure></p><h4 id="training-data"><a href="#training-data" class="headerlink" title="training data"></a>training data</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x_train_o, x_train, y_train = generateData(<span class="keyword">True</span>, <span class="number">500000</span>, <span class="number">5</span>)</span><br></pre></td></tr></table></figure><h4 id="validation-data"><a href="#validation-data" class="headerlink" title="validation data"></a>validation data</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x_valid_o, x_valid, y_valid = generateData(<span class="keyword">False</span>, <span class="number">50000</span>, <span class="number">5</span>)</span><br></pre></td></tr></table></figure><h3 id="Build-the-DNN-model-by-Keras"><a href="#Build-the-DNN-model-by-Keras" class="headerlink" title="Build the DNN model by Keras"></a>Build the DNN model by Keras</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> layer_utils</span><br><span class="line"><span class="keyword">from</span> keras.initializers <span class="keyword">import</span> glorot_uniform</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> optimizers, losses, activations, models</span><br></pre></td></tr></table></figure><pre><code>Using TensorFlow backend.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers.core <span class="keyword">import</span> Activation, RepeatVector, Dense, Dropout</span><br><span class="line"><span class="keyword">from</span> keras.layers.wrappers <span class="keyword">import</span> TimeDistributed</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Input, LSTM</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> load_model</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(input_shape, output_shape)</span>:</span></span><br><span class="line">    X_input = Input(input_shape)</span><br><span class="line">    X = Dense(<span class="number">8</span>, activation=<span class="string">'relu'</span>)(X_input)</span><br><span class="line">    X = Dense(<span class="number">32</span>, activation=<span class="string">'relu'</span>)(X)</span><br><span class="line">    X = Dense(<span class="number">128</span>, activation=<span class="string">'relu'</span>)(X)</span><br><span class="line">    X = Dropout(rate=<span class="number">0.5</span>)(X)</span><br><span class="line">    </span><br><span class="line">    X = Dense(<span class="number">128</span>, activation=<span class="string">'relu'</span>)(X)</span><br><span class="line">    X = Dropout(rate=<span class="number">0.5</span>)(X)</span><br><span class="line">    </span><br><span class="line">    X1 = Dense(<span class="number">32</span>, activation=<span class="string">'relu'</span>)(X)</span><br><span class="line">    X1 = Dropout(rate=<span class="number">0.5</span>)(X1)</span><br><span class="line">    </span><br><span class="line">    X2 = Dense(<span class="number">32</span>, activation=<span class="string">'relu'</span>)(X)</span><br><span class="line">    X2 = Dropout(rate=<span class="number">0.5</span>)(X2)</span><br><span class="line">    </span><br><span class="line">    X3 = Dense(<span class="number">32</span>, activation=<span class="string">'relu'</span>)(X)</span><br><span class="line">    X3 = Dropout(rate=<span class="number">0.5</span>)(X3)</span><br><span class="line">    </span><br><span class="line">    X4 = Dense(<span class="number">32</span>, activation=<span class="string">'relu'</span>)(X)</span><br><span class="line">    X4 = Dropout(rate=<span class="number">0.5</span>)(X4)</span><br><span class="line">    </span><br><span class="line">    X5 = Dense(<span class="number">32</span>, activation=<span class="string">'relu'</span>)(X)</span><br><span class="line">    X5 = Dropout(rate=<span class="number">0.5</span>)(X5)</span><br><span class="line">    </span><br><span class="line">    Y1 = Dense(output_shape, activation=<span class="string">'softmax'</span>)(X1)</span><br><span class="line">    Y2 = Dense(output_shape, activation=<span class="string">'softmax'</span>)(X2)</span><br><span class="line">    Y3 = Dense(output_shape, activation=<span class="string">'softmax'</span>)(X3)</span><br><span class="line">    Y4 = Dense(output_shape, activation=<span class="string">'softmax'</span>)(X4)</span><br><span class="line">    Y5 = Dense(output_shape, activation=<span class="string">'softmax'</span>)(X5)</span><br><span class="line">    </span><br><span class="line">    model = Model(inputs = X_input, outputs = [Y1,Y2,Y3,Y4,Y5], name=<span class="string">'dnnModel'</span>)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dnnModel = model(x_train.shape[<span class="number">1</span>:], <span class="number">5</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dnnModel.compile(optimizer = <span class="string">"adam"</span>, loss = <span class="string">"categorical_crossentropy"</span>, metrics = [<span class="string">"accuracy"</span>])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dnnModel.fit(x_train, y_train, batch_size = <span class="number">4096</span>, validation_data=(x_valid, y_valid), epochs=<span class="number">20</span>, shuffle=<span class="keyword">True</span>, verbose=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><pre><code>Train on 500000 samples, validate on 50000 samplesEpoch 19/20 - 10s - loss: 1.7910 - dense_10_loss: 0.1319 - dense_11_loss: 0.4869 - dense_12_loss: 0.5322 - dense_13_loss: 0.5044 - dense_14_loss: 0.1357 - dense_10_acc: 0.9594 - dense_11_acc: 0.8106 - dense_12_acc: 0.7886 - dense_13_acc: 0.7934 - dense_14_acc: 0.9565 - val_loss: 0.4033 - val_dense_10_loss: 0.0304 - val_dense_11_loss: 0.1082 - val_dense_12_loss: 0.1206 - val_dense_13_loss: 0.1106 - val_dense_14_loss: 0.0334 - val_dense_10_acc: 0.9893 - val_dense_11_acc: 0.9739 - val_dense_12_acc: 0.9713 - val_dense_13_acc: 0.9730 - val_dense_14_acc: 0.9865Epoch 20/20 - 10s - loss: 1.7874 - dense_10_loss: 0.1309 - dense_11_loss: 0.4863 - dense_12_loss: 0.5303 - dense_13_loss: 0.5029 - dense_14_loss: 0.1369 - dense_10_acc: 0.9595 - dense_11_acc: 0.8112 - dense_12_acc: 0.7899 - dense_13_acc: 0.7953 - dense_14_acc: 0.9565 - val_loss: 0.3875 - val_dense_10_loss: 0.0310 - val_dense_11_loss: 0.1055 - val_dense_12_loss: 0.1149 - val_dense_13_loss: 0.1076 - val_dense_14_loss: 0.0286 - val_dense_10_acc: 0.9864 - val_dense_11_acc: 0.9743 - val_dense_12_acc: 0.9731 - val_dense_13_acc: 0.9741 - val_dense_14_acc: 0.9888&lt;keras.callbacks.History at 0x125df0550&gt;</code></pre><p>Make prediction<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x_test_o, x_test, y_test = generateData(<span class="keyword">False</span>, <span class="number">10</span>, <span class="number">5</span>)</span><br><span class="line">y_predict = dnnModel.predict(x_test)</span><br></pre></td></tr></table></figure></p><h4 id="Decode-function-for-prediction"><a href="#Decode-function-for-prediction" class="headerlink" title="Decode function for prediction"></a>Decode function for prediction</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decode</span><span class="params">(x_test_o, y_predict)</span>:</span></span><br><span class="line">    output = []</span><br><span class="line">    n, f = x_test_o.shape</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        index = []</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(f):</span><br><span class="line">            idx_v = y_predict[j][i]</span><br><span class="line">            index.append(x_test_o[i][np.argmax(idx_v)])</span><br><span class="line">        output.append(index)</span><br><span class="line">    <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">corrected = decode(x_test_o, y_predict)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">corrected</span><br></pre></td></tr></table></figure><h4 id="result"><a href="#result" class="headerlink" title="result"></a>result</h4><pre><code>[[-215, -120, 75, 129, 167], [-232, -187, -161, -82, 58], [-152, 12, 32, 135, 255], [-224, -208, -20, -3, 22], [-209, 5, 63, 112, 150], [-203, -70, -28, 219, 247], [-170, -16, -1, 15, 72], [-126, -118, -91, 166, 230], [-199, -123, -50, 54, 132], [-246, -79, -75, 139, 196]]</code></pre><p>It seems that this model do a good job in sorting the test data. </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;In this example, I will train a deep neural network to sort an array of 5 data. &lt;/p&gt;
&lt;h3 id=&quot;Dara-Generation&quot;&gt;&lt;a href=&quot;#Dara-Generation&quot; 
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Decision Tree and Random Forest with Decision Stump</title>
    <link href="https://wushbin.github.io/2018/02/01/Decision-Tree-Implementation/"/>
    <id>https://wushbin.github.io/2018/02/01/Decision-Tree-Implementation/</id>
    <published>2018-02-01T21:11:53.000Z</published>
    <updated>2018-03-01T21:13:31.373Z</updated>
    
    <content type="html"><![CDATA[<p>This post is to implement a Binary Decision Tree and a Random Forest with Decision Stump</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;This post is to implement a Binary Decision Tree and a Random Forest with Decision Stump&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="machine learning" scheme="https://wushbin.github.io/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>PacMan Game: Artificial Intellegence</title>
    <link href="https://wushbin.github.io/2017/12/23/PacMan-Game-Artificial-Intellegence/"/>
    <id>https://wushbin.github.io/2017/12/23/PacMan-Game-Artificial-Intellegence/</id>
    <published>2017-12-24T02:54:37.000Z</published>
    <updated>2017-12-23T13:54:37.995Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Optical Character Recognition</title>
    <link href="https://wushbin.github.io/2017/12/23/Optical-Character-Recognition/"/>
    <id>https://wushbin.github.io/2017/12/23/Optical-Character-Recognition/</id>
    <published>2017-12-24T02:53:38.000Z</published>
    <updated>2018-02-13T20:47:22.348Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Image-Segmentation"><a href="#Image-Segmentation" class="headerlink" title="Image Segmentation"></a>Image Segmentation</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.image <span class="keyword">as</span> mpimg</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">showImage</span><span class="params">(image)</span>:</span></span><br><span class="line">    plt.figure(figsize=(<span class="number">8</span>,<span class="number">8</span>))</span><br><span class="line">    plt.imshow(image)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">img = cv2.imread(<span class="string">'IMG_0377_o.JPG'</span>, cv2.IMREAD_COLOR)</span><br><span class="line">showImage(img)</span><br></pre></td></tr></table></figure><p><img src="/images/Optical-Character-Recognition/output_3_0.png" alt="png"></p><h4 id="Use-the-Canny-Algorithm-in-OpenCV-to-extract-the-edges"><a href="#Use-the-Canny-Algorithm-in-OpenCV-to-extract-the-edges" class="headerlink" title="Use the Canny Algorithm in OpenCV to extract the edges"></a>Use the Canny Algorithm in OpenCV to extract the edges</h4><p>Canny algorithm is applied to extracted edges in this image, then the edges can be use to local contours</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">edges = cv2.Canny(img,<span class="number">200</span>,<span class="number">240</span>)</span><br><span class="line">plt.figure(figsize=(<span class="number">16</span>,<span class="number">8</span>))</span><br><span class="line">plt.subplot(<span class="number">121</span>),plt.imshow(img,cmap = <span class="string">'gray'</span>)</span><br><span class="line">plt.title(<span class="string">'Original Image'</span>), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.subplot(<span class="number">122</span>),plt.imshow(edges,cmap = <span class="string">'gray'</span>)</span><br><span class="line">plt.title(<span class="string">'Edge Image'</span>), plt.xticks([]), plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/images/Optical-Character-Recognition/output_5_0.png" alt="png"></p><h4 id="Find-countours-based-on-the-edges-extracted-from-image"><a href="#Find-countours-based-on-the-edges-extracted-from-image" class="headerlink" title="Find countours based on the edges extracted from image"></a>Find countours based on the edges extracted from image</h4><p>Edges found by canny is used to find contours of this images, from the images showed belowed we can see that  a lot of contours are extracted from the image.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">im2, contours, hierarchy = cv2.findContours(edges, cv2.RETR_EXTERNAL , cv2.CHAIN_APPROX_SIMPLE)</span><br><span class="line">imcopy = img.copy()</span><br><span class="line">cv2.drawContours(imcopy, contours, <span class="number">-1</span>, (<span class="number">0</span>,<span class="number">255</span>,<span class="number">0</span>), <span class="number">10</span>)</span><br><span class="line">showImage(imcopy)</span><br></pre></td></tr></table></figure><p><img src="/images/Optical-Character-Recognition/output_7_0.png" alt="png"></p><h4 id="Find-the-Max-Contor-with-largest-contour-area"><a href="#Find-the-Max-Contor-with-largest-contour-area" class="headerlink" title="Find the Max Contor with largest contour area"></a>Find the Max Contor with largest contour area</h4><p>For this problem, the white paper sheet in this image has the largest contour, we can extracted the contour of this white paper sheet by finding the largest contour.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">c = max(contours, key = cv2.contourArea)</span><br><span class="line">imcopy = img.copy()</span><br><span class="line">cv2.drawContours(imcopy, c, <span class="number">-1</span>, (<span class="number">0</span>,<span class="number">255</span>,<span class="number">0</span>), <span class="number">10</span>)</span><br><span class="line">showImage(imcopy)</span><br></pre></td></tr></table></figure><p><img src="/images/Optical-Character-Recognition/output_10_0.png" alt="png"></p><h4 id="Approximate-the-Contor"><a href="#Approximate-the-Contor" class="headerlink" title="Approximate the Contor"></a>Approximate the Contor</h4><p>After finding the contour of this white paper sheet, we can use Geometric shape such as rectangle or polygon to approximate this countour. From the result showed belowed, we can see that polygon did well in the shape approxiamtion.   </p><h5 id="rectangle-approximation"><a href="#rectangle-approximation" class="headerlink" title="rectangle approximation"></a>rectangle approximation</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">imcopy = img.copy()</span><br><span class="line">x,y,w,h = cv2.boundingRect(c)</span><br><span class="line">cv2.rectangle(imcopy,(x,y),(x+w,y+h),(<span class="number">0</span>,<span class="number">255</span>,<span class="number">0</span>),<span class="number">10</span>)</span><br><span class="line">showImage(imcopy)</span><br></pre></td></tr></table></figure><p><img src="/images/Optical-Character-Recognition/output_13_0.png" alt="png"></p><h5 id="Polygon-approximation"><a href="#Polygon-approximation" class="headerlink" title="Polygon approximation"></a>Polygon approximation</h5><p>By applying polygon approximation, the 4 coner points was extracted, which can be used in the later experiment of perspective transformation.   </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">epsilon = <span class="number">0.01</span>*cv2.arcLength(c,<span class="keyword">True</span>)</span><br><span class="line">approx = cv2.approxPolyDP(c,epsilon,<span class="keyword">True</span>)</span><br><span class="line">imcopy = img.copy()</span><br><span class="line">cv2.drawContours(imcopy, [approx], <span class="number">-1</span>, (<span class="number">0</span>,<span class="number">255</span>,<span class="number">0</span>), <span class="number">12</span>)</span><br><span class="line">cv2.putText(imcopy, <span class="string">'Point 1'</span>, (approx[<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>], approx[<span class="number">0</span>][<span class="number">0</span>][<span class="number">1</span>]),cv2.FONT_HERSHEY_PLAIN, <span class="number">6</span>,(<span class="number">255</span>,<span class="number">0</span>,<span class="number">0</span>),<span class="number">10</span>, cv2.LINE_AA)</span><br><span class="line">cv2.putText(imcopy, <span class="string">'Point 2'</span>, (approx[<span class="number">1</span>][<span class="number">0</span>][<span class="number">0</span>], approx[<span class="number">1</span>][<span class="number">0</span>][<span class="number">1</span>]),cv2.FONT_HERSHEY_PLAIN, <span class="number">6</span>,(<span class="number">255</span>,<span class="number">0</span>,<span class="number">0</span>),<span class="number">10</span>, cv2.LINE_AA)</span><br><span class="line">cv2.putText(imcopy, <span class="string">'Point 3'</span>, (approx[<span class="number">2</span>][<span class="number">0</span>][<span class="number">0</span>], approx[<span class="number">2</span>][<span class="number">0</span>][<span class="number">1</span>]),cv2.FONT_HERSHEY_PLAIN, <span class="number">6</span>,(<span class="number">255</span>,<span class="number">0</span>,<span class="number">0</span>),<span class="number">10</span>, cv2.LINE_AA)</span><br><span class="line">cv2.putText(imcopy, <span class="string">'Point 4'</span>, (approx[<span class="number">3</span>][<span class="number">0</span>][<span class="number">0</span>], approx[<span class="number">3</span>][<span class="number">0</span>][<span class="number">1</span>]),cv2.FONT_HERSHEY_PLAIN, <span class="number">6</span>,(<span class="number">255</span>,<span class="number">0</span>,<span class="number">0</span>),<span class="number">10</span>, cv2.LINE_AA)</span><br><span class="line">showImage(imcopy)</span><br></pre></td></tr></table></figure><p><img src="/images/Optical-Character-Recognition/output_15_0.png" alt="png"></p><h4 id="Applied-Perspective-Transformation"><a href="#Applied-Perspective-Transformation" class="headerlink" title="Applied Perspective Transformation"></a>Applied Perspective Transformation</h4><ul><li>Projective transformation(Perspective transformation) is the combination of affine transformation and projective wrap.<br>Suppose(x, y, 1) is a point in homogeneous coordinate. The projective transformation of this point is as followed.  <script type="math/tex; mode=display">\begin{bmatrix}  x'\\  y'\\  w'\\\end{bmatrix}=\begin{bmatrix}  a & b & c \\  d & e & f \\  g & h & 1 \\\end{bmatrix}*\begin{bmatrix}  x\\  y\\  1\\\end{bmatrix}</script></li></ul><p>This 8 parameters matrix maps point$(x,y,1)$ in one projective to point $(x’/w’,y’/w’,1)$ in another projective.  </p><script type="math/tex; mode=display">x' =  \frac{ax+by+c}{gx+hy}</script><script type="math/tex; mode=display">y' =  \frac{dx+ey+f}{gx+hy}</script><p>We can get 2 equations from one point mapping, to solve this 8 parameter tranformation equation, we need more than 4 points mapping. When this tranformation equation be solved, can can applied it to get a new image.  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">imcopy = img.copy()</span><br><span class="line">pts1 = np.float32(approx)</span><br><span class="line"><span class="comment">## the size is propotional to a US letter's size 425:550 = 8.5:11</span></span><br><span class="line">pts2 = np.float32([[<span class="number">0</span>,<span class="number">550</span>],[<span class="number">425</span>,<span class="number">550</span>],[<span class="number">425</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">0</span>]])</span><br><span class="line">M = cv2.getPerspectiveTransform(pts1,pts2)</span><br><span class="line">dst = cv2.warpPerspective(imcopy,M,(<span class="number">425</span>,<span class="number">550</span>))</span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>,<span class="number">16</span>))</span><br><span class="line">plt.subplot(<span class="number">121</span>),plt.imshow(imcopy),plt.title(<span class="string">'Input'</span>)</span><br><span class="line">plt.subplot(<span class="number">122</span>),plt.imshow(dst),plt.title(<span class="string">'Output'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/images/Optical-Character-Recognition/output_18_0.png" alt="png"></p><h3 id="Build-a-CNN-model-with-tensorflow"><a href="#Build-a-CNN-model-with-tensorflow" class="headerlink" title="Build a CNN model with tensorflow"></a>Build a CNN model with tensorflow</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">'MNIST_data'</span>, one_hot=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(shape,name)</span>:</span></span><br><span class="line">    initial = tf.truncated_normal(shape, stddev=<span class="number">0.1</span>)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial, name=name)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(shape,name)</span>:</span></span><br><span class="line">    initial = tf.constant(<span class="number">0.1</span>, shape=shape)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial, name=name)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(x, W)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.conv2d(x, W, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.max_pool(x, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_placeholders</span><span class="params">(n_x=<span class="number">784</span>, n_y=<span class="number">10</span>)</span>:</span></span><br><span class="line">    x = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, n_x])</span><br><span class="line">    y_ = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, n_y])</span><br><span class="line">    keep_prob = tf.placeholder(tf.float32)</span><br><span class="line">    <span class="keyword">return</span> x, y_, keep_prob</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment">## first CNN layer</span></span><br><span class="line">    W_conv1 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">32</span>], <span class="string">'W_conv1'</span>)</span><br><span class="line">    b_conv1 = bias_variable([<span class="number">32</span>], <span class="string">'b_conv1'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">## second CNN layer</span></span><br><span class="line">    W_conv2 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">64</span>], <span class="string">'W_conv2'</span>)</span><br><span class="line">    b_conv2 = bias_variable([<span class="number">64</span>], <span class="string">'b_conv2'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">## fully connected layer</span></span><br><span class="line">    W_fc1 = weight_variable([<span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>, <span class="number">1024</span>], <span class="string">'W_fc1'</span>)</span><br><span class="line">    b_fc1 = bias_variable([<span class="number">1024</span>], <span class="string">'b_fc1'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">## read out layer</span></span><br><span class="line">    W_fc2 = weight_variable([<span class="number">1024</span>, <span class="number">10</span>], <span class="string">'W_fc2'</span>)</span><br><span class="line">    b_fc2 = bias_variable([<span class="number">10</span>], <span class="string">'b_fc2'</span>)</span><br><span class="line">    </span><br><span class="line">    parameters = &#123;<span class="string">'W_conv1'</span>: W_conv1,</span><br><span class="line">                  <span class="string">'b_conv1'</span>: b_conv1,</span><br><span class="line">                  <span class="string">'W_conv2'</span>: W_conv2,</span><br><span class="line">                  <span class="string">'b_conv2'</span>: b_conv2,</span><br><span class="line">                  <span class="string">'W_fc1'</span>: W_fc1,</span><br><span class="line">                  <span class="string">'b_fc1'</span>: b_fc1,</span><br><span class="line">                  <span class="string">'W_fc2'</span>: W_fc2,</span><br><span class="line">                  <span class="string">'b_fc2'</span>: b_fc2</span><br><span class="line">                 &#125;</span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_prop</span><span class="params">(x, keep_prob, parameters)</span>:</span></span><br><span class="line">    W_conv1 = parameters[<span class="string">'W_conv1'</span>]</span><br><span class="line">    b_conv1 = parameters[<span class="string">'b_conv1'</span>]</span><br><span class="line">    W_conv2 = parameters[<span class="string">'W_conv2'</span>]</span><br><span class="line">    b_conv2 = parameters[<span class="string">'b_conv2'</span>]</span><br><span class="line">    W_fc1 = parameters[<span class="string">'W_fc1'</span>]</span><br><span class="line">    b_fc1 = parameters[<span class="string">'b_fc1'</span>]</span><br><span class="line">    W_fc2 = parameters[<span class="string">'W_fc2'</span>]</span><br><span class="line">    b_fc2 = parameters[<span class="string">'b_fc2'</span>]</span><br><span class="line">    </span><br><span class="line">    x_image = tf.reshape(x, [<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>])</span><br><span class="line">    </span><br><span class="line">    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)</span><br><span class="line">    h_pool1 = max_pool_2x2(h_conv1)</span><br><span class="line">    </span><br><span class="line">    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)</span><br><span class="line">    h_pool2 = max_pool_2x2(h_conv2)</span><br><span class="line">    </span><br><span class="line">    h_pool2_flat = tf.reshape(h_pool2, [<span class="number">-1</span>, <span class="number">7</span>*<span class="number">7</span>*<span class="number">64</span>])</span><br><span class="line">    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)</span><br><span class="line">    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)</span><br><span class="line">    y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> y_conv</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tf.reset_default_graph()</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    x, y_, keep_prob = create_placeholders();</span><br><span class="line">    parameters = initialize_parameters()</span><br><span class="line">    y_conv = forward_prop(x, <span class="number">1.0</span>, parameters)</span><br><span class="line">    print(<span class="string">"y_conv = "</span> + str(y_conv))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">num_epochs = <span class="number">16</span></span><br><span class="line">learning_rate = <span class="number">1e-4</span></span><br><span class="line">costs = []</span><br><span class="line">x, y_, keep_prob = create_placeholders()</span><br><span class="line">parameters = initialize_parameters()</span><br><span class="line">y_conv = forward_prop(x, keep_prob, parameters)</span><br><span class="line"></span><br><span class="line">cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))</span><br><span class="line"><span class="comment">## Optimizer</span></span><br><span class="line">train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)</span><br><span class="line">correct_prediction = tf.equal(tf.argmax(y_conv, <span class="number">1</span>), tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        epoch_cost = <span class="number">0.0</span></span><br><span class="line">        num_minibatches = int(<span class="number">55000</span>/<span class="number">50</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1100</span>):</span><br><span class="line">            batch = mnist.train.next_batch(<span class="number">50</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">                train_accuracy = accuracy.eval(feed_dict=&#123;x: batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>],keep_prob:<span class="number">1.0</span>&#125;)</span><br><span class="line">                print(<span class="string">'epoch %d, step %d, training accuracy %g'</span> % (epoch, i, train_accuracy))</span><br><span class="line"></span><br><span class="line">            _, minibatch_cost = sess.run([train_step, cross_entropy],feed_dict=&#123;x: batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>], keep_prob: <span class="number">0.5</span>&#125;)</span><br><span class="line">            epoch_cost += minibatch_cost / num_minibatches</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">if</span> epoch % <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"Cost after epoch %i: %f"</span> % (epoch, epoch_cost))</span><br><span class="line">        costs.append(epoch_cost)</span><br><span class="line">    </span><br><span class="line">    plt.plot(np.squeeze(costs))</span><br><span class="line">    plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'iterations per epoch'</span>)</span><br><span class="line">    plt.title(<span class="string">"Learning rate ="</span> + str(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">    parameters = sess.run(parameters)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Parameters have been trained!"</span>)</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">'test accuracy %g'</span> % accuracy.eval(feed_dict=&#123;x: mnist.test.images, y_: mnist.test.labels, keep_prob: <span class="number">1.0</span>&#125;))</span><br></pre></td></tr></table></figure><p><img src="/images/Optical-Character-Recognition/output_8_1.png" alt="png"></p><pre><code>Parameters have been trained!test accuracy 0.9927</code></pre><h4 id="Save-the-parameters-to-local-data"><a href="#Save-the-parameters-to-local-data" class="headerlink" title="Save the parameters to local data"></a>Save the parameters to local data</h4><p>Because the CNN model takes a long long time to train, thus it will save time if we can save the trained parameters to a local file </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line">pickle.dump(parameters, open(<span class="string">"params.pkl"</span>, <span class="string">"wb"</span>))</span><br></pre></td></tr></table></figure><h4 id="Load-data-from-saved-file"><a href="#Load-data-from-saved-file" class="headerlink" title="Load data from saved file"></a>Load data from saved file</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">testparams = pickle.load(open(<span class="string">"params.pkl"</span>,<span class="string">"rb"</span>))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(x, parameters)</span>:</span></span><br><span class="line">    W_conv1 = tf.convert_to_tensor(parameters[<span class="string">'W_conv1'</span>])</span><br><span class="line">    b_conv1 = tf.convert_to_tensor(parameters[<span class="string">'b_conv1'</span>])</span><br><span class="line">    W_conv2 = tf.convert_to_tensor(parameters[<span class="string">'W_conv2'</span>])</span><br><span class="line">    b_conv2 = tf.convert_to_tensor(parameters[<span class="string">'b_conv2'</span>])</span><br><span class="line">    W_fc1 = tf.convert_to_tensor(parameters[<span class="string">'W_fc1'</span>])</span><br><span class="line">    b_fc1 = tf.convert_to_tensor(parameters[<span class="string">'b_fc1'</span>])</span><br><span class="line">    W_fc2 = tf.convert_to_tensor(parameters[<span class="string">'W_fc2'</span>])</span><br><span class="line">    b_fc2 = tf.convert_to_tensor(parameters[<span class="string">'b_fc2'</span>])</span><br><span class="line">    </span><br><span class="line">    parameters = &#123;<span class="string">'W_conv1'</span>: W_conv1,</span><br><span class="line">                  <span class="string">'b_conv1'</span>: b_conv1,</span><br><span class="line">                  <span class="string">'W_conv2'</span>: W_conv2,</span><br><span class="line">                  <span class="string">'b_conv2'</span>: b_conv2,</span><br><span class="line">                  <span class="string">'W_fc1'</span>: W_fc1,</span><br><span class="line">                  <span class="string">'b_fc1'</span>: b_fc1,</span><br><span class="line">                  <span class="string">'W_fc2'</span>: W_fc2,</span><br><span class="line">                  <span class="string">'b_fc2'</span>: b_fc2</span><br><span class="line">                 &#125;</span><br><span class="line">    </span><br><span class="line">    x_input = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, <span class="number">784</span>])</span><br><span class="line">    </span><br><span class="line">    y_out = forward_propagation_for_predict(x_input, parameters)</span><br><span class="line">    p = tf.argmax(y_out, <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    sess = tf.Session()</span><br><span class="line">    prediction = sess.run(p, feed_dict = &#123;x_input: x&#125;)</span><br><span class="line">    <span class="keyword">return</span> prediction</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation_for_predict</span><span class="params">(x, parameters)</span>:</span></span><br><span class="line">    W_conv1 = parameters[<span class="string">'W_conv1'</span>]</span><br><span class="line">    b_conv1 = parameters[<span class="string">'b_conv1'</span>]</span><br><span class="line">    W_conv2 = parameters[<span class="string">'W_conv2'</span>]</span><br><span class="line">    b_conv2 = parameters[<span class="string">'b_conv2'</span>]</span><br><span class="line">    W_fc1 = parameters[<span class="string">'W_fc1'</span>]</span><br><span class="line">    b_fc1 = parameters[<span class="string">'b_fc1'</span>]</span><br><span class="line">    W_fc2 = parameters[<span class="string">'W_fc2'</span>]</span><br><span class="line">    b_fc2 = parameters[<span class="string">'b_fc2'</span>]</span><br><span class="line">    </span><br><span class="line">    x_image = tf.reshape(x, [<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>])</span><br><span class="line">    </span><br><span class="line">    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)</span><br><span class="line">    h_pool1 = max_pool_2x2(h_conv1)</span><br><span class="line">    </span><br><span class="line">    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)</span><br><span class="line">    h_pool2 = max_pool_2x2(h_conv2)</span><br><span class="line">    </span><br><span class="line">    h_pool2_flat = tf.reshape(h_pool2, [<span class="number">-1</span>, <span class="number">7</span>*<span class="number">7</span>*<span class="number">64</span>])</span><br><span class="line">    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)</span><br><span class="line">    h_fc1_drop = tf.nn.dropout(h_fc1, <span class="number">1.0</span>)</span><br><span class="line">    y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> y_conv</span><br></pre></td></tr></table></figure><h4 id="Experiment-of-Self-Written-Digits-Image"><a href="#Experiment-of-Self-Written-Digits-Image" class="headerlink" title="Experiment of Self-Written Digits Image"></a>Experiment of Self-Written Digits Image</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">img = cv2.imread(<span class="string">'test_digit4.png'</span>, cv2.IMREAD_COLOR)</span><br><span class="line">img = cv2.cvtColor( img, cv2.COLOR_RGB2GRAY )</span><br><span class="line">im_gray = cv2.GaussianBlur(img, (<span class="number">5</span>, <span class="number">5</span>), <span class="number">0</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">10</span>,<span class="number">5</span>))</span><br><span class="line">plt.imshow(im_gray, cmap = <span class="string">'gray'</span>, interpolation = <span class="string">'bicubic'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/images/Optical-Character-Recognition/output_18_0_2.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">blur_img = cv2.GaussianBlur(img, (<span class="number">5</span>,<span class="number">5</span>), <span class="number">0</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ret,thresh = cv2.threshold(img,<span class="number">127</span>,<span class="number">255</span>,cv2.THRESH_BINARY_INV)</span><br><span class="line">im2, contours, hierarchy = cv2.findContours(thresh, cv2.RETR_EXTERNAL , cv2.CHAIN_APPROX_SIMPLE)</span><br><span class="line">rects = [cv2.boundingRect(contour) <span class="keyword">for</span> contour <span class="keyword">in</span> contours]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.imshow(thresh, cmap = <span class="string">'gray'</span>, interpolation = <span class="string">'bicubic'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/images/Optical-Character-Recognition/output_21_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generateImage</span><span class="params">(small)</span>:</span></span><br><span class="line">    row, col = small.shape</span><br><span class="line">    </span><br><span class="line">    small = cv2.resize(small, (<span class="number">20</span>*col//row, <span class="number">20</span>), interpolation=cv2.INTER_AREA)</span><br><span class="line">    <span class="comment">##small = cv2.dilate(small, (3, 3))</span></span><br><span class="line">    small = cv2.dilate(small, (<span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">    blank_image = np.zeros((<span class="number">28</span>,<span class="number">28</span>), np.uint8)</span><br><span class="line">    row, col = small.shape</span><br><span class="line">    shift_x = (<span class="number">28</span> - row) // <span class="number">2</span></span><br><span class="line">    shift_y = (<span class="number">28</span> - col) // <span class="number">2</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,row):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>,col):</span><br><span class="line">            blank_image[i+shift_x][j+shift_y] = small[i][j]</span><br><span class="line">    <span class="keyword">return</span> blank_image</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">images = []</span><br><span class="line"><span class="keyword">for</span> i, rect <span class="keyword">in</span> enumerate(rects):</span><br><span class="line">    <span class="comment"># Draw the rectangles</span></span><br><span class="line">    cv2.rectangle(img, (rect[<span class="number">0</span>], rect[<span class="number">1</span>]), (rect[<span class="number">0</span>] + rect[<span class="number">2</span>], rect[<span class="number">1</span>] + rect[<span class="number">3</span>]), (<span class="number">0</span>, <span class="number">255</span>, <span class="number">0</span>), <span class="number">3</span>) </span><br><span class="line">    <span class="comment"># Make the rectangular region around the digit</span></span><br><span class="line">    width = int(rect[<span class="number">3</span>] * <span class="number">1.1</span>)</span><br><span class="line">    height = int(rect[<span class="number">2</span>] * <span class="number">1.1</span>)</span><br><span class="line">    pt1 = int(rect[<span class="number">1</span>] + rect[<span class="number">3</span>] // <span class="number">2</span> - width // <span class="number">2</span>)</span><br><span class="line">    pt2 = int(rect[<span class="number">0</span>] + rect[<span class="number">2</span>] // <span class="number">2</span> - height // <span class="number">2</span>)</span><br><span class="line">    roi = thresh[pt1:pt1+width, pt2:pt2+height]</span><br><span class="line">    <span class="comment"># Resize the image</span></span><br><span class="line">    roi = generateImage(roi)</span><br><span class="line">    <span class="comment"># dilate the image</span></span><br><span class="line">    images.append(roi)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">fig=plt.figure(figsize=(<span class="number">12</span>,<span class="number">4</span>))</span><br><span class="line"><span class="keyword">for</span> i, image <span class="keyword">in</span> enumerate(images):</span><br><span class="line">    <span class="comment">#image = cv2.bitwise_not(image)</span></span><br><span class="line">    sub_fig = fig.add_subplot(<span class="number">2</span>,<span class="number">5</span>,i+<span class="number">1</span>)</span><br><span class="line">    sub_fig.imshow(image, cmap = <span class="string">'gray'</span>, interpolation = <span class="string">'bicubic'</span>)</span><br><span class="line">    images[i] = image</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/images/Optical-Character-Recognition/output_24_0.png" alt="png"></p><h4 id="Test-my-Own-hand-written"><a href="#Test-my-Own-hand-written" class="headerlink" title="Test my Own hand written"></a>Test my Own hand written</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">fig_out=plt.figure(figsize=(<span class="number">12</span>,<span class="number">6</span>))</span><br><span class="line"><span class="keyword">for</span> i, image <span class="keyword">in</span> enumerate(images):</span><br><span class="line">    test_im = np.array([image.reshape(<span class="number">28</span>*<span class="number">28</span>)], <span class="string">'float32'</span>)</span><br><span class="line">    my_image_prediction=predict(test_im, testparams)</span><br><span class="line">    label = np.squeeze(my_image_prediction)</span><br><span class="line">    sub_fig = fig_out.add_subplot(<span class="number">2</span>,<span class="number">5</span>,i+<span class="number">1</span>)</span><br><span class="line">    sub_fig.annotate(label, xy=(<span class="number">2</span>, <span class="number">1</span>),size= <span class="number">25</span>,color=<span class="string">'#ee8d18'</span>, xytext=(<span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">    sub_fig.imshow(image.reshape(<span class="number">28</span>,<span class="number">28</span>), cmap = <span class="string">'gray'</span>, interpolation = <span class="string">'bicubic'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/images/Optical-Character-Recognition/output_26_0.png" alt="png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Image-Segmentation&quot;&gt;&lt;a href=&quot;#Image-Segmentation&quot; class=&quot;headerlink&quot; title=&quot;Image Segmentation&quot;&gt;&lt;/a&gt;Image Segmentation&lt;/h3&gt;&lt;figure c
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Speech Recognition</title>
    <link href="https://wushbin.github.io/2017/12/23/Speech-Recognition/"/>
    <id>https://wushbin.github.io/2017/12/23/Speech-Recognition/</id>
    <published>2017-12-24T02:51:56.000Z</published>
    <updated>2017-12-23T13:51:56.572Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Sharing On Campus: Web Application</title>
    <link href="https://wushbin.github.io/2017/10/12/Sharing-On-Campus-Web-Application/"/>
    <id>https://wushbin.github.io/2017/10/12/Sharing-On-Campus-Web-Application/</id>
    <published>2017-10-13T01:41:29.000Z</published>
    <updated>2018-01-13T21:58:42.165Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://github.com/wushbin/sharingoncampus" target="_blank" rel="noopener">link to this project in github</a><br><a href="https://sharingoncampus.herokuapp.com/" target="_blank" rel="noopener">link to webapplication</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://github.com/wushbin/sharingoncampus&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;link to this project in github&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https:
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Word Embedding: Syntactics or Semantics</title>
    <link href="https://wushbin.github.io/2017/10/09/Word-Embedding-Syntactics-or-Semantics/"/>
    <id>https://wushbin.github.io/2017/10/09/Word-Embedding-Syntactics-or-Semantics/</id>
    <published>2017-10-10T01:39:09.000Z</published>
    <updated>2018-02-13T18:25:21.581Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>Word embedding is a mapping from words to vector representations of the words. The method that represents a word as a vector or array of numbers related in someway to counts is called vector semantics. Ideally, the geometry of the vectors will capture the semantic and syntactic meaning of the words—for example, words similar in meaning should have representations which are close to each other in the vector space.<br>In this project, a word co-occurrence matrix of 10,000 words from Wikipedia corpus with 1.5 billion words will be used to study the two method learned from class. Entry of the matrix denotes the number of times in the corpus this the ith and jth words occur within 5 words of each other The data file in this project is downloaded from Standford CS168 course website. The dictionary in this data set is sorted by frequency, thus the first row of the co-occurrence matrix is the most frequent word.<br><a href="http://web.stanford.edu/class/cs168/co_occur.csv" target="_blank" rel="noopener">data file</a></p><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><h4 id="Sparse-Vector-Representation"><a href="#Sparse-Vector-Representation" class="headerlink" title="Sparse Vector Representation"></a>Sparse Vector Representation</h4><p>The co-occurrence matrix in represented each cell by the raw frequency of the co-occurrence of two words. The raw frequency in a matrix may be skewed. Pointwise mutual information PPMI is a good measure for association between words which can tell us how much often the two words occur.<br>The pointwise mutual information is a measure of how often two events x and y occur, compared with what we would expect if they were independent:</p><script type="math/tex; mode=display">PMI(x,y) = log_2\frac{P(x,y)}{P(x)P(y)}</script><p>PMI between two words is:</p><script type="math/tex; mode=display">PMI(word_1,word_2) = log_2\frac{P(word_1,word_2)}{P(word_1)P(word_2)}</script><p>Positive PMI(PPMI) replace all negative values of PMI by 0, which eliminate the problem of unreliable negative PMI values.</p><script type="math/tex; mode=display">PPMI(word_1,word_2) = max(log_2\frac{P(word_1,word_2)}{P(word_1)P(word_2)}, 0)</script><p>For infrequent events, PPMI is bias which would have high PMI values for very rare word. There are two solution for this problem, one is to give rare word slightly higher probabilities, one is add-one smoothing.<br>In this project, the PPMI with add-two smoothing is used to build a model which can compute the similar target words of a given context words. In this project, the similarity of two words will be defined as the cosine-similarity between their embeddings.<br>Then, analogy analysis is performed in this model. Analogy analysis is by given two words with some relation and a third word, find the fourth word which has the similar relation with the third word as the relation between the first two words.<br>For example, if given ‘man’, ‘woman’, ‘king’, the fourth word should be ‘queen’.  </p><h4 id="Cosine-Similarity"><a href="#Cosine-Similarity" class="headerlink" title="Cosine Similarity"></a>Cosine Similarity</h4><p>Given two word vector, we can measure the similarity of these two vectoe by the inner product or dot product of them.  </p><script type="math/tex; mode=display">dotProduct(v, w) = v \cdot w = v_1w_1 + v_2w_2 + ...+ v_nw_n</script><p>The dot product is high when two vectors have large values in same dimension and it value is low when they are orthogonal vectors with complementary distributions. However, a problem exist in this metric, dot product will be longer is these two vectors are longer, the length of a vector is  </p><script type="math/tex; mode=display">|v| = \sqrt{\sum_{i = 1}^{N} v_i^2}</script><p>This means that more frequent words will have higher dot product. The solution for this problem is dividing the dot product by the length of two vectors,which is the $cosine$ of the angle bwtween these two vectors.  </p><h4 id="Dense-Vector-Representation"><a href="#Dense-Vector-Representation" class="headerlink" title="Dense Vector Representation"></a>Dense Vector Representation</h4><p>Short and dense vectors have a number of potential advantages. Short vectors may be easier    to use as features in machine learning. Dense    vectors    may    generalize    better than storing explicit counts. In addition, dense vectors may perform better in capturing synonymy than sparse vectors.<br>Singular Value Decomposition(SVD) is a classic method for generating dense vectors, which is applied to a task of generating embedding from term document matrices in a model called Latent Semantic Analysis (LSA).<br>SVD factorizes any a rectangular matrix $X$ into the product of three matrices $W, s, C$.<br>For $W$, its rows corresponding    to original matrix but columns    represents a dimension in a    new    latent space, such that the column vectors are orthogonal to each other. Columns are ordered    by the amount of variance in the dataset each new dimension accounts for.<br>$S$ is a diagonal matrix of singular values expressing    the importance of each dimension.$C$ represents documents or contexts, but each row now represents one of the new latent dimensions and the m row vectors are orthogonal to each other.<br>In this project, SVD is applied to the term-term matrix, let $M$ denote the$10000 \times 10000$ word co-occurrence matrix. After SVD, only 100 dimension is kept by keeping the top 100 singular values. Then, a low rank approximation matric is got. But only the truncated $W$ matrix is used as word embedding.<br>The word embedding is used to compute the similarity of words in the data set of this project as in the PMI method. After that, Analogy analysis is also performed in this method.  </p><h3 id="Result-and-Discussion"><a href="#Result-and-Discussion" class="headerlink" title="Result and Discussion"></a>Result and Discussion</h3><h4 id="Result-of-Weighted-Terms-Presentation-PPMI"><a href="#Result-of-Weighted-Terms-Presentation-PPMI" class="headerlink" title="Result of Weighted Terms Presentation: PPMI"></a>Result of Weighted Terms Presentation: PPMI</h4><p>After applying add two smooth to the co-occurrence matrix $M$, PPMI matrix is computed. Then this PMI matrix is used as the word embeddings in this data set.<br>For a given word, the corresponding vector can be retrieved from the PMI matrix by the index of this word. Then, we can compute the cosine similarity between this word and any other words. The increasing of the value of cosine similarity indicates the more relevance of two words.<br>For the similar word experiment, several word as inputs are given, and then the PPMI model is applied to search the 10 most similar words. The result of this experiment is shown as below. </p><table class="table table-bordered"><thead><tr><th style="text-align:center">imput</th><th style="text-align:center">output(top 10 most similar words in PPMI)</th></tr></thead><tbody><tr><td style="text-align:center">&#39;water&#39;</td><td style="text-align:center">&#39;water&#39;, &#39;liquid&#39;, &#39;oxygen&#39;, &#39;gas&#39;, &#39;fuel&#39;, &#39;fluid&#39;, &#39;soil&#39;, &#39;drainage&#39;, &#39;sand&#39;, &#39;waste&#39;</td></tr><tr><td style="text-align:center">&#39;boy&#39;</td><td style="text-align:center">&#39;boy&#39;, &#39;girl&#39;, &#39;kid&#39;, &#39;baby&#39;, &#39;man&#39;, &#39;crazy&#39;, &#39;woman&#39;, &#39;dog&#39;, &#39;cat&#39;, &#39;daddy&#39;</td></tr><tr><td style="text-align:center">&#39;apple&#39;</td><td style="text-align:center">&#39;apple&#39;, &#39;microsoft&#39;, &#39;ibm&#39;, &#39;linux&#39;, &#39;proprietary&#39;, &#39;os&#39;, &#39;compatible&#39;, &#39;processor&#39;, &#39;console&#39;, &#39;sony&#39;</td></tr><tr><td style="text-align:center">&#39;fruit&#39;</td><td style="text-align:center">&#39;fruit&#39;, &#39;fruits&#39;, &#39;flowers&#39;, &#39;vegetables&#39;, &#39;meat&#39;, &#39;corn&#39;, &#39;wheat&#39;, &#39;flower&#39;, &#39;seeds&#39;, &#39;milk&#39;</td></tr><tr><td style="text-align:center">&#39;cat&#39;</td><td style="text-align:center">&#39;cat&#39;, &#39;dog&#39;, &#39;rabbit&#39;, &#39;monkey&#39;, &#39;pig&#39;, &#39;rat&#39;, &#39;cow&#39;, &#39;duck&#39;, &#39;cats&#39;, &#39;frog&#39;</td></tr><tr><td style="text-align:center">&#39;beautiful&#39;</td><td style="text-align:center">&#39;beautiful&#39;, &#39;pretty&#39;, &#39;wonderful&#39;, &#39;attractive&#39;, &#39;lonely&#39;, &#39;nice&#39;, &#39;magnificent&#39;, &#39;sweet&#39;, &#39;funny&#39;, &#39;sad&#39;</td></tr><tr><td style="text-align:center">&#39;good&#39;</td><td style="text-align:center">&#39;good&#39;, &#39;bad&#39;, &#39;excellent&#39;, &#39;better&#39;, &#39;poor&#39;, &#39;nice&#39;, &#39;sure&#39;, &#39;wrong&#39;, &#39;my&#39;, &#39;wonderful&#39;</td></tr><tr><td style="text-align:center">&#39;many&#39;</td><td style="text-align:center">&#39;many&#39;, &#39;several&#39;, &#39;numerous&#39;, &#39;some&#39;, &#39;other&#39;, &#39;various&#39;, &#39;including&#39;, &#39;these&#39;, &#39;such&#39;, &#39;few&#39;</td></tr><tr><td style="text-align:center">&#39;blue&#39;</td><td style="text-align:center">&#39;blue&#39;, &#39;red&#39;, &#39;yellow&#39;, &#39;purple&#39;, &#39;black&#39;, &#39;pink&#39;, &#39;white&#39;, &#39;green&#39;, &#39;orange&#39;, &#39;colored&#39;</td></tr><tr><td style="text-align:center">&#39;play&#39;</td><td style="text-align:center">&#39;play&#39;, &#39;playing&#39;, &#39;plays&#39;, &#39;played&#39;, &#39;compete&#39;, &#39;perform&#39;, &#39;sing&#39;, &#39;participate&#39;, &#39;players&#39;, &#39;game&#39;</td></tr><tr><td style="text-align:center">&#39;try&#39;</td><td style="text-align:center">&#39;try&#39;, &#39;trying&#39;, &#39;tried&#39;, &#39;tries&#39;, &#39;attempting&#39;, &#39;attempt&#39;, &#39;able&#39;, &#39;attempts&#39;, &#39;help&#39;, &#39;attempted&#39;</td></tr><tr><td style="text-align:center">&#39;walk&#39;</td><td style="text-align:center">&#39;walk&#39;, &#39;walking&#39;, &#39;walks&#39;, &#39;walked&#39;, &#39;climb&#39;, &#39;swim&#39;, &#39;ride&#39;, &#39;throw&#39;, &#39;sit&#39;, &#39;wait&#39;</td></tr></tbody></table><p>From the result of similar words above, we can see that the PPMI do a good job in searching similar words for the given inputs. The similar words getting from PPMI are all relevant to the input word. There are some interesting output such as the output the ‘good’, the most similar word except itself is ‘bad’. Although We may think that these two words are opposite but not similar, for the given content these two words are used in most similar surrounding. That is to say they are syntactically similar. Thus, the PPMI will treat them as similar. The word embedding from PPMI matrix capture both the syntactic and semantic meaning of a word.<br>After the similarity experiment, analogy experiment was also perform by using the word embeding from PPMI. An analogy question— “man is to woman as king is to <em>__</em>”, where the goal of this analogy task is to fill in the blank space.<br>This question can be solved by finding a word whose word embedding is most closest to </p><p><script type="math/tex">w_{woman} - w_{man} + w_{king}</script>.<br>The result for this problem using cosine similarity and PPMI is as below.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&apos;man to woman&apos; is similar as &apos;king to prince&apos;</span><br><span class="line">&apos;boy to girl&apos; is similar as &apos;king to queen&apos;</span><br><span class="line">&apos;beijing to china&apos; is similar as &apos;paris to france&apos;</span><br><span class="line">&apos;king to queen&apos; is similar as &apos;ibm to microsoft&apos;</span><br><span class="line">&apos;korea to seoul&apos; is similar as &apos;thailand to bangkok&apos;</span><br></pre></td></tr></table></figure></p><p>It seems that, the PPMI does a good job in the problem except the first analogy. Then, an experiment through a test file which contains more than 5585 analogy samples to text the accurary of analogy is conducted. After a long time computation, the accuracy of PPMI in analogy is as below. It only get $41.6\%$ accuracy in this analogy test. It seems that the accuracy of word analogy by PPMI word embedding is quite low. Besides, the computation complexity of this 10000* 10000 matrix is very high, which makes the process slow dowm.  </p><h4 id="Result-of-Dense-vector-via-SVD"><a href="#Result-of-Dense-vector-via-SVD" class="headerlink" title="Result of Dense vector via SVD"></a>Result of Dense vector via SVD</h4><p>In this part, SVD is applied to decomposite the co-occurrence matrix in order to get a short and dense vector for word embedding. The plot of the singular value in this SVD decomposition is show as below.  </p><table class="table table-bordered"><thead><tr><th>imput</th><th>output(top 10 most similar words in PPMI using SVD)</th></tr></thead><tbody><tr><td>&#39;water&#39;</td><td>&#39;water&#39;, &#39;liquid&#39;, &#39;oxygen&#39;, &#39;gas&#39;, &#39;fuel&#39;, &#39;fluid&#39;, &#39;soil&#39;, &#39;drainage&#39;, &#39;sand&#39;, &#39;waste&#39;</td></tr><tr><td>&#39;boy&#39;</td><td>&#39;boy&#39;, &#39;girl&#39;, &#39;kid&#39;, &#39;baby&#39;, &#39;man&#39;, &#39;crazy&#39;, &#39;woman&#39;, &#39;dog&#39;, &#39;cat&#39;, &#39;daddy&#39;</td></tr><tr><td>&#39;apple&#39;</td><td>&#39;apple&#39;, &#39;microsoft&#39;, &#39;ibm&#39;, &#39;linux&#39;, &#39;proprietary&#39;, &#39;os&#39;, &#39;compatible&#39;, &#39;processor&#39;, &#39;console&#39;, &#39;sony&#39;</td></tr><tr><td>&#39;fruit&#39;</td><td>&#39;fruit&#39;, &#39;fruits&#39;, &#39;flowers&#39;, &#39;vegetables&#39;, &#39;meat&#39;, &#39;corn&#39;, &#39;wheat&#39;, &#39;flower&#39;, &#39;seeds&#39;, &#39;milk&#39;</td></tr><tr><td>&#39;cat&#39;</td><td>&#39;cat&#39;, &#39;dog&#39;, &#39;rabbit&#39;, &#39;monkey&#39;, &#39;pig&#39;, &#39;rat&#39;, &#39;cow&#39;, &#39;duck&#39;, &#39;cats&#39;, &#39;frog&#39;</td></tr><tr><td>&#39;beautiful&#39;</td><td>&#39;beautiful&#39;, &#39;pretty&#39;, &#39;wonderful&#39;, &#39;attractive&#39;, &#39;lonely&#39;, &#39;nice&#39;, &#39;magnificent&#39;, &#39;sweet&#39;, &#39;funny&#39;, &#39;sad&#39;</td></tr><tr><td>&#39;good&#39;</td><td>&#39;good&#39;, &#39;bad&#39;, &#39;excellent&#39;, &#39;better&#39;, &#39;poor&#39;, &#39;nice&#39;, &#39;sure&#39;, &#39;wrong&#39;, &#39;my&#39;, &#39;wonderful&#39;</td></tr><tr><td>&#39;many&#39;</td><td>&#39;many&#39;, &#39;several&#39;, &#39;numerous&#39;, &#39;some&#39;, &#39;other&#39;, &#39;various&#39;, &#39;including&#39;, &#39;these&#39;, &#39;such&#39;, &#39;few&#39;</td></tr><tr><td>&#39;blue&#39;</td><td>&#39;blue&#39;, &#39;red&#39;, &#39;yellow&#39;, &#39;purple&#39;, &#39;black&#39;, &#39;pink&#39;, &#39;white&#39;, &#39;green&#39;, &#39;orange&#39;, &#39;colored&#39;</td></tr><tr><td>&#39;play&#39;</td><td>&#39;play&#39;, &#39;playing&#39;, &#39;plays&#39;, &#39;played&#39;, &#39;compete&#39;, &#39;perform&#39;, &#39;sing&#39;, &#39;participate&#39;, &#39;players&#39;, &#39;game&#39;</td></tr><tr><td>&#39;try&#39;</td><td>&#39;try&#39;, &#39;trying&#39;, &#39;tried&#39;, &#39;tries&#39;, &#39;attempting&#39;, &#39;attempt&#39;, &#39;able&#39;, &#39;attempts&#39;, &#39;help&#39;, &#39;attempted&#39;</td></tr><tr><td>&#39;walk&#39;</td><td>&#39;walk&#39;, &#39;walking&#39;, &#39;walks&#39;, &#39;walked&#39;, &#39;climb&#39;, &#39;swim&#39;, &#39;ride&#39;, &#39;throw&#39;, &#39;sit&#39;, &#39;wait&#39;</td></tr></tbody></table><p>From the result, we can see that co-occurrence matrix with SVD also get a well job in searching the similar words.<br>After the similarity experiment, analogy experiment was also perform by using the word embedding from truncated co-occurrence matrix.<br>The result for this problem using cosine similarity and Dense vector is as below.<br>From the result, we can see that the dense vector perform well in this analogy problem. It gets the right result for the analogy problem given.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&apos;man to woman&apos; is similar as &apos;king to queen&apos;</span><br><span class="line">&apos;boy to girl&apos; is similar as &apos;king to queen&apos;</span><br><span class="line">&apos;beijing to china&apos; is similar as &apos;paris to france&apos;</span><br><span class="line">&apos;king to queen&apos; is similar as &apos;ibm to microsoft&apos;</span><br><span class="line">&apos;korea to seoul&apos; is similar as &apos;thailand to bangkok&apos;</span><br></pre></td></tr></table></figure></p><p>As the PPMI model, an experiment through a test file which contains more than 5585 analogy samples to text the accurary of analogy is conducted. The accuracy using dense vector is $54\%$, which is much better than the sparse vector PPMI method.</p><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>From all the result of the two method, we know that the dense vector method get a better result than the sparse PPMI method in analogy analysis and similar word search. In addition, the computational efficiency of the dense vector is also better than the PPMI. Short vectors may be easier to use as features in machine learning. Dense vectors    may    generalize better than storing explicit counts. In addition, dense vectors may perform better in capturing synonymy than sparse vectors.  </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h3&gt;&lt;p&gt;Word embedding is a mapping fr
      
    
    </summary>
    
    
      <category term="machine learning" scheme="https://wushbin.github.io/tags/machine-learning/"/>
    
      <category term="natural language processing" scheme="https://wushbin.github.io/tags/natural-language-processing/"/>
    
  </entry>
  
  <entry>
    <title>Hidden Markov Model for Part of Speech Tagging</title>
    <link href="https://wushbin.github.io/2017/09/23/Hidden-Markov-Model-for-Part-of-Speech-Tagging/"/>
    <id>https://wushbin.github.io/2017/09/23/Hidden-Markov-Model-for-Part-of-Speech-Tagging/</id>
    <published>2017-09-24T01:50:50.000Z</published>
    <updated>2018-02-13T22:08:17.466Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>In this post, we will train a Hidden Markov Model for part of speech tagging. The first 10K tagged sentence of ‘news’ in the brown corpus will be used to train the Hidden Markov Model. A sentence from ‘alice’ will be infered by this Hidden Markov Model. </p><h4 id="Extract-data"><a href="#Extract-data" class="headerlink" title="Extract data"></a>Extract data</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> nltk.tag.sequential <span class="keyword">import</span> UnigramTagger</span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> brown</span><br><span class="line"><span class="keyword">from</span> nltk.tokenize <span class="keyword">import</span> TreebankWordTokenizer</span><br><span class="line"></span><br><span class="line">alice = nltk.corpus.gutenberg.raw(<span class="string">'carroll-alice.txt'</span>)</span><br><span class="line">tagger = UnigramTagger(brown.tagged_sents(categories=<span class="string">'news'</span>)[:<span class="number">10000</span>])</span><br><span class="line">tokens = TreebankWordTokenizer().tokenize(alice[:<span class="number">1000</span>])</span><br><span class="line">tags = tagger.tag(tokens)</span><br><span class="line">sents = brown.tagged_sents(categories=<span class="string">'news'</span>)[:<span class="number">10000</span>]</span><br><span class="line">words = [w[<span class="number">0</span>] <span class="keyword">for</span> s <span class="keyword">in</span> sents <span class="keyword">for</span> w <span class="keyword">in</span> s]</span><br></pre></td></tr></table></figure><h5 id="Compress-the-tags"><a href="#Compress-the-tags" class="headerlink" title="Compress the tags"></a>Compress the tags</h5><p>The tags provided by brown corpus is to detail, it can be compressed in some extent.<br>Detelte the postfix of the generated tags, as followed. All the dashes and everything behind them are removed and all the asterisks are also removed, while the $ is kept as its original way.  </p><blockquote><p>NN-TL -&gt; NN<br>  BEZ<em> -&gt; BEZ<br>  FW-</em> -&gt; FW<br>  :-HL -&gt; :  </p></blockquote><p>After this process, the tags like “NN-TL” and “NN-JJ” become “NN” without its postfix and “JJ-TL” will become “JJ”.<br>Before compressing the tags, the total number of tags is 218 and after compressing,  the number of tags decrease to 88. Although it will have some effect on the precision of tags but it will improve the accurary of the output prediction.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">sents_temp = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(sents)):</span><br><span class="line">    sents_revised_list = []</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(len(sents[i])):</span><br><span class="line">        content = list(sents[i][j])</span><br><span class="line">        <span class="comment">#content[1]= re.sub("([\$]*[\-\+]+[\w\$\*]+)+$|\$+$|(?&lt;=\w)\*+$", "", content[1]) ## remove the $ version</span></span><br><span class="line">        content[<span class="number">1</span>]= re.sub(<span class="string">"([\-\+]+[\w\$\*]+)+$|(?&lt;=\w)\*+$"</span>, <span class="string">""</span>, content[<span class="number">1</span>]) <span class="comment">## keep the $ version</span></span><br><span class="line">        content[<span class="number">1</span>]= re.sub(<span class="string">"(?&lt;=\w)\*$"</span>, <span class="string">""</span>, content[<span class="number">1</span>])</span><br><span class="line">        content_tuple = tuple(content)</span><br><span class="line">        sents_revised_list.append(content_tuple)</span><br><span class="line">    sents_temp.append(sents_revised_list)</span><br><span class="line">temp = sents</span><br><span class="line">sents = sents_temp</span><br><span class="line">sents_temp = temp</span><br></pre></td></tr></table></figure></p><h4 id="Generate-a-set-of-tags-and-a-set-of-words"><a href="#Generate-a-set-of-tags-and-a-set-of-words" class="headerlink" title="Generate a set of tags and a set of words"></a>Generate a set of tags and a set of words</h4><ul><li>python can maintain a set without duplicated elements. After the sets of tags and words is built, a list of tags and a list of words which contains all the elements in the set are generated. And then dictionarys for tags index and word index are generated.  </li><li>The tags list and words list are used to get a tag or a word via its index. And the tags dictionanry and words dictionary are used to get their index.</li><li>In the tags list and tags set, the tags of start state and end state are added.  </li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">sents_list = [t <span class="keyword">for</span> s <span class="keyword">in</span> sents <span class="keyword">for</span> t <span class="keyword">in</span> s]</span><br><span class="line">tags_set = set()</span><br><span class="line">words_set = set()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(sents_list)):</span><br><span class="line">    tags_set.add(sents_list[i][<span class="number">1</span>])</span><br><span class="line">    words_set.add(sents_list[i][<span class="number">0</span>])</span><br><span class="line">tags_list = list(tags_set)</span><br><span class="line">words_list = list(words_set)</span><br><span class="line"></span><br><span class="line"><span class="comment">## add the start and end state</span></span><br><span class="line">tags_list.insert(<span class="number">0</span>, <span class="string">'START_STATE'</span>)</span><br><span class="line">tags_list.insert(len(tags_list), <span class="string">'END_STATE'</span>)</span><br><span class="line"></span><br><span class="line">tags_hash = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> index, tag <span class="keyword">in</span> enumerate(tags_list):</span><br><span class="line">    tags_hash[tag] = index</span><br><span class="line">words_hash = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> index, word <span class="keyword">in</span> enumerate(words_list):</span><br><span class="line">    words_hash[word] = index</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Tags after compressing:"</span>)</span><br><span class="line">print(tags_hash)</span><br></pre></td></tr></table></figure><blockquote><p>Tags after compressing:<br>{‘START_STATE’: 0, ‘NPS’: 1, ‘WPO’: 2, ‘PPLS’: 3, ‘BEN’: 4, ‘NNS’: 5, ‘BER’: 6, ‘VBN’: 7, ‘CS’: 8, ‘TO’: 9, ‘DTX’: 10, ‘)’: 11, ‘,’: 12, ‘NP’: 13, ‘VBD’: 14, ‘FW’: 15, ‘PP$$’: 16, ‘JJR’: 17, ‘WPS’: 18, ‘OD’: 19, ‘QLP’: 20, ‘AT’: 21, ‘EX’: 22, ‘CC’: 23, ‘HVD’: 24, ‘PPS’: 25, ‘PP$’: 26, ‘NR$’: 27, ‘``’: 28, ‘QL’: 29, ‘JJS’: 30, ‘BE’: 31, ‘HVN’: 32, ‘PN$’: 33, ‘ABN’: 34, ‘NN$’: 35, ‘AP’: 36, ‘NNS$’: 37, ‘RB$’: 38, ‘VBZ’: 39, ‘NR’: 40, ‘PPL’: 41, ‘NP$’: 42, ‘DT$’: 43, ‘BEDZ’: 44, ‘RBT’: 45, ‘MD’: 46, ‘DT’: 47, ‘NN’: 48, ‘ABL’: 49, ‘BEM’: 50, ‘BED’: 51, ‘AP$’: 52, ‘HV’: 53, ‘(‘: 54, ‘WDT’: 55, ‘DTS’: 56, ‘RP’: 57, ‘VBG’: 58, ‘HVG’: 59, ‘NPS$’: 60, ‘BEZ’: 61, ‘JJT’: 62, ‘.’: 63, “‘’”: 64, ‘DOZ’: 65, ‘ABX’: 66, ‘CD’: 67, ‘DOD’: 68, ‘RB’: 69, ‘DO’: 70, ‘BEG’: 71, ‘RBR’: 72, “‘“: 73, ‘—‘: 74, ‘PPSS’: 75, ‘WQL’: 76, ‘*’: 77, ‘IN’: 78, ‘VB’: 79, ‘PPO’: 80, ‘HVZ’: 81, ‘WP$’: 82, ‘WRB’: 83, ‘PN’: 84, ‘CD$’: 85, ‘JJ’: 86, ‘UH’: 87, ‘DTI’: 88, ‘:’: 89, ‘END_STATE’: 90}</p></blockquote><h3 id="Forward-Algorithm"><a href="#Forward-Algorithm" class="headerlink" title="Forward Algorithm"></a>Forward Algorithm</h3><h4 id="Transition-Probability-Matrix-A"><a href="#Transition-Probability-Matrix-A" class="headerlink" title="Transition Probability Matrix: A"></a>Transition Probability Matrix: A</h4><ul><li>The row length and column length both are the same as the length of tags</li><li>In order to reduce the effect of the zero values in this matrix, all elements in matrix A was added by a small value 1e-10 and then re-normalized.  </li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">A_len = len(tags_hash)</span><br><span class="line">A = np.zeros((A_len, A_len))</span><br><span class="line"></span><br><span class="line">start_state = tags_hash[<span class="string">'START_STATE'</span>]</span><br><span class="line">end_state = tags_hash[<span class="string">'END_STATE'</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(sents)):</span><br><span class="line">    </span><br><span class="line">    start_tag_index = tags_hash[sents[i][<span class="number">0</span>][<span class="number">1</span>]]</span><br><span class="line">    A[start_state][start_tag_index] += <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(len(sents[i]) - <span class="number">1</span>):</span><br><span class="line">        row_index = tags_hash[sents[i][j][<span class="number">1</span>]]</span><br><span class="line">        col_index = tags_hash[sents[i][j + <span class="number">1</span>][<span class="number">1</span>]]</span><br><span class="line">        A[row_index][col_index] += <span class="number">1</span></span><br><span class="line">    end_tag_index = tags_hash[sents[i][len(sents[i]) - <span class="number">1</span>][<span class="number">1</span>]]</span><br><span class="line">    A[end_tag_index][end_state] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">A[A_len - <span class="number">1</span>] += <span class="number">1e-10</span></span><br><span class="line">A_row_sum = np.sum(A, axis = <span class="number">1</span>)</span><br><span class="line">A_row_sum = A_row_sum.reshape(A.shape[<span class="number">0</span>],<span class="number">1</span>)</span><br><span class="line">A_normalized = A / A_row_sum</span><br><span class="line"><span class="comment">## re-normalized</span></span><br><span class="line">A_normalized += <span class="number">1e-10</span></span><br><span class="line">A_row_sum = np.sum(A_normalized, axis = <span class="number">1</span>)</span><br><span class="line">A_row_sum = A_row_sum.reshape(A_normalized.shape[<span class="number">0</span>],<span class="number">1</span>)</span><br><span class="line">A_normalized = A_normalized / A_row_sum</span><br></pre></td></tr></table></figure><h4 id="State-Observation-Likelihood-Matrix-B"><a href="#State-Observation-Likelihood-Matrix-B" class="headerlink" title="State Observation Likelihood Matrix: B"></a>State Observation Likelihood Matrix: B</h4><ul><li>The row of matrix B is the tags generated, the column of matrix B is the words in our trainnign set.  </li><li>In order to reduce the effect of the zero values in this matrix, all elements in matrix B was added by a small value 1e-10 and then re-normalized.  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">B_row = len(tags_hash)</span><br><span class="line">B_col = len(words_hash)</span><br><span class="line">B = np.zeros((B_row, B_col))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(sents_list)):</span><br><span class="line">    row_index = tags_hash[sents_list[i][<span class="number">1</span>]]</span><br><span class="line">    col_index = words_hash[sents_list[i][<span class="number">0</span>]]</span><br><span class="line">    B[row_index][col_index] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">B_col_sum = np.sum(B, axis = <span class="number">0</span>)</span><br><span class="line">B_col_sum = B_col_sum.reshape(<span class="number">1</span>, B_col)</span><br><span class="line">B_normalized = B / B_col_sum</span><br><span class="line"></span><br><span class="line"><span class="comment">## re-normalized</span></span><br><span class="line">B_normalized += <span class="number">1e-10</span></span><br><span class="line">B_col_sum = np.sum(B_normalized, axis = <span class="number">0</span>)</span><br><span class="line">B_col_sum = B_col_sum.reshape(<span class="number">1</span>, B_col)</span><br><span class="line">B_normalized = B_normalized / B_col_sum</span><br></pre></td></tr></table></figure></li></ul><h4 id="Get-a-test-case"><a href="#Get-a-test-case" class="headerlink" title="Get a test case"></a>Get a test case</h4><p>In this post, we use a sentence from ‘alice’<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.tokenize <span class="keyword">import</span> word_tokenize</span><br><span class="line">test_case = nltk.corpus.gutenberg.raw(<span class="string">'carroll-alice.txt'</span>)[<span class="number">91</span>:<span class="number">683</span>]</span><br><span class="line">words_test = word_tokenize(test_case)</span><br></pre></td></tr></table></figure></p><h4 id="Forward-Algorithm-1"><a href="#Forward-Algorithm-1" class="headerlink" title="Forward Algorithm"></a>Forward Algorithm</h4><p>The cell below is the implementation of forward algorithm. The matrix ‘forward’ is the porbablity of states in the test sequence.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">num_state = len(tags_hash)</span><br><span class="line">num_obv = len(words_test)</span><br><span class="line">forward = np.zeros((num_obv, num_state))</span><br><span class="line">start_state = tags_hash[<span class="string">'START_STATE'</span>]</span><br><span class="line">end_state = tags_hash[<span class="string">'END_STATE'</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    word_index = words_hash[words_test[<span class="number">0</span>]]</span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> range(num_state):</span><br><span class="line">        forward[<span class="number">0</span>][y] = A_normalized[start_state][y] * B_normalized[y][word_index]</span><br><span class="line">        </span><br><span class="line"><span class="keyword">except</span> KeyError:</span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> range(num_state):</span><br><span class="line">        forward[<span class="number">0</span>][y] = A_normalized[start_state][y] * (<span class="number">1</span>/num_state)</span><br><span class="line">        </span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">1</span>, num_obv):</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        word_index = words_hash[words_test[t]]</span><br><span class="line">        <span class="keyword">for</span> y <span class="keyword">in</span> range(num_state):</span><br><span class="line">            <span class="keyword">for</span> y_pre <span class="keyword">in</span> range(num_state):</span><br><span class="line">                forward[t][y] += forward[t<span class="number">-1</span>][y_pre] * A_normalized[y_pre][y] * B_normalized[y][word_index]</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">except</span> KeyError:</span><br><span class="line">        <span class="keyword">for</span> y <span class="keyword">in</span> range(num_state):</span><br><span class="line">            <span class="keyword">for</span> y_pre <span class="keyword">in</span> range(num_state):</span><br><span class="line">                forward[t][y] += forward[t<span class="number">-1</span>][y_pre] * A_normalized[y_pre][y] * (<span class="number">1</span>/num_state)</span><br></pre></td></tr></table></figure></p><h4 id="Decoding-of-Forward-Algorithm"><a href="#Decoding-of-Forward-Algorithm" class="headerlink" title="Decoding of Forward Algorithm"></a>Decoding of Forward Algorithm</h4><p>For a given forward matrix, the state in a step can be predicted by finding a state with largest probability in this step.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">forward_output = []</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(num_obv):</span><br><span class="line">    mx = <span class="number">-1</span></span><br><span class="line">    tag_id = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> index, tags <span class="keyword">in</span> enumerate(forward[t]):</span><br><span class="line">        <span class="keyword">if</span> mx &lt; forward[t][index]:</span><br><span class="line">            mx = forward[t][index]</span><br><span class="line">            tag_id = index</span><br><span class="line">    forward_output.append((words_test[t], tags_list[tag_id]))</span><br></pre></td></tr></table></figure></p><h4 id="The-result-of-forward-algorithm-is-shown-as-belowed"><a href="#The-result-of-forward-algorithm-is-shown-as-belowed" class="headerlink" title="The result of forward algorithm is shown as belowed."></a>The result of forward algorithm is shown as belowed.</h4><p>Because the tags are compressed, all the tags in the results are without profix.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> (<span class="string">"The result of test case from forward algorithms is as belowed"</span>)</span><br><span class="line">print(forward_output)</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">The result of test case from forward algorithms is as belowed</span><br><span class="line">[(&apos;Alice&apos;, &apos;NP&apos;), (&apos;was&apos;, &apos;BEDZ&apos;), (&apos;beginning&apos;, &apos;NN&apos;), (&apos;to&apos;, &apos;IN&apos;), (&apos;get&apos;, &apos;VB&apos;), (&apos;very&apos;, &apos;QL&apos;), (&apos;tired&apos;, &apos;VBN&apos;), (&apos;of&apos;, &apos;IN&apos;), (&apos;sitting&apos;, &apos;VBG&apos;), (&apos;by&apos;, &apos;IN&apos;), (&apos;her&apos;, &apos;PP$&apos;), (&apos;sister&apos;, &apos;NN&apos;), (&apos;on&apos;, &apos;IN&apos;), (&apos;the&apos;, &apos;AT&apos;), (&apos;bank&apos;, &apos;NN&apos;), (&apos;,&apos;, &apos;,&apos;), (&apos;and&apos;, &apos;CC&apos;), (&apos;of&apos;, &apos;IN&apos;), (&apos;having&apos;, &apos;HVG&apos;), (&apos;nothing&apos;, &apos;PN&apos;), (&apos;to&apos;, &apos;IN&apos;), (&apos;do&apos;, &apos;DO&apos;), (&apos;:&apos;, &apos;:&apos;), (&apos;once&apos;, &apos;RB&apos;), (&apos;or&apos;, &apos;CC&apos;), (&apos;twice&apos;, &apos;RB&apos;), (&apos;she&apos;, &apos;PPS&apos;), (&apos;had&apos;, &apos;HVD&apos;), (&apos;peeped&apos;, &apos;VBN&apos;), (&apos;into&apos;, &apos;IN&apos;), (&apos;the&apos;, &apos;AT&apos;), (&apos;book&apos;, &apos;NN&apos;), (&apos;her&apos;, &apos;PP$&apos;), (&apos;sister&apos;, &apos;NN&apos;), (&apos;was&apos;, &apos;BEDZ&apos;), (&apos;reading&apos;, &apos;VBG&apos;), (&apos;,&apos;, &apos;,&apos;), (&apos;but&apos;, &apos;CC&apos;), (&apos;it&apos;, &apos;PPS&apos;), (&apos;had&apos;, &apos;HVD&apos;), (&apos;no&apos;, &apos;AT&apos;), (&apos;pictures&apos;, &apos;NNS&apos;), (&apos;or&apos;, &apos;CC&apos;), (&apos;conversations&apos;, &apos;NNS&apos;), (&apos;in&apos;, &apos;IN&apos;), (&apos;it&apos;, &apos;PPO&apos;), (&apos;,&apos;, &apos;,&apos;), (&quot;&apos;and&quot;, &apos;NP&apos;), (&apos;what&apos;, &apos;WDT&apos;), (&apos;is&apos;, &apos;BEZ&apos;), (&apos;the&apos;, &apos;AT&apos;), (&apos;use&apos;, &apos;NN&apos;), (&apos;of&apos;, &apos;IN&apos;), (&apos;a&apos;, &apos;AT&apos;), (&apos;book&apos;, &apos;NN&apos;), (&apos;,&apos;, &apos;,&apos;), (&quot;&apos;&quot;, &quot;&apos;&quot;), (&apos;thought&apos;, &apos;VBN&apos;), (&apos;Alice&apos;, &apos;NP&apos;), (&quot;&apos;without&quot;, &apos;NP&apos;), (&apos;pictures&apos;, &apos;NNS&apos;), (&apos;or&apos;, &apos;CC&apos;), (&apos;conversation&apos;, &apos;NN&apos;), (&apos;?&apos;, &apos;.&apos;), (&quot;&apos;&quot;, &quot;&apos;&quot;), (&apos;So&apos;, &apos;RB&apos;), (&apos;she&apos;, &apos;PPS&apos;), (&apos;was&apos;, &apos;BEDZ&apos;), (&apos;considering&apos;, &apos;VBG&apos;), (&apos;in&apos;, &apos;IN&apos;), (&apos;her&apos;, &apos;PP$&apos;), (&apos;own&apos;, &apos;JJ&apos;), (&apos;mind&apos;, &apos;NN&apos;), (&apos;(&apos;, &apos;(&apos;), (&apos;as&apos;, &apos;CS&apos;), (&apos;well&apos;, &apos;RB&apos;), (&apos;as&apos;, &apos;CS&apos;), (&apos;she&apos;, &apos;PPS&apos;), (&apos;could&apos;, &apos;MD&apos;), (&apos;,&apos;, &apos;,&apos;), (&apos;for&apos;, &apos;IN&apos;), (&apos;the&apos;, &apos;AT&apos;), (&apos;hot&apos;, &apos;JJ&apos;), (&apos;day&apos;, &apos;NN&apos;), (&apos;made&apos;, &apos;VBD&apos;), (&apos;her&apos;, &apos;PP$&apos;), (&apos;feel&apos;, &apos;NN&apos;), (&apos;very&apos;, &apos;QL&apos;), (&apos;sleepy&apos;, &apos;JJ&apos;), (&apos;and&apos;, &apos;CC&apos;), (&apos;stupid&apos;, &apos;NP&apos;), (&apos;)&apos;, &apos;)&apos;), (&apos;,&apos;, &apos;,&apos;), (&apos;whether&apos;, &apos;CS&apos;), (&apos;the&apos;, &apos;AT&apos;), (&apos;pleasure&apos;, &apos;NN&apos;), (&apos;of&apos;, &apos;IN&apos;), (&apos;making&apos;, &apos;VBG&apos;), (&apos;a&apos;, &apos;AT&apos;), (&apos;daisy-chain&apos;, &apos;NN&apos;), (&apos;would&apos;, &apos;MD&apos;), (&apos;be&apos;, &apos;BE&apos;), (&apos;worth&apos;, &apos;JJ&apos;), (&apos;the&apos;, &apos;AT&apos;), (&apos;trouble&apos;, &apos;NN&apos;), (&apos;of&apos;, &apos;IN&apos;), (&apos;getting&apos;, &apos;VBG&apos;), (&apos;up&apos;, &apos;RP&apos;), (&apos;and&apos;, &apos;CC&apos;), (&apos;picking&apos;, &apos;VBG&apos;), (&apos;the&apos;, &apos;AT&apos;), (&apos;daisies&apos;, &apos;NN&apos;), (&apos;,&apos;, &apos;,&apos;), (&apos;when&apos;, &apos;WRB&apos;), (&apos;suddenly&apos;, &apos;RB&apos;), (&apos;a&apos;, &apos;AT&apos;), (&apos;White&apos;, &apos;JJ&apos;), (&apos;Rabbit&apos;, &apos;NN&apos;), (&apos;with&apos;, &apos;IN&apos;), (&apos;pink&apos;, &apos;JJ&apos;), (&apos;eyes&apos;, &apos;NNS&apos;), (&apos;ran&apos;, &apos;VBD&apos;), (&apos;close&apos;, &apos;RB&apos;), (&apos;by&apos;, &apos;IN&apos;), (&apos;her&apos;, &apos;PP$&apos;), (&apos;.&apos;, &apos;.&apos;)]</span><br></pre></td></tr></table></figure><h3 id="Viterbi-Algorithm"><a href="#Viterbi-Algorithm" class="headerlink" title="Viterbi Algorithm"></a>Viterbi Algorithm</h3><ul><li>The implementation of viterbi algorithm is given as below.  </li><li>The Node class is a class with previous node and next node which can be used as a back pointer in viterbi algorithms.  </li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Node</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, state, observ, val, pre_node, next_node)</span>:</span></span><br><span class="line">        self.state = state</span><br><span class="line">        self.observ = observ</span><br><span class="line">        self.val = val</span><br><span class="line">        self.pre_node = pre_node</span><br><span class="line">        self.next_node = next_node</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.state + <span class="string">"--&gt;"</span> + self.observ + <span class="string">"--&gt;"</span> + str(self.val) + <span class="string">'\n'</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__lt__</span><span class="params">(self, other)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> (self.val &lt; other.val)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line">num_state = len(tags_hash)</span><br><span class="line">num_obv = len(words_test)</span><br><span class="line"><span class="comment">#viterbi = [[None for i in range(num_state)] for j in range(num_obv)]</span></span><br><span class="line">viterbi = []</span><br><span class="line">path = []</span><br><span class="line"></span><br><span class="line">start_state = tags_hash[<span class="string">'START_STATE'</span>]</span><br><span class="line">end_state = tags_hash[<span class="string">'END_STATE'</span>]</span><br><span class="line"></span><br><span class="line">start_node = Node(<span class="string">"START_STATE"</span>, <span class="string">""</span>, <span class="number">0</span>, <span class="keyword">None</span>, <span class="keyword">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    word_index = words_hash[words_test[<span class="number">0</span>]]</span><br><span class="line">    curr_step = []</span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> range(<span class="number">1</span>, num_state<span class="number">-1</span>):</span><br><span class="line">        val = A_normalized[start_state][y] * B_normalized[y][word_index]</span><br><span class="line">        curr_node = Node(tags_list[y], words_test[<span class="number">0</span>], val, start_node, <span class="keyword">None</span>)</span><br><span class="line">        <span class="comment">#print(curr_node)</span></span><br><span class="line">        curr_step.append(curr_node)</span><br><span class="line">    viterbi.append(curr_step)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">except</span> KeyError:</span><br><span class="line">    curr_step = []</span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> range(<span class="number">1</span>, num_state<span class="number">-1</span>):</span><br><span class="line">        val = A_normalized[start_state][y]  * (<span class="number">1</span>/(num_state<span class="number">-2</span>))</span><br><span class="line">        curr_node = Node(tags_list[y], words_test[<span class="number">0</span>], val, start_node, <span class="keyword">None</span>)</span><br><span class="line">        curr_step.append(curr_node)</span><br><span class="line">    viterbi.append(curr_step)</span><br><span class="line"></span><br><span class="line"><span class="comment">## </span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">1</span>, num_obv):</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        word_index = words_hash[words_test[t]]</span><br><span class="line">        curr_step = []</span><br><span class="line">        <span class="keyword">for</span> y <span class="keyword">in</span> range(<span class="number">1</span>,num_state<span class="number">-1</span>):</span><br><span class="line">            val = <span class="number">-1</span></span><br><span class="line">            prenode = <span class="keyword">None</span></span><br><span class="line">            <span class="keyword">for</span> y_pre <span class="keyword">in</span> range(<span class="number">1</span>,num_state<span class="number">-1</span>):</span><br><span class="line">                temp_val = viterbi[t<span class="number">-1</span>][y_pre<span class="number">-1</span>].val * A_normalized[y_pre][y] * B_normalized[y][word_index]</span><br><span class="line">                <span class="keyword">if</span> val &lt; temp_val :</span><br><span class="line">                    val = temp_val</span><br><span class="line">                    prenode = viterbi[t<span class="number">-1</span>][y_pre<span class="number">-1</span>]</span><br><span class="line">            curr_node = Node(tags_list[y], words_test[t], val, prenode, <span class="keyword">None</span>)</span><br><span class="line">            curr_step.append(curr_node)</span><br><span class="line">        viterbi.append(curr_step)</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">except</span> KeyError:</span><br><span class="line">        curr_step = []</span><br><span class="line">        <span class="keyword">for</span> y <span class="keyword">in</span> range(<span class="number">1</span>, num_state<span class="number">-1</span>):</span><br><span class="line">            val = <span class="number">-1</span></span><br><span class="line">            prenode = <span class="keyword">None</span></span><br><span class="line">            <span class="keyword">for</span> y_pre <span class="keyword">in</span> range(<span class="number">1</span>, num_state<span class="number">-1</span>):</span><br><span class="line">                temp_val = viterbi[t<span class="number">-1</span>][y_pre<span class="number">-1</span>].val * A_normalized[y_pre][y] * (<span class="number">1</span>/(num_state<span class="number">-2</span>))</span><br><span class="line">                <span class="keyword">if</span> val &lt; temp_val :</span><br><span class="line">                    val = temp_val</span><br><span class="line">                    prenode = viterbi[t<span class="number">-1</span>][y_pre<span class="number">-1</span>]</span><br><span class="line">            curr_node = Node(tags_list[y], words_test[t], val, prenode, <span class="keyword">None</span>)</span><br><span class="line">            curr_step.append(curr_node)</span><br><span class="line">        viterbi.append(curr_step)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span>(len(viterbi) == num_obv)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">val = <span class="number">-1</span></span><br><span class="line">prenode = <span class="keyword">None</span></span><br><span class="line"><span class="keyword">for</span> y_pre <span class="keyword">in</span> range(<span class="number">1</span>, num_state<span class="number">-1</span>):</span><br><span class="line">    temp_val = viterbi[num_obv<span class="number">-1</span>][y_pre<span class="number">-1</span>].val * A_normalized[y_pre][end_state]</span><br><span class="line">    <span class="keyword">if</span> val &lt; temp_val:</span><br><span class="line">        val = temp_val</span><br><span class="line">        prenode = viterbi[num_obv<span class="number">-1</span>][y_pre<span class="number">-1</span>]</span><br><span class="line">end_node = Node(<span class="string">"END_STATE"</span>, <span class="string">""</span>, val, prenode, <span class="keyword">None</span>)</span><br></pre></td></tr></table></figure><h4 id="The-result-of-viterbi-algorithm-is-shown-as-belowed"><a href="#The-result-of-viterbi-algorithm-is-shown-as-belowed" class="headerlink" title="The result of viterbi algorithm is shown as belowed"></a>The result of viterbi algorithm is shown as belowed</h4><p>Because the tags are compressed, all the tags in the results are without profix.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">node = end_node</span><br><span class="line">outlist = []</span><br><span class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">    <span class="keyword">if</span> (node <span class="keyword">is</span> <span class="keyword">None</span>):</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    outlist.append((node.observ, node.state))</span><br><span class="line">    node = node.pre_node</span><br><span class="line"></span><br><span class="line">outlist.reverse()</span><br><span class="line">print(<span class="string">"The result of test case from Viterbi Algorithms is as belowed"</span>)</span><br><span class="line">print(outlist[<span class="number">1</span>:len(outlist)<span class="number">-1</span>])</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">The result of test case from forward algorithms is as belowed</span><br><span class="line">[(&apos;Alice&apos;, &apos;NP&apos;), (&apos;was&apos;, &apos;BEDZ&apos;), (&apos;beginning&apos;, &apos;NN&apos;), (&apos;to&apos;, &apos;TO&apos;), (&apos;get&apos;, &apos;VB&apos;), (&apos;very&apos;, &apos;QL&apos;), (&apos;tired&apos;, &apos;VBN&apos;), (&apos;of&apos;, &apos;IN&apos;), (&apos;sitting&apos;, &apos;VBG&apos;), (&apos;by&apos;, &apos;IN&apos;), (&apos;her&apos;, &apos;PP$&apos;), (&apos;sister&apos;, &apos;NN&apos;), (&apos;on&apos;, &apos;IN&apos;), (&apos;the&apos;, &apos;AT&apos;), (&apos;bank&apos;, &apos;NN&apos;), (&apos;,&apos;, &apos;,&apos;), (&apos;and&apos;, &apos;CC&apos;), (&apos;of&apos;, &apos;IN&apos;), (&apos;having&apos;, &apos;HVG&apos;), (&apos;nothing&apos;, &apos;PN&apos;), (&apos;to&apos;, &apos;TO&apos;), (&apos;do&apos;, &apos;DO&apos;), (&apos;:&apos;, &apos;*&apos;), (&apos;once&apos;, &apos;RB&apos;), (&apos;or&apos;, &apos;CC&apos;), (&apos;twice&apos;, &apos;RB&apos;), (&apos;she&apos;, &apos;PPS&apos;), (&apos;had&apos;, &apos;HVD&apos;), (&apos;peeped&apos;, &apos;VBN&apos;), (&apos;into&apos;, &apos;IN&apos;), (&apos;the&apos;, &apos;AT&apos;), (&apos;book&apos;, &apos;NN&apos;), (&apos;her&apos;, &apos;PP$&apos;), (&apos;sister&apos;, &apos;NN&apos;), (&apos;was&apos;, &apos;BEDZ&apos;), (&apos;reading&apos;, &apos;NN&apos;), (&apos;,&apos;, &apos;,&apos;), (&apos;but&apos;, &apos;CC&apos;), (&apos;it&apos;, &apos;PPS&apos;), (&apos;had&apos;, &apos;HVD&apos;), (&apos;no&apos;, &apos;AT&apos;), (&apos;pictures&apos;, &apos;NNS&apos;), (&apos;or&apos;, &apos;CC&apos;), (&apos;conversations&apos;, &apos;NNS&apos;), (&apos;in&apos;, &apos;IN&apos;), (&apos;it&apos;, &apos;PPO&apos;), (&apos;,&apos;, &apos;,&apos;), (&quot;&apos;and&quot;, &apos;IN&apos;), (&apos;what&apos;, &apos;WDT&apos;), (&apos;is&apos;, &apos;BEZ&apos;), (&apos;the&apos;, &apos;AT&apos;), (&apos;use&apos;, &apos;NN&apos;), (&apos;of&apos;, &apos;IN&apos;), (&apos;a&apos;, &apos;AT&apos;), (&apos;book&apos;, &apos;NN&apos;), (&apos;,&apos;, &apos;,&apos;), (&quot;&apos;&quot;, &quot;&apos;&quot;), (&apos;thought&apos;, &apos;VBN&apos;), (&apos;Alice&apos;, &apos;NP&apos;), (&quot;&apos;without&quot;, &apos;CD&apos;), (&apos;pictures&apos;, &apos;NNS&apos;), (&apos;or&apos;, &apos;CC&apos;), (&apos;conversation&apos;, &apos;NN&apos;), (&apos;?&apos;, &apos;.&apos;), (&quot;&apos;&quot;, &quot;&apos;&quot;), (&apos;So&apos;, &apos;RB&apos;), (&apos;she&apos;, &apos;PPS&apos;), (&apos;was&apos;, &apos;BEDZ&apos;), (&apos;considering&apos;, &apos;VBG&apos;), (&apos;in&apos;, &apos;IN&apos;), (&apos;her&apos;, &apos;PP$&apos;), (&apos;own&apos;, &apos;JJ&apos;), (&apos;mind&apos;, &apos;NN&apos;), (&apos;(&apos;, &apos;(&apos;), (&apos;as&apos;, &apos;CS&apos;), (&apos;well&apos;, &apos;RB&apos;), (&apos;as&apos;, &apos;CS&apos;), (&apos;she&apos;, &apos;PPS&apos;), (&apos;could&apos;, &apos;MD&apos;), (&apos;,&apos;, &apos;,&apos;), (&apos;for&apos;, &apos;IN&apos;), (&apos;the&apos;, &apos;AT&apos;), (&apos;hot&apos;, &apos;JJ&apos;), (&apos;day&apos;, &apos;NN&apos;), (&apos;made&apos;, &apos;VBD&apos;), (&apos;her&apos;, &apos;PP$&apos;), (&apos;feel&apos;, &apos;NN&apos;), (&apos;very&apos;, &apos;QL&apos;), (&apos;sleepy&apos;, &apos;JJ&apos;), (&apos;and&apos;, &apos;CC&apos;), (&apos;stupid&apos;, &apos;NP&apos;), (&apos;)&apos;, &apos;)&apos;), (&apos;,&apos;, &apos;,&apos;), (&apos;whether&apos;, &apos;CS&apos;), (&apos;the&apos;, &apos;AT&apos;), (&apos;pleasure&apos;, &apos;NN&apos;), (&apos;of&apos;, &apos;IN&apos;), (&apos;making&apos;, &apos;VBG&apos;), (&apos;a&apos;, &apos;AT&apos;), (&apos;daisy-chain&apos;, &apos;NN&apos;), (&apos;would&apos;, &apos;MD&apos;), (&apos;be&apos;, &apos;BE&apos;), (&apos;worth&apos;, &apos;JJ&apos;), (&apos;the&apos;, &apos;AT&apos;), (&apos;trouble&apos;, &apos;NN&apos;), (&apos;of&apos;, &apos;IN&apos;), (&apos;getting&apos;, &apos;VBG&apos;), (&apos;up&apos;, &apos;RP&apos;), (&apos;and&apos;, &apos;CC&apos;), (&apos;picking&apos;, &apos;VBG&apos;), (&apos;the&apos;, &apos;AT&apos;), (&apos;daisies&apos;, &apos;NN&apos;), (&apos;,&apos;, &apos;,&apos;), (&apos;when&apos;, &apos;WRB&apos;), (&apos;suddenly&apos;, &apos;RB&apos;), (&apos;a&apos;, &apos;AT&apos;), (&apos;White&apos;, &apos;JJ&apos;), (&apos;Rabbit&apos;, &apos;NN&apos;), (&apos;with&apos;, &apos;IN&apos;), (&apos;pink&apos;, &apos;JJ&apos;), (&apos;eyes&apos;, &apos;NNS&apos;), (&apos;ran&apos;, &apos;VBD&apos;), (&apos;close&apos;, &apos;RB&apos;), (&apos;by&apos;, &apos;IN&apos;), (&apos;her&apos;, &apos;PPO&apos;), (&apos;.&apos;, &apos;.&apos;)]</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h3&gt;&lt;p&gt;In this post, we will train a 
      
    
    </summary>
    
    
      <category term="machine learning" scheme="https://wushbin.github.io/tags/machine-learning/"/>
    
      <category term="natural language processing" scheme="https://wushbin.github.io/tags/natural-language-processing/"/>
    
  </entry>
  
  <entry>
    <title>cat-and-dog</title>
    <link href="https://wushbin.github.io/2017/05/02/cat-and-dog/"/>
    <id>https://wushbin.github.io/2017/05/02/cat-and-dog/</id>
    <published>2017-05-03T00:42:35.000Z</published>
    <updated>2017-12-23T13:49:09.193Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>In the note, a classifier of SVM(Support Vector Machine) and a classifier of CNN(Convolutional Neural Network) using transfer learning will be explored to experiment the accuracy of automatically classifying the image set of cats and dogs.  </p><h4 id="Method-One"><a href="#Method-One" class="headerlink" title="Method One"></a>Method One</h4><p>For the first method, an SVM classifier was trained to classify the images by families(family dog or family cat). SIFT will be applied to extracted the features from each image in the training set. Then the bag of word model will be applied to create a dictionary for these extracted features. This dictionary is formed by a clustering algorithm K-means. One cluster of features is viewed as a visual word in this dictionary. After the dictionary is created, images are represented by frequency vectors which represent the proportion of features belong to a visual word. Then, a SVM classifier is trained based on these frequency features.  </p><h4 id="Method-Two"><a href="#Method-Two" class="headerlink" title="Method Two"></a>Method Two</h4><p>For the second method, we used CNN model to extracted features from the images. For this project, keras application are used. These applications are deep learning models with pre-trained weights. They can be used for prediction, feature extraction and fine tuning. The ResNet50, InceptionV3 and Xception model are used for extracting features from the images. Then these features will be used to trained a CNN model.  </p><h4 id="Data-Set"><a href="#Data-Set" class="headerlink" title="Data Set"></a>Data Set</h4><p>The data set for this project is the provided by Microsoft Research and Kaggle. The training set consists of 25,000 images with half cat images and half dog images. The training set contains 12,500 images without labels. The size of these images are about $350\times 350$.<br>After downloading the image set from Kaggle, the images are separated into two folder, one for cat images, one for dog images. For dogs, the corresponding label is 1, for cats, the corresponding label is 0. The sample images in the data set are shown in the <strong><em>Figure 1</em></strong>.<br><img src="/images/catAndDog.png" alt="data set images">“Figure 1: images in data set”</p><h3 id="Process"><a href="#Process" class="headerlink" title="Process"></a>Process</h3><h4 id="Method-One-Traditional-Machine-Learning-Approach"><a href="#Method-One-Traditional-Machine-Learning-Approach" class="headerlink" title="Method One: Traditional Machine Learning Approach"></a>Method One: Traditional Machine Learning Approach</h4><p>For traditional image classification problems, features extracted by human are chosen to train classifiers. Then feature descriptors are use to represent the images. SIFT, HoG, RGB and HSV are the common features that are used to represent images.<br>For this project, SIFT are used to extracted the features and compute the feature descriptors. Because we know that the shapes of images of cat and dog are different with each other. Features extracted by SIFT will play an important role in the classification of the image set.<br>After implementing the SIFT algorithms to extract features, we get the features descriptors of all the images in the training set. K-means clustering algorithms is applied to generate a dictionary of visual words for the features descriptors in the training set. All the images are then represented by by frequency vectors which represent the proportion of features belong to a visual word.<br>Based on the frequency vectors generated by the BOW method, a SVM classifier was trained to make classification. The visualize process of this method is shown in the <strong><em>Figure 2</em></strong>.<br><img src="/images/SIFT_proc.jpeg" alt="method one">“Figure 2: Process of Method One”</p><p><a href="https://github.com/wushbin/DogAndCat/blob/master/catVsDog.ipynb" target="_blank" rel="noopener">Source code for this method</a><br>The accuracy of this method is about $62\%$ when the images were compressed to $128 \times 128$, which is quite dissatisfactory for a binary classification problem. While when the images were compressed to $256 \times 256$, the accuracy only increase to $65.46\%$.<br><img src="/images/sift_result.png" alt="method one result">“Result using Method One”</p><h4 id="Method-Two-Deep-Neural-Netword-CNN-with-transfer-learning"><a href="#Method-Two-Deep-Neural-Netword-CNN-with-transfer-learning" class="headerlink" title="Method Two: Deep Neural Netword(CNN with transfer learning)"></a>Method Two: Deep Neural Netword(CNN with transfer learning)</h4><p><em>Method two is refer to Peiwen Yang’s Post in Zhihu</em>.  </p><p>In Method Two, image features will be extracted by a Convolutional Neural Network model. After that, we can simply use dropout to classify the validation set and test set. Compared with the feature extraction by SIFT, convolutional neural network learn features from the images.<br>Several pre-trained model in keras application are used in this method to learn the features from the cat and dog image set. The ResNet50, InceptionV3 and Xception models are chosen to learn the features,which are object detection model in image recognition provided by keras. The weights of these models are pre-trained on ImageNet.<br>In order to improve the performance of the classification model, these three models are used together to learn the features from the images. These three models build up a huge network. If a fully connected layer is added directly after this huge network to train the classification model, the computation cost will be extremely large. Thus, the features extraction and classifier training are conducted separately. The pre-trained models are used to extract features. And then, these features are used to train the classifier.<br>For the trainning process, a simple neural network was used as the classification model, this model includes an input layer, a hidden layer with dropout rate $0.5$, and an output layer with sigmoid as activation function. The features number learned by each model is 2048, and then 6144 features was learned from the feature extraction process. The number of nodes in the input layer is 6144. For the hidden layer, there is also 6144 nodes, but they are not fully connected because dropout was applied in this layer. For the output layer, there is only 1 node because it is a binary classification problem.\newline<br>With fine features learning by pre-trained models, a simple model can make a good classification. The visualized process of this model is shown is <strong><em>Figure 3</em></strong>.<br><img src="/images/cnnModel.jpeg" alt="method two">“Figure 3: Model of Method Two”</p><p><a href="https://github.com/wushbin/DogAndCat/blob/master/cnnCatVsDog.ipynb" target="_blank" rel="noopener">Source code for this method</a> </p><p><img src="/images/cnn_result.png" alt="method two result">“Result using Method two”<br>In addition, the cost time of feature extraction for each model(ResNet50, InceptionV3 and Xception) is less than 20 minutes, which is also more efficient than the method one. Moreover, it only take less than 10 minutes to train the classifier.   </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h3&gt;&lt;p&gt;In the note, a classifier of S
      
    
    </summary>
    
    
      <category term="machine learning" scheme="https://wushbin.github.io/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>Duke Gather: Android Application</title>
    <link href="https://wushbin.github.io/2017/05/01/Duke-Gather-Android-Application/"/>
    <id>https://wushbin.github.io/2017/05/01/Duke-Gather-Android-Application/</id>
    <published>2017-05-02T01:40:30.000Z</published>
    <updated>2018-01-13T21:56:48.781Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://github.com/wushbin/GatherApp" target="_blank" rel="noopener">link to this project in github</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://github.com/wushbin/GatherApp&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;link to this project in github&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Java" scheme="https://wushbin.github.io/tags/Java/"/>
    
      <category term="Mobile Application" scheme="https://wushbin.github.io/tags/Mobile-Application/"/>
    
  </entry>
  
  <entry>
    <title>High Speed Test Rig Development</title>
    <link href="https://wushbin.github.io/2016/06/01/High-Speed-Test-Rig-Development/"/>
    <id>https://wushbin.github.io/2016/06/01/High-Speed-Test-Rig-Development/</id>
    <published>2016-06-02T01:53:03.000Z</published>
    <updated>2018-01-13T21:22:44.023Z</updated>
    
    <content type="html"><![CDATA[<h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>这是一个失败的试验台设计，试验过程及数据详见文末论文链接。<br>本项目主要是搭建一个综合实验台,该实验台用以研究一款气体动压轴承和一款气体静压轴承的在高速旋转状态下的动态特性。  </p><ol><li>设计开发一个用以研究气体箔片止推轴承和多孔质静压气体轴承的实验台,实验台主要包括高速旋转部分,加载部分、供气系统和数据采集系统。其中高速旋转部分采用多孔质静压气体轴承作为转子径向支撑,实验台的轴向限位采用气体箔片止推轴承。</li><li>尝试并开发气体箔片止推轴承的制作工艺。</li><li>对实验台高速旋转部分的轴承-转子系统进行了动平衡实验和转子动力学实验。动平衡实验包括单面动平衡和双面动平衡实验。转子动力学实验研究多孔质静压气体轴承对高速转子性能的影响。</li></ol><h3 id="试验台设计"><a href="#试验台设计" class="headerlink" title="试验台设计"></a>试验台设计</h3><p>试验台主要由加载部分和高速旋转部分两部分组成<br><img src="/images/High-Speed-Test-Rig-Development/Picture1.png" style="width: 800px;"></p><h4 id="试验台加载部分"><a href="#试验台加载部分" class="headerlink" title="试验台加载部分"></a>试验台加载部分</h4><p><img src="/images/High-Speed-Test-Rig-Development/Picture2.jpg" style="width: 800px;"></p><h4 id="试验台高速旋转部分"><a href="#试验台高速旋转部分" class="headerlink" title="试验台高速旋转部分"></a>试验台高速旋转部分</h4><p><img src="/images/High-Speed-Test-Rig-Development/Picture3.jpg" style="width: 800px;"></p><h4 id="试验台供气系统"><a href="#试验台供气系统" class="headerlink" title="试验台供气系统"></a>试验台供气系统</h4><p><img src="/images/High-Speed-Test-Rig-Development/Picture4.png" style="width: 500px;"></p><h4 id="试验台整体"><a href="#试验台整体" class="headerlink" title="试验台整体"></a>试验台整体</h4><p><img src="/images/High-Speed-Test-Rig-Development/Picture5.png" style="width: 800px;"></p><h4 id="转子动平衡试验台"><a href="#转子动平衡试验台" class="headerlink" title="转子动平衡试验台"></a>转子动平衡试验台</h4><p><img src="/images/High-Speed-Test-Rig-Development/Picture6.png" style="width: 500px;"><br><img src="/images/High-Speed-Test-Rig-Development/Picture7.png" style="width: 500px;"></p><h3 id="试验台加工制造以及试验部分（见论文）"><a href="#试验台加工制造以及试验部分（见论文）" class="headerlink" title="试验台加工制造以及试验部分（见论文）"></a>试验台加工制造以及试验部分（见论文）</h3><p><a href="http://kns.cnki.net/KCMS/detail/detail.aspx?dbcode=CMFD&amp;dbname=CMFD201701&amp;filename=1016251956.nh&amp;v=MDAwMzZyV00xRnJDVVJMS2ZadWRyRmkzaFdyM0xWRjI2R0xHOUg5akpxWkViUElSOGVYMUx1eFlTN0RoMVQzcVQ=" target="_blank" rel="noopener">论文链接</a> </p><h4 id="气体箔片止推轴承制造工艺（见论文）"><a href="#气体箔片止推轴承制造工艺（见论文）" class="headerlink" title="气体箔片止推轴承制造工艺（见论文）"></a>气体箔片止推轴承制造工艺（见论文）</h4><h4 id="箔片止推轴承性能测试（见论文）"><a href="#箔片止推轴承性能测试（见论文）" class="headerlink" title="箔片止推轴承性能测试（见论文）"></a>箔片止推轴承性能测试（见论文）</h4><h4 id="高速转自动平衡试验（见论文）"><a href="#高速转自动平衡试验（见论文）" class="headerlink" title="高速转自动平衡试验（见论文）"></a>高速转自动平衡试验（见论文）</h4><h4 id="多孔质轴承-转子系统试验（见论文）"><a href="#多孔质轴承-转子系统试验（见论文）" class="headerlink" title="多孔质轴承-转子系统试验（见论文）"></a>多孔质轴承-转子系统试验（见论文）</h4><hr><p>于2016年夏天<br>导师：没有导师<br><a href="">试验台UG三维模型及详细工程图</a><br><a href="http://kns.cnki.net/KCMS/detail/detail.aspx?dbcode=CMFD&amp;dbname=CMFD201701&amp;filename=1016251956.nh&amp;v=MDAwMzZyV00xRnJDVVJMS2ZadWRyRmkzaFdyM0xWRjI2R0xHOUg5akpxWkViUElSOGVYMUx1eFlTN0RoMVQzcVQ=" target="_blank" rel="noopener">论文链接</a> </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h3&gt;&lt;p&gt;这是一个失败的试验台设计，试验过程及数据详见文末论文链接。&lt;br&gt;本项目主要是搭建一个综合实验台,该实验台用以研究一款气体动压轴承和一款气体
      
    
    </summary>
    
    
      <category term="mechanical design" scheme="https://wushbin.github.io/tags/mechanical-design/"/>
    
      <category term="experiment" scheme="https://wushbin.github.io/tags/experiment/"/>
    
  </entry>
  
  <entry>
    <title>SCARA Robot</title>
    <link href="https://wushbin.github.io/2013/06/01/SCARA-Robot/"/>
    <id>https://wushbin.github.io/2013/06/01/SCARA-Robot/</id>
    <published>2013-06-02T01:52:32.000Z</published>
    <updated>2018-01-13T21:47:44.585Z</updated>
    
    <content type="html"><![CDATA[<h3 id="SCARA机器人"><a href="#SCARA机器人" class="headerlink" title="SCARA机器人"></a>SCARA机器人</h3><p>SCARA平面关节式机器人是目前使用较为广泛的通用型机器人。在动作相对简单，而又需要有高产量的环境中，SCARA机器人相比六轴机器人而言很有优势的。SCARA机器人在点对点的运动中是最好的机器人，常用于分配、搬运、装载、包装、安放以及装配等作业之中。<br><img src="/images/SCARA-Robot/Picture0.jpg" style="width: 600px;"></p><h3 id="机器人本体机械设计"><a href="#机器人本体机械设计" class="headerlink" title="机器人本体机械设计"></a>机器人本体机械设计</h3><h4 id="机器人构型"><a href="#机器人构型" class="headerlink" title="机器人构型"></a>机器人构型</h4><p>两种比较具有代表性的SCARA机器人的构型，初步选择了两种方案，如下图所示：</p><ol><li>RRRT型SCARA机器人</li><li>TRRR型SCARA机器人<br><img src="/images/SCARA-Robot/Picture1.jpg" style="width: 400px;"><h4 id="机器人技术参数"><a href="#机器人技术参数" class="headerlink" title="机器人技术参数"></a>机器人技术参数</h4>该款SCARA机器人的关键设计参数如下图所示。<br><img src="/images/SCARA-Robot/Picture2.jpg" style="width: 600px;"></li></ol><h4 id="机器人传动方式"><a href="#机器人传动方式" class="headerlink" title="机器人传动方式"></a>机器人传动方式</h4><p>该四自由度关节型工业机器人各个轴的传动方案确定如下：</p><ul><li>Ｘ轴回转：底座→伺服电机→谐波减速器→大臂回转</li><li>Ｙ轴回转：大臂→伺服电机→谐波减速器→小臂回转 </li><li>Ｚ轴移动：小臂→伺服电机→同步带→丝杆螺母副→滚珠花键副上下平动</li><li>Ｒ轴回转：小臂→伺服电机→同步带→谐波减速器→滚珠花键轴套→滚珠花键副<br><img src="/images/SCARA-Robot/Picture3.jpg" style="width: 600px;"></li></ul><h4 id="机器人尺寸及工作空间"><a href="#机器人尺寸及工作空间" class="headerlink" title="机器人尺寸及工作空间"></a>机器人尺寸及工作空间</h4><p>机器人的整体设计结构如下图所示<br><img src="/images/SCARA-Robot/Picture4.jpg" style="width: 800px;"></p><p>机器人的整体尺寸以及工作空间如下图所示</p><p><img src="/images/SCARA-Robot/Picture5.jpg" style="width: 800px;"></p><p>机器人整体渲染效果图</p><div class="video-container"><iframe src="//www.youtube.com/embed/AxYgaXBQnxo" frameborder="0" allowfullscreen></iframe></div><p><img src="/images/SCARA-Robot/Picture6.jpg" style="width: 800px;"></p><h3 id="机器人关键结构设计"><a href="#机器人关键结构设计" class="headerlink" title="机器人关键结构设计"></a>机器人关键结构设计</h3><h4 id="机械臂一结构设计"><a href="#机械臂一结构设计" class="headerlink" title="机械臂一结构设计"></a>机械臂一结构设计</h4><p>电机固定在底座上，有利于减少机械臂的惯量。所选的谐波减速器为日本哈默纳科简易型谐波减速器SHG/SHF类型，该类型谐波属于简易型谐波减速器，内部置有用于支撑外部负载的精密、具有高刚性的交叉滚子轴承，不用再在外部安装用于支承负载的轴承，所以只需将刚轮、柔轮分别与底座和机械臂一固定，就能实现X轴的转动。<br><img src="/images/SCARA-Robot/Picture7.jpg" style="width: 400px;"></p><h4 id="机械臂手腕结构设计"><a href="#机械臂手腕结构设计" class="headerlink" title="机械臂手腕结构设计"></a>机械臂手腕结构设计</h4><p>由于主轴位于机器人小臂的末端，对重量和惯量比较敏感，所以要求整个结构紧凑、重量轻，同时考虑到控制系统设计的相对简单和成本的相对低廉，采用滚珠花键和滚珠螺杆组合的方式。目前SCARA机器人最新的结构是采用滚珠花键-丝杆一体的结构，但这样的结构需要两个电机耦合控制来实现末端的旋转和上下移动，使控制系统较为复杂。</p><p><img src="/images/SCARA-Robot/Picture8.jpg" style="width: 600px;"></p><h3 id="机器人运动学分析"><a href="#机器人运动学分析" class="headerlink" title="机器人运动学分析"></a>机器人运动学分析</h3><h4 id="工业机器人运动学方程"><a href="#工业机器人运动学方程" class="headerlink" title="工业机器人运动学方程"></a>工业机器人运动学方程</h4><p><img src="/images/SCARA-Robot/Picture9.jpg" style="width: 400px;"><br><img src="/images/SCARA-Robot/Picture10.jpg" style="width: 400px;"></p><h4 id="该SCARAb机器人D-H模型"><a href="#该SCARAb机器人D-H模型" class="headerlink" title="该SCARAb机器人D-H模型"></a>该SCARAb机器人D-H模型</h4><p>根据所设计的SCARA机器人结构，机器人D-H坐标系如下图所示<br><img src="/images/SCARA-Robot/Picture11.jpg" style="width: 400px;"><br>其连杆参数表如下图所示<br><img src="/images/SCARA-Robot/Picture12.jpg" style="width: 600px;"></p><h4 id="正运动学分析"><a href="#正运动学分析" class="headerlink" title="正运动学分析"></a>正运动学分析</h4><p>各连杆变换矩阵连乘，便能得到机器人末端连杆的位姿方程，也就是正运动学方程。<br><img src="/images/SCARA-Robot/Picture13.jpg" style="width: 600px;"></p><h4 id="逆运动学分析"><a href="#逆运动学分析" class="headerlink" title="逆运动学分析"></a>逆运动学分析</h4><p>在四自由度关节型机器人基坐标系中，机械手末端执行器的位姿矢量设为已知<br><img src="/images/SCARA-Robot/Picture15.jpg" style="width: 200px;"><br>也就是<br><img src="/images/SCARA-Robot/Picture16.jpg" style="width: 800px;"><br>最终可求得<br><img src="/images/SCARA-Robot/Picture17.jpg" style="width: 800px;"></p><hr><p>于2013年<br>导师：王念峰<br>华南理工大大学<br>源文件下载链接<br><a href="">solidworks模型及工程图</a>  </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;SCARA机器人&quot;&gt;&lt;a href=&quot;#SCARA机器人&quot; class=&quot;headerlink&quot; title=&quot;SCARA机器人&quot;&gt;&lt;/a&gt;SCARA机器人&lt;/h3&gt;&lt;p&gt;SCARA平面关节式机器人是目前使用较为广泛的通用型机器人。在动作相对简单，而又需要有高产量
      
    
    </summary>
    
    
      <category term="mechanical design" scheme="https://wushbin.github.io/tags/mechanical-design/"/>
    
      <category term="experiment" scheme="https://wushbin.github.io/tags/experiment/"/>
    
  </entry>
  
</feed>
